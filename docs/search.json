[
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes",
    "section": "",
    "text": "Main concepts:\n\nBest practices for coding in R\nWhat are algorithms\nCensus data foundations\nData Analytics are Subjective\n\nTechnical skills covered:\n\ndplyr basics practice\ntidycensus"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 Notes",
    "section": "",
    "text": "Main concepts:\n\nBest practices for coding in R\nWhat are algorithms\nCensus data foundations\nData Analytics are Subjective\n\nTechnical skills covered:\n\ndplyr basics practice\ntidycensus"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes",
    "section": "Reflection",
    "text": "Reflection"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\nHello! I am Katie Knox, a first-year MCP student at University of Pennsylvania. By undergraduate degree was in computer science and francophone studies at Swarthmore College, and for the past three years I have worked for National Digital Inclusion Alliance. I am taking this course because I want to learn how to apply data analysis skills towards public interest problems. I just moved from Toronto back to Philadelphia, so go birds! (Raptors and Eagles)\n\n\n\n\nEmail: knox1@upenn.edu\nGitHub: @kkxix"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Hello! I am Katie Knox, a first-year MCP student at University of Pennsylvania. By undergraduate degree was in computer science and francophone studies at Swarthmore College, and for the past three years I have worked for National Digital Inclusion Alliance. I am taking this course because I want to learn how to apply data analysis skills towards public interest problems. I just moved from Toronto back to Philadelphia, so go birds! (Raptors and Eagles)"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: knox1@upenn.edu\nGitHub: @kkxix"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Main concepts:\n\nBreakdown of syllabus (stay on toes for weekly quizzes)\n\nTechnical skills covered\n\nRStudio\nQuarto\nGit and Github"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Main concepts:\n\nBreakdown of syllabus (stay on toes for weekly quizzes)\n\nTechnical skills covered\n\nRStudio\nQuarto\nGit and Github"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nR functions:\n\nselect() - choose columns\nfilter() - choose rows\nmutate() - create new variables\nsummarize() - calculate statistics\ngroup_by() - operate on groups\nnames() - column names\nglimpse() - little glimpse of data\n\nQuarto features learned:\n\nQuarto is cool and hip :-)\nA way to show code snippets, run code blocks, share visuals, and share text as a webpage or slides (kinda like python notebook or observable)\n\n\ncommon syntax pattern: save_to &lt;- function(df, ...)\neverything except column: select(df, -col_to_exclude)\nanother common pattern: mutate(df, new_col = case_when(condition ~ code, TRUE ~ code2)\nIconic couple: group_by() and summarize() - group_by() – create categories out of values in a column - summarize() – summarize values in another column by a user-defined function - summarize(n=n()) – basic count of grouped categories\npiped syntax pattern: save_to &lt;- df %&gt;% function(...) %&gt;% funciton(...) ^^you don’t need df as first argument if first you state df and pass to functions through pipe"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWhy don’t you need to include df as argument when you are using piping syntax?"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nGot a preview of policy topics we will cover over semester:\n\nhousing\nrecidivism\nbikeshare equity\n\nWhat sets this apart from stats is emphasis on prediction"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nHype for this course!\nPhillies &gt; Mets"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html",
    "href": "labs/lab0/scripts/lab0_template.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you’ll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#data-structure-exploration",
    "href": "labs/lab0/scripts/lab0_template.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\nglimpse(car_data)\n\nRows: 50,000\nColumns: 7\n$ Manufacturer          &lt;chr&gt; \"Ford\", \"Porsche\", \"Ford\", \"Toyota\", \"VW\", \"Ford…\n$ Model                 &lt;chr&gt; \"Fiesta\", \"718 Cayman\", \"Mondeo\", \"RAV4\", \"Polo\"…\n$ `Engine size`         &lt;dbl&gt; 1.0, 4.0, 1.6, 1.8, 1.0, 1.4, 1.8, 1.4, 1.2, 2.0…\n$ `Fuel type`           &lt;chr&gt; \"Petrol\", \"Petrol\", \"Diesel\", \"Hybrid\", \"Petrol\"…\n$ `Year of manufacture` &lt;dbl&gt; 2002, 2016, 2014, 1988, 2006, 2018, 2010, 2015, …\n$ Mileage               &lt;dbl&gt; 127300, 57850, 39190, 210814, 127869, 33603, 866…\n$ Price                 &lt;dbl&gt; 3074, 49704, 24072, 1705, 4101, 29204, 14350, 30…\n\n# Check the column names\nnames(car_data)\n\n[1] \"Manufacturer\"        \"Model\"               \"Engine size\"        \n[4] \"Fuel type\"           \"Year of manufacture\" \"Mileage\"            \n[7] \"Price\"              \n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 × 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym…           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer: - How many rows and columns does the dataset have? - What types of variables do you see (numeric, character, etc.)? - Are there any column names that might cause problems? Why?\nYour answers:\n\nRows: 50k\nColumns: 7\n\nVariable types: There are both character and numeric (double) types of variables\nProblematic names: ‘Engine size’ ‘Fuel type’ and ‘Year of manufacture’ all have a space in them"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#tibble-vs-data-frame",
    "href": "labs/lab0/scripts/lab0_template.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 49,990 more rows\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\ncar_df &lt;- as.data.frame(car_data)\nhead(car_df)\n\n  Manufacturer      Model Engine size Fuel type Year of manufacture Mileage\n1         Ford     Fiesta         1.0    Petrol                2002  127300\n2      Porsche 718 Cayman         4.0    Petrol                2016   57850\n3         Ford     Mondeo         1.6    Diesel                2014   39190\n4       Toyota       RAV4         1.8    Hybrid                1988  210814\n5           VW       Polo         1.0    Petrol                2006  127869\n6         Ford      Focus         1.4    Petrol                2018   33603\n  Price\n1  3074\n2 49704\n3 24072\n4  1705\n5  4101\n6 29204\n\n\nQuestion: What differences do you notice in how they print?\nYour answer: The tibble automatically shows only 10 rows, and wraps the column names with spaces in them in ’’ marks. It also says what data type the different variables are. The data frame shows wayyyyyyy more rows by defaults and is formatted more consistently."
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#selecting-columns",
    "href": "labs/lab0/scripts/lab0_template.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\n# Select just Model and Mileage columns\ncar_df %&gt;% select(Model, Mileage) %&gt;% head()\n\n       Model Mileage\n1     Fiesta  127300\n2 718 Cayman   57850\n3     Mondeo   39190\n4       RAV4  210814\n5       Polo  127869\n6      Focus   33603\n\n# Select Manufacturer, Price, and Fuel type\ncar_df %&gt;% select(Manufacturer, Price, 'Fuel type') %&gt;% head()\n\n  Manufacturer Price Fuel type\n1         Ford  3074    Petrol\n2      Porsche 49704    Petrol\n3         Ford 24072    Diesel\n4       Toyota  1705    Hybrid\n5           VW  4101    Petrol\n6         Ford 29204    Petrol\n\n# Challenge: Select all columns EXCEPT Engine Size\ncar_df %&gt;% select(-'Engine size') %&gt;% head()\n\n  Manufacturer      Model Fuel type Year of manufacture Mileage Price\n1         Ford     Fiesta    Petrol                2002  127300  3074\n2      Porsche 718 Cayman    Petrol                2016   57850 49704\n3         Ford     Mondeo    Diesel                2014   39190 24072\n4       Toyota       RAV4    Hybrid                1988  210814  1705\n5           VW       Polo    Petrol                2006  127869  4101\n6         Ford      Focus    Petrol                2018   33603 29204"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#renaming-columns",
    "href": "labs/lab0/scripts/lab0_template.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet’s fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\ncar_data &lt;- rename(car_data, year = 'Year of manufacture')\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\" \"Model\"        \"Engine size\"  \"Fuel type\"    \"year\"        \n[6] \"Mileage\"      \"Price\"       \n\n\nQuestion: Why did we need backticks around Year of manufacture but not around year?\nYour answer: We don’t need backticks around year because it has no spaces."
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#calculate-car-age",
    "href": "labs/lab0/scripts/lab0_template.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create an 'age' column (2025 minus year of manufacture)\ncar_data &lt;- mutate(car_data, age = 2025-year)\n\n# Create a mileage_per_year column  \ncar_data &lt;- mutate(car_data, mileage_per_year = Mileage/age)\n\n# Look at your new columns\nselect(car_data, Model, year, age, Mileage, mileage_per_year)\n\n# A tibble: 50,000 × 5\n   Model       year   age Mileage mileage_per_year\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1 Fiesta      2002    23  127300            5535.\n 2 718 Cayman  2016     9   57850            6428.\n 3 Mondeo      2014    11   39190            3563.\n 4 RAV4        1988    37  210814            5698.\n 5 Polo        2006    19  127869            6730.\n 6 Focus       2018     7   33603            4800.\n 7 Mondeo      2010    15   86686            5779.\n 8 Prius       2015    10   30663            3066.\n 9 Polo        2012    13   73470            5652.\n10 Focus       1992    33  262514            7955.\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#categorize-cars",
    "href": "labs/lab0/scripts/lab0_template.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, its is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is mid-range (use case_when)\ncar_data &lt;- car_data %&gt;% mutate(price_category = case_when(\n  Price &lt; 15000 ~ \"budget\",\n  Price &gt;= 15000 & Price &lt; 30000 ~ \"midrange\",\n  Price &gt;= 30000 ~ \"luxury\"\n))\n\n\n# Check your categories select the new column and show it\nselect(car_data, price_category)\n\n# A tibble: 50,000 × 1\n   price_category\n   &lt;chr&gt;         \n 1 budget        \n 2 luxury        \n 3 midrange      \n 4 budget        \n 5 budget        \n 6 midrange      \n 7 budget        \n 8 luxury        \n 9 budget        \n10 budget        \n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#basic-filtering",
    "href": "labs/lab0/scripts/lab0_template.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\nfilter(car_data, Manufacturer == \"Toyota\")\n\n# A tibble: 12,554 × 10\n   Manufacturer Model `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4            1.8 Hybrid       1988  210814  1705    37\n 2 Toyota       Prius           1.4 Hybrid       2015   30663 30297    10\n 3 Toyota       RAV4            2.2 Petrol       2007   79393 16026    18\n 4 Toyota       Yaris           1.4 Petrol       1998   97286  4046    27\n 5 Toyota       RAV4            2.4 Hybrid       2003  117425 11667    22\n 6 Toyota       Yaris           1.2 Petrol       1992  245990   720    33\n 7 Toyota       RAV4            2   Hybrid       2018   28381 52671     7\n 8 Toyota       Prius           1   Hybrid       2003  115291  6512    22\n 9 Toyota       Prius           1   Hybrid       1990  238571   961    35\n10 Toyota       Prius           1.8 Hybrid       2017   31958 38961     8\n# ℹ 12,544 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with mileage less than 30,000\nfilter(car_data, Mileage&lt;30000)\n\n# A tibble: 5,402 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 Ford         Mondeo               1.6 Diesel       2015   21834 28435    10\n 9 VW           Passat               1.6 Diesel       2018   22122 36634     7\n10 VW           Passat               1.4 Diesel       2020   21413 39310     5\n# ℹ 5,392 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find luxury cars (from price category) with low mileage\ncar_data %&gt;% filter(Mileage&lt;30000) %&gt;% filter(price_category == \"luxury\")\n\n# A tibble: 3,257 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 VW           Passat               1.6 Diesel       2018   22122 36634     7\n 9 VW           Passat               1.4 Diesel       2020   21413 39310     5\n10 Toyota       RAV4                 2.4 Petrol       2021    6829 66031     4\n# ℹ 3,247 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#multiple-conditions",
    "href": "labs/lab0/scripts/lab0_template.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Honda OR Nissan\nfilter(car_data, Manufacturer == \"Honda\" | Manufacturer == \"Nissan\")\n\n# A tibble: 0 × 10\n# ℹ 10 variables: Manufacturer &lt;chr&gt;, Model &lt;chr&gt;, Engine size &lt;dbl&gt;,\n#   Fuel type &lt;chr&gt;, year &lt;dbl&gt;, Mileage &lt;dbl&gt;, Price &lt;dbl&gt;, age &lt;dbl&gt;,\n#   mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with price between $20,000 and $35,000\nfilter(car_data, Price &gt;20000 & Price &lt; 35000)\n\n# A tibble: 7,301 × 10\n   Manufacturer Model  `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Mondeo           1.6 Diesel       2014   39190 24072    11\n 2 Ford         Focus            1.4 Petrol       2018   33603 29204     7\n 3 Toyota       Prius            1.4 Hybrid       2015   30663 30297    10\n 4 Toyota       Prius            1.4 Hybrid       2016   43893 29946     9\n 5 Toyota       Prius            1.4 Hybrid       2016   43130 30085     9\n 6 VW           Passat           1.6 Petrol       2016   64344 23641     9\n 7 Ford         Mondeo           1.6 Diesel       2015   21834 28435    10\n 8 BMW          M5               4.4 Petrol       2008  109941 31711    17\n 9 BMW          Z4               2.2 Petrol       2014   61332 26084    11\n10 Porsche      911              3.5 Petrol       2003  107705 24378    22\n# ℹ 7,291 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find diesel cars less than 10 years old\ncar_data %&gt;% filter(`Fuel type` == \"Diesel\" & age &lt; 10)\n\n# A tibble: 2,040 × 10\n   Manufacturer Model   `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta            1   Diesel       2017   38370 16257     8\n 2 VW           Passat            1.6 Diesel       2018   22122 36634     7\n 3 VW           Passat            1.4 Diesel       2020   21413 39310     5\n 4 BMW          X3                2   Diesel       2018   27389 44018     7\n 5 Ford         Mondeo            2   Diesel       2016   51724 28482     9\n 6 Porsche      Cayenne           2.6 Diesel       2019   20147 76182     6\n 7 VW           Polo              1.2 Diesel       2018   37411 19649     7\n 8 Ford         Mondeo            1.8 Diesel       2016   29439 30886     9\n 9 Ford         Mondeo            1.4 Diesel       2020   18929 37720     5\n10 Ford         Mondeo            1.4 Diesel       2018   42017 28904     7\n# ℹ 2,030 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: 2040"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#basic-summaries",
    "href": "labs/lab0/scripts/lab0_template.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\navg_mileage_by_fuel_type &lt;- car_data %&gt;%\n  group_by(`Fuel type`) %&gt;%\n  summarize(avg_mileage = mean(Mileage, na.rm = TRUE))\n\navg_mileage_by_fuel_type\n\n# A tibble: 3 × 2\n  `Fuel type` avg_mileage\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Diesel          112667.\n2 Hybrid          111622.\n3 Petrol          112795.\n\n# Count cars by manufacturer\ncars_per_manufacturer &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(count = n())\n\ncars_per_manufacturer\n\n# A tibble: 5 × 2\n  Manufacturer count\n  &lt;chr&gt;        &lt;int&gt;\n1 BMW           4965\n2 Ford         14959\n3 Porsche       2609\n4 Toyota       12554\n5 VW           14913"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#categorical-summaries",
    "href": "labs/lab0/scripts/lab0_template.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories\ncar_data %&gt;% \n  group_by(price_category) %&gt;%\n  summarise(frequency = n())\n\n# A tibble: 3 × 2\n  price_category frequency\n  &lt;chr&gt;              &lt;int&gt;\n1 budget             34040\n2 luxury              6179\n3 midrange            9781"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#notes",
    "href": "weekly-notes/week-02-notes.html#notes",
    "title": "Week 2 Notes",
    "section": "Notes",
    "text": "Notes\n\nCopying labs from MUSA repo to portfolio repo\n\ntemplate goes to scripts folder, data goes to data folder\n\n\n\nLab0 / General workflow in R\n\nWant to avoid “hoarding” tibbles, try not to create a new tibble for every operation\nMay way to read in data and store as a ‘pristine’ version, but then create a working version\nUse piping to string together operations, no need to store in-between temporary tibbles\n\n\n\nAlgorithmic Decision-Making & Census Data\n\nAlgorithms = set of instructions for solving a problem/completing a task\nAlgorithmic decision-making in government\n\nSystems used to assist or replace human decision-making (humans may be inefficient, flawed/biased)\nDecisions are made based on predictions from models that process historical data relevant to the decision-making process:\n\nInputs = features = predictors = independent variables = x\nOutputs = labels - outcome = dependent variable = y\n\nExamples:\n\nRecidivism risk for bail and sentencing decisions\nMortgage lending and tenant screening\npatient care prioritization and health care resource allocation\n\n\nClarifying terms:\n\nData science -&gt; engineering, algorithms, and methods\nData analytic -&gt;applying data science to other disciplines\nMachine Learning -&gt; algorithms for classification and prediction/regression\nAI -&gt; Algorithms that adjust and improve across iterations (neural networks, deep learning)\n\nGov’t data collection historically\n\ncivic registrations systems\ncensus data\nadministration records\noperations research – post-WWII efforts to determine things like the best place for hospitals etc\n\nNew data sources\n\nMore data available from both official and ‘unofficial’ or ‘accidental’ sources like Instagram\nTurn from explanation to prediction\nMessier data\n\nWhy use algos in gov’t?\n\nEfficiency - process more cases faster\nConsistency - same process applied equally (In theory)\nObjectivity - remove human bias\nCost-saving - remove costly human labors\n\nBig But – Data Analytics are Subjective\n\nAt every step, there are human choices:\n\nData cleaning decisions\nRecoding/classification\nData collection biases\nImperfect proxies\nInterpreting results\nWhat variables you include in your model\n\n\nExample - Healthcare Algorithm Bias\n\nAlgorithm to identify high-risk patients systematically discriminated against Black patients, why?\n\nAlgo used healthcare cost as a proxy for need (high cost = more risk, less cost = low risk)\nProblem: black patients incur lower costs because of exclusion from health care and unequal access\n\nResult: Black patients were systematically not prioritized\n\nExample - COMPAS Recidivism Prediction\n\nAlgo flagged 2x as likely Black defendants as White as high risk of reoffending, why?\n\nData came from pigs\n\n\nExample - Dutch Welfare Fraud Detection\n\nAlgo disproportionately targeted vulnerable populations\n\n\n\n\nScenario\n\nAutomated traffic enforcements\n\nHighest traffic volume + Mots crashes as proxy for places needing more traffic enforcement\nBlind spot -&gt; unreported crashes (folks avoiding insurance fees for instance)\nHarm -&gt; Could under-enforce actually dangerous intersections, otherwise could be over-enforcement in over-policed areas\nSafeguard -&gt; Cap disparities across areas\n\n\n\n\nCensus Data Foundations\n\nDecennial census mandated by constitution for allocating representative governments\n\nOnly 9 questions on 10y census – age, race, sex, housing\n\nACS\n\n3% of households surveyed annually\nDetailed questions\n\n2020 Innovation: Differential Privacy\n\nadded noise to protect privacy, but this leads to weird edge cases like people living under water\n\nAccessing Census Dat ain R\n\ntidycensus !!! hooray\n\nStructure:\n\nData organized in tables\nEach table has multiple variables\n\n……E = estimate variable\n…..M = MOE variable\n\n\nRule of thumb\n\nLarge MOE : estimate means less reliable\nSmall MOE : estimate means more reliable\n\nTIGER/Line Files\n\nGeographic shape data not automatically included in ACS tables, need to add on\nNow released as shapefiles\n\nTwo types of census outputs:\n\nWide\n\nColumns: Tract, Variable_1, Variable_2…\ni.e. One row per tract\n\nLong\n\nColumns: Tract, Var, Value\ni.e. Tract * variable number of rows"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html",
    "href": "labs/lab1/scripts/assignment1_template.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Michigan Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#scenario",
    "href": "labs/lab1/scripts/assignment1_template.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Michigan Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#learning-objectives",
    "href": "labs/lab1/scripts/assignment1_template.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#submission-instructions",
    "href": "labs/lab1/scripts/assignment1_template.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#data-retrieval",
    "href": "labs/lab1/scripts/assignment1_template.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\n\n# Display the first few rows"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#data-quality-assessment",
    "href": "labs/lab1/scripts/assignment1_template.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#high-uncertainty-counties",
    "href": "labs/lab1/scripts/assignment1_template.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\n\n# Format as table with kable() - include appropriate column names and caption\n\nData Quality Commentary:\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#focus-area-selection",
    "href": "labs/lab1/scripts/assignment1_template.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nComment on the output: [write something :)]"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#tract-level-demographics",
    "href": "labs/lab1/scripts/assignment1_template.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\n\n# Add readable tract and county name columns using str_extract() or similar"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#demographic-analysis",
    "href": "labs/lab1/scripts/assignment1_template.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\n\n# Create a nicely formatted table of your results using kable()"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#moe-analysis-for-demographic-variables",
    "href": "labs/lab1/scripts/assignment1_template.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\n# Create summary statistics showing how many tracts have data quality issues"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#pattern-analysis",
    "href": "labs/lab1/scripts/assignment1_template.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nPattern Analysis: [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?]"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#analysis-integration-and-professional-summary",
    "href": "labs/lab1/scripts/assignment1_template.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\n[Your integrated 4-paragraph summary here]"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#specific-recommendations",
    "href": "labs/lab1/scripts/assignment1_template.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\n# Format as a professional table with kable()\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: [List counties with high confidence data and explain why they’re appropriate]\nCounties requiring additional oversight: [List counties with moderate confidence data and describe what kind of monitoring would be needed]\nCounties needing alternative approaches: [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.]"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#questions-for-further-investigation",
    "href": "labs/lab1/scripts/assignment1_template.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[List 2-3 questions that your analysis raised that you’d like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#submission-checklist",
    "href": "labs/lab1/scripts/assignment1_template.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html",
    "href": "labs/lab1/scripts/assignment1_kk.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Michigan Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#scenario",
    "href": "labs/lab1/scripts/assignment1_kk.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Michigan Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#learning-objectives",
    "href": "labs/lab1/scripts/assignment1_kk.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#submission-instructions",
    "href": "labs/lab1/scripts/assignment1_kk.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#data-retrieval",
    "href": "labs/lab1/scripts/assignment1_kk.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\nmi_county_pop_income &lt;- get_acs(\n  geography = \"county\", \n  variables = c(\n    median_household_income = \"B19013_001\",\n    total_pop = \"B01003_001\"),\n  state = \"MI\",\n  year = 2022,\n  output= \"wide\",\n  survey = \"acs5\")\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\nmi_county_pop_income &lt;- mi_county_pop_income %&gt;% \n  mutate(County_Name = str_remove(NAME, \" County, Michigan\")) \n\n# Display the first few rows\nhead(mi_county_pop_income)\n\n# A tibble: 6 × 7\n  GEOID NAME            median_household_inc…¹ median_household_inc…² total_popE\n  &lt;chr&gt; &lt;chr&gt;                            &lt;dbl&gt;                  &lt;dbl&gt;      &lt;dbl&gt;\n1 26001 Alcona County,…                  50295                   2243      10238\n2 26003 Alger County, …                  55528                   2912       8866\n3 26005 Allegan County…                  75543                   2369     120189\n4 26007 Alpena County,…                  49133                   2119      28911\n5 26009 Antrim County,…                  68850                   3115      23662\n6 26011 Arenac County,…                  53487                   2018      15031\n# ℹ abbreviated names: ¹​median_household_incomeE, ²​median_household_incomeM\n# ℹ 2 more variables: total_popM &lt;dbl&gt;, County_Name &lt;chr&gt;"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#data-quality-assessment",
    "href": "labs/lab1/scripts/assignment1_kk.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\nmi_county_pop_income &lt;- mi_county_pop_income %&gt;%\n  mutate(\n    income_moe_percent = (median_household_incomeM/ median_household_incomeE)*100\n      ) %&gt;%\n  mutate(\n    income_e_reliability = case_when(\n      income_moe_percent &lt; 5 ~ \"High Confidence\",\n      income_moe_percent &gt;=5 & income_moe_percent &lt;= 10 ~ \"Moderate Confidence\",\n      income_moe_percent &gt; 10 ~ \"Low Confidence\"\n    )\n  ) %&gt;%\n  mutate(\n    unreliable_income_e = case_when(\n      income_e_reliability == \"Low Confidence\" ~ TRUE,\n      TRUE ~ FALSE\n    )\n  )\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\nincome_reliability_summary &lt;- mi_county_pop_income %&gt;%\n  group_by(income_e_reliability) %&gt;%\n  summarise(num_counties = n()) %&gt;%\n  mutate(\n    percent_of_counties = (num_counties/ sum(num_counties)) *100\n  )"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#high-uncertainty-counties",
    "href": "labs/lab1/scripts/assignment1_kk.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\ntop_5_unreliable &lt;- mi_county_pop_income %&gt;%\n  arrange(desc(income_moe_percent)) %&gt;%\n  top_n(5, income_moe_percent) %&gt;%\n  select(NAME, median_household_incomeE, median_household_incomeM, income_moe_percent, income_e_reliability)\n  \n\n# Format as table with kable() - include appropriate column names and caption\ntop_5_unreliable_formatted &lt;- top_5_unreliable %&gt;%\n  mutate(\n    median_household_incomeE = paste0(\"$\", format(median_household_incomeE, big.mark = \",\", scientific = FALSE)),\n    median_household_incomeM = paste0(\"±\", \"$\", format(median_household_incomeM, big.mark = \",\", scientific = FALSE)),\n    income_moe_percent = paste0(round(income_moe_percent, 2), \"%\"),\n  )\n\n  kable(top_5_unreliable_formatted,\n      col.names = c(\"County Name\", \"Median Income\", \"MOE\", \"MOE Percent of Estimate\", \"Reliability of Estimate\"),\n      align = \"l\",\n      caption = \"5 Least Reliable Median Income Estimates of Counties in Michigan\",\n      digits = 2,\n      format.args = list(big.mark = \",\", scientific = FALSE))\n\n\n5 Least Reliable Median Income Estimates of Counties in Michigan\n\n\n\n\n\n\n\n\n\nCounty Name\nMedian Income\nMOE\nMOE Percent of Estimate\nReliability of Estimate\n\n\n\n\nKeweenaw County, Michigan\n$55,560\n±$7,301\n13.14%\nLow Confidence\n\n\nSchoolcraft County, Michigan\n$55,071\n±$6,328\n11.49%\nLow Confidence\n\n\nGogebic County, Michigan\n$47,913\n±$4,766\n9.95%\nModerate Confidence\n\n\nOtsego County, Michigan\n$62,865\n±$5,910\n9.4%\nModerate Confidence\n\n\nMontmorency County, Michigan\n$46,345\n±$3,796\n8.19%\nModerate Confidence\n\n\n\n\n\nData Quality Commentary:\nAssistance programs and policies aimed at community development may look at median income to identify priority areas. Areas with lower reliability may have a far lower or higher actual median income than what is reported in the census, and could affect the calculations of how resources are allocated to those areas. Additionally, higher unreliability is more likely in less population geographies because it is harder to get a representative sample, so smaller or less densely populated geographies, which are already often economically vulnerable, may be further marginalized by being overlooked in the prioritization of assistance programs based on these metrics."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#focus-area-selection",
    "href": "labs/lab1/scripts/assignment1_kk.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\ncounties &lt;- c(\"Kalamazoo\", \"Crawford\", \"Keweenaw\")\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\nselected_counties &lt;- filter(mi_county_pop_income, County_Name %in% counties)\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nselected_counties %&gt;%\n  select(NAME, median_household_incomeE, income_moe_percent, income_e_reliability) %&gt;%\n  mutate(\n    median_household_incomeE = paste0(\"$\", format(median_household_incomeE, big.mark = \",\", scientific = FALSE)),\n    income_moe_percent = paste0(round(income_moe_percent, 2), \"%\"),\n  ) %&gt;%\nkable(col.names = c(\"County Name\", \"Median Income\", \"MOE Percent of Estimate\", \"Reliability of Estimate\"),\n      align = \"l\",\n      caption = \"3 Selected Counties in Michigan\")\n\n\n3 Selected Counties in Michigan\n\n\n\n\n\n\n\n\nCounty Name\nMedian Income\nMOE Percent of Estimate\nReliability of Estimate\n\n\n\n\nCrawford County, Michigan\n$57,998\n5.93%\nModerate Confidence\n\n\nKalamazoo County, Michigan\n$67,905\n2.31%\nHigh Confidence\n\n\nKeweenaw County, Michigan\n$55,560\n13.14%\nLow Confidence\n\n\n\n\n\nComment on the output:\nI chose:\n\nKalamazoo as a high reliable county and a relatively bigger city in Western Michigan with multiple universities.\nCrawford is a moderate reliable county and where I used to go fishing with my grandpa :)\nKeweenaw is a low reliable county in the most northern tip of the upper penninsula with a very small population (I didn’t know anyone even lived there)"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#tract-level-demographics",
    "href": "labs/lab1/scripts/assignment1_kk.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements:\n\nGeography: tract level\nVariables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001)\nUse the same state and year as before\nOutput format: wide\n\nChallenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\ndemographics &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(White = \"B03002_003\", Black = \"B03002_004\", Latinx = \"B03002_012\", Total = \"B03002_001\"),\n  state = \"MI\", \n  county = counties,\n  year = 2022,\n  output=\"wide\"\n)\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\ndemographics &lt;- demographics %&gt;%\n  mutate(percent_black = if_else(TotalE == 0, 0, BlackE / TotalE * 100),\n         percent_white = if_else(TotalE == 0, 0, WhiteE / TotalE * 100),\n         percent_latin = if_else(TotalE == 0, 0, LatinxE / TotalE * 100) \n  )%&gt;%\n  mutate(percent_black_c = paste0(round(percent_black, 2), \"%\"),\n         percent_white_c = paste0(round(percent_white), \"%\"),\n         percent_latin_c = paste0(round(percent_latin, 2), \"%\")\n  )\n\n# Add readable tract and county name columns using str_extract() or similar\ndemographics &lt;- demographics %&gt;%\n  mutate(\n    County_Name = str_extract(NAME, \"(?&lt;=;\\\\s)[A-Za-z ]+(?=\\\\sCounty;)\"),\n    Tract = str_extract(NAME, \"(?&lt;=Census Tract )[0-9.]+\")\n  )"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#demographic-analysis",
    "href": "labs/lab1/scripts/assignment1_kk.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\ndemographics %&gt;% \n  filter(!is.nan(percent_latin)) %&gt;%\n  arrange(desc(percent_latin)) %&gt;% \n  slice(1) %&gt;%\n  select(County_Name, Tract, percent_latin_c) %&gt;%\n  kable(col.names = c(\"County\", \"Tract\", \"Percent Latin\"),\n        align = \"c\")\n\n\n\n\nCounty\nTract\nPercent Latin\n\n\n\n\nKalamazoo\n10.01\n25.44%\n\n\n\n\n\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\navg_demo_by_tract &lt;- demographics %&gt;%\n  group_by(County_Name) %&gt;%\n  summarize(avg_black = mean(percent_black),\n            avg_white = mean(percent_white),\n            avg_latin = mean(percent_latin))\n\n# Create a nicely formatted table of your results using kable()\navg_demo_by_tract %&gt;%\n  mutate(avg_black_c = paste0(round(avg_black, 2), \"%\"),\n         avg_white_c = paste0(round(avg_white, 2), \"%\"),\n         avg_latin_c = paste0(round(avg_latin, 2), \"%\")) %&gt;%\n  select(County_Name, avg_black_c, avg_white_c, avg_latin_c) %&gt;% \n  kable(col.names = c(\"County\", \"Avg % Black\", \"Avg % White\", \"Avg % Latin\"),\n        caption = \"Average Demographic Makeup of All Census Tracts in 3 MI Counties\",\n        align = \"r\")\n\n\nAverage Demographic Makeup of All Census Tracts in 3 MI Counties\n\n\nCounty\nAvg % Black\nAvg % White\nAvg % Latin\n\n\n\n\nCrawford\n0.28%\n92.25%\n2.19%\n\n\nKalamazoo\n12.7%\n73.33%\n5.81%\n\n\nKeweenaw\n0%\n65.28%\n0.53%"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#moe-analysis-for-demographic-variables",
    "href": "labs/lab1/scripts/assignment1_kk.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\ndemographics &lt;- demographics %&gt;%\n  mutate(White_percent_MOE = WhiteM/WhiteE*100,\n         Black_percent_MOE = BlackM/BlackE*100, \n         Latin_percent_MOE = LatinxM/LatinxE*100)\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\ndemographics &lt;- demographics %&gt;% \n  mutate(\n    flag = case_when(\n      White_percent_MOE &gt;=100 |\n      Black_percent_MOE &gt;=100 | \n      Latin_percent_MOE &gt;=100 ~\n      \"MOE Higher than Estimate\",\n      (White_percent_MOE &gt;=50 & White_percent_MOE &lt; 100) |\n      (Black_percent_MOE &gt;=50 & Black_percent_MOE &lt; 100) | \n      (Latin_percent_MOE &gt;=50 & Latin_percent_MOE &lt; 100) ~\n      \"Very Unreliable\",\n      (White_percent_MOE &gt;10 & White_percent_MOE &lt; 50) |\n      (Black_percent_MOE &gt;10 & Black_percent_MOE &lt; 50) | \n      (Latin_percent_MOE &gt;10 & Latin_percent_MOE &lt; 50) ~\n      \"Unreliable\", \n      White_percent_MOE &lt;=10 |\n      Black_percent_MOE &lt;=10 | \n      Latin_percent_MOE &lt;=10 ~\n      \"Reliable\"\n    )\n  )\n\n# Create summary statistics showing how many tracts have data quality issues\nreliability_summary &lt;- demographics %&gt;%\n  group_by(flag) %&gt;% \n  summarize(count = n())\n\nkable(reliability_summary,\n      col.names = c(\"Flag\", \"Number of Tracts\"),\n      align=\"c\")\n\n\n\n\nFlag\nNumber of Tracts\n\n\n\n\nMOE Higher than Estimate\n37\n\n\nUnreliable\n3\n\n\nVery Unreliable\n36"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#pattern-analysis",
    "href": "labs/lab1/scripts/assignment1_kk.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\nmoe_issues &lt;- demographics %&gt;% \n  group_by(flag) %&gt;%\n  summarise(\n    avg_pop_size = mean(TotalE),\n    avg_percent_black = mean(if_else(TotalE != 0, BlackE / TotalE * 100, NA_real_), na.rm = TRUE),\n    avg_percent_white = mean(if_else(TotalE != 0, WhiteE / TotalE * 100, NA_real_), na.rm = TRUE),\n    avg_percent_latin = mean(if_else(TotalE != 0, LatinxE / TotalE * 100, NA_real_), na.rm = TRUE)\n  )\n  \n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\nmoe_issues &lt;- moe_issues %&gt;%\n  arrange(desc(avg_percent_black)) %&gt;%\n  mutate(avg_percent_black_c = paste0(round(avg_percent_black, 2), \"%\"),\n         avg_percent_white_c = paste0(round(avg_percent_white, 2), \"%\"),\n         avg_percent_latin_c = paste0(round(avg_percent_latin, 2), \"%\")) \n\n\nmoe_issues %&gt;%\n  select(flag, avg_pop_size, avg_percent_black_c, avg_percent_white_c, avg_percent_latin_c) %&gt;% \n  kable(col.names = c(\"Reliability (Bad to Worst)\", \"Avg Tract Population Size\", \"Avg % Black\", \"Avg % White\", \"Avg % Latin\"),\n        caption = \"Average Demographic Makeup of All Census Tracts in 3 MI Counties\",\n        align = \"r\",\n        digits = 0)\n\n\nAverage Demographic Makeup of All Census Tracts in 3 MI Counties\n\n\n\n\n\n\n\n\n\nReliability (Bad to Worst)\nAvg Tract Population Size\nAvg % Black\nAvg % White\nAvg % Latin\n\n\n\n\nUnreliable\n2762\n21.99%\n52.73%\n19.85%\n\n\nVery Unreliable\n4112\n14.88%\n70.22%\n6.23%\n\n\nMOE Higher than Estimate\n3254\n7.32%\n82.16%\n3.43%\n\n\n\n\n\n\nmoe_issues_2 &lt;- demographics %&gt;% \n  group_by(County_Name) %&gt;%\n  summarise(\n    total_pop_size = sum(TotalE),\n    avg_moe_percent_black = mean(if_else(BlackE != 0, BlackM / BlackE * 100, NA_real_), na.rm = TRUE),\n    avg_moe_percent_white = mean(if_else(WhiteE != 0, WhiteM / WhiteE * 100, NA_real_), na.rm = TRUE),\n    avg_moe_percent_latin = mean(if_else(LatinxE != 0, LatinxM / LatinxE * 100, NA_real_), na.rm = TRUE)\n  )\n  \n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\nmoe_issues_2 &lt;- moe_issues_2 %&gt;%\n  mutate(avg_moe_percent_black_c = paste0(round(avg_moe_percent_black, 2), \"%\"),\n         avg_moe_percent_white_c = paste0(round(avg_moe_percent_white, 2), \"%\"),\n         avg_moe_percent_latin_c = paste0(round(avg_moe_percent_latin, 2), \"%\")) \n\nmoe_issues_2 %&gt;%\n  arrange(desc(total_pop_size))%&gt;%\n  select(County_Name, total_pop_size, avg_moe_percent_black_c, avg_moe_percent_white_c, avg_moe_percent_latin_c) %&gt;% \n  kable(col.names = c(\"County\", \"Total Population Size\", \"Avg Black MOE % of Estimate\", \"Avg White MOE % of Estimate\", \"Avg Latin MOE % of Estimate\"),\n        caption = \"Average MOE % of Estimate of Different Racial Groups by County\",\n        align = \"r\",\n        digits = 0)\n\n\nAverage MOE % of Estimate of Different Racial Groups by County\n\n\n\n\n\n\n\n\n\nCounty\nTotal Population Size\nAvg Black MOE % of Estimate\nAvg White MOE % of Estimate\nAvg Latin MOE % of Estimate\n\n\n\n\nKalamazoo\n261426\n70.69%\n17.82%\n86.12%\n\n\nCrawford\n13197\n124.44%\n11.33%\n88.48%\n\n\nKeweenaw\n2088\nNaN%\n25.15%\n12.12%\n\n\n\n\n\nPattern Analysis:\nGiven that there was not a single tract in the three counties where all three racial population MOEs were below 10% of the estimate itself, I decided to make categories “Unreliable” where MOE was up to 50% of estimate, “Very unreliable” if 50- 100 % of estimate, and then if it was over 100, that indicates the MOE is bigger than the estimate itself, or “Inf” shows that the estimate was 0, which can’t be a denominator. We can see that on this scale, the more racially homogenous an area is on average, the less reliable the estimates are for that area. I also looked at the averagve MOEs as percent of Estimate for the three different racial groups by county, and accross all three counties, the White population had the lowest average MOE % of Estimate (except in Keweenaw, but this is based on only one tract having any estimate Latinx population at all)."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#analysis-integration-and-professional-summary",
    "href": "labs/lab1/scripts/assignment1_kk.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements:\n\nOverall Pattern Identification: What are the systematic patterns across all your analyses?\nEquity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings?\nRoot Cause Analysis: What underlying factors drive both data quality issues and bias risk?\nStrategic Recommendations: What should the Department implement to address these systematic issues?\n\nExecutive Summary:\nAccross the data I analyzed, the clearest pattern that emerged is that the more homogenous a tract, the more unreliable the estimates. We also see that this unreliability is not evenly shared, but whatever population is in the minority faces more unreliable estimates, leading to a sort of double marginalization, where an already marginalized population is less accurately represented in data, and decisions made on these data could be very misguided.\nIn Michigan, Black and Latinx populations face the greatest risk of algorithmic bias, even in a county that is relatively diverse, like Kalamazoo, any given tract might have a much higher White majority and unreliable estimates of Black and Latinx populations.\nThe underlying issues that could be at play here are that when attaining a survey sample especially in a census tract with a relatively low population and very low diversity, there might be a handful of people of the demographic the census is trying to estimate the size of, causing issues in extrapolating from the sample to the overall makeup of the tract and the county. In Michigan specifically, the state is about 3/4 White, and there is a history of racialized segregration of living areas that persists today, so even in diverse areas, any given tract might have high unreliability because of a more homogenous makeup.\nThe Department could address this by supplementing census data with further sampling to add to the dataset, however this risks also overburdening minority populations with survey fatigue. The Department should at least be very cautious and skeptical of algorithmic decision-making based on data at the census tract level, and should set a higher minimum population size where data reliability of all racial/ethnic groups is adequately low for any allocations or programs that might be based on these estimates."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#specific-recommendations",
    "href": "labs/lab1/scripts/assignment1_kk.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\nincome_summary &lt;- mi_county_pop_income %&gt;% select(County_Name, median_household_incomeE, income_moe_percent, income_e_reliability)\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\nincome_summary &lt;- income_summary %&gt;%\n  mutate(safe = case_when(\n    income_e_reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n    income_e_reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n    income_e_reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n  ))\n\n# Format as a professional table with kable()\nincome_summary %&gt;%\n  select(County_Name, median_household_incomeE, safe) %&gt;%\n  arrange(median_household_incomeE) %&gt;%\n  kable(\n    col.names = c(\"County Name\", \"Median Household Income\", \"Algorithm Recommendation\"),\n    digit = 0,\n    format.args = list(big.mark = \",\"),\n    align = \"l\",\n    caption = \"Median Income of Michigan Counties by How Safe they are to use for Algorithmic Decision-making\"\n  )\n\n\nMedian Income of Michigan Counties by How Safe they are to use for Algorithmic Decision-making\n\n\n\n\n\n\n\nCounty Name\nMedian Household Income\nAlgorithm Recommendation\n\n\n\n\nLake\n45,946\nUse with caution - monitor outcomes\n\n\nIosco\n46,224\nUse with caution - monitor outcomes\n\n\nMontmorency\n46,345\nUse with caution - monitor outcomes\n\n\nClare\n47,816\nSafe for algorithmic decisions\n\n\nGogebic\n47,913\nUse with caution - monitor outcomes\n\n\nOntonagon\n48,316\nUse with caution - monitor outcomes\n\n\nOscoda\n48,692\nUse with caution - monitor outcomes\n\n\nAlpena\n49,133\nSafe for algorithmic decisions\n\n\nRoscommon\n49,898\nUse with caution - monitor outcomes\n\n\nAlcona\n50,295\nSafe for algorithmic decisions\n\n\nOgemaw\n50,377\nSafe for algorithmic decisions\n\n\nLuce\n51,015\nUse with caution - monitor outcomes\n\n\nBaraga\n51,911\nUse with caution - monitor outcomes\n\n\nIron\n52,241\nUse with caution - monitor outcomes\n\n\nIsabella\n52,638\nSafe for algorithmic decisions\n\n\nHoughton\n52,736\nSafe for algorithmic decisions\n\n\nArenac\n53,487\nSafe for algorithmic decisions\n\n\nGladwin\n53,717\nSafe for algorithmic decisions\n\n\nDelta\n53,852\nUse with caution - monitor outcomes\n\n\nMenominee\n54,074\nUse with caution - monitor outcomes\n\n\nMecosta\n54,132\nUse with caution - monitor outcomes\n\n\nHuron\n54,475\nSafe for algorithmic decisions\n\n\nOsceola\n54,875\nSafe for algorithmic decisions\n\n\nSchoolcraft\n55,071\nRequires manual review or additional data\n\n\nAlger\n55,528\nUse with caution - monitor outcomes\n\n\nKeweenaw\n55,560\nRequires manual review or additional data\n\n\nSanilac\n55,740\nSafe for algorithmic decisions\n\n\nPresque Isle\n55,986\nUse with caution - monitor outcomes\n\n\nKalkaska\n56,380\nUse with caution - monitor outcomes\n\n\nSaginaw\n56,579\nSafe for algorithmic decisions\n\n\nWayne\n57,223\nSafe for algorithmic decisions\n\n\nMissaukee\n57,667\nUse with caution - monitor outcomes\n\n\nBay\n57,887\nSafe for algorithmic decisions\n\n\nGratiot\n57,934\nSafe for algorithmic decisions\n\n\nCrawford\n57,998\nUse with caution - monitor outcomes\n\n\nCalhoun\n58,191\nSafe for algorithmic decisions\n\n\nChippewa\n58,408\nSafe for algorithmic decisions\n\n\nGenesee\n58,594\nSafe for algorithmic decisions\n\n\nWexford\n58,652\nUse with caution - monitor outcomes\n\n\nNewaygo\n59,065\nSafe for algorithmic decisions\n\n\nHillsdale\n59,425\nSafe for algorithmic decisions\n\n\nManistee\n59,467\nSafe for algorithmic decisions\n\n\nCheboygan\n59,557\nSafe for algorithmic decisions\n\n\nDickinson\n59,651\nSafe for algorithmic decisions\n\n\nTuscola\n59,815\nSafe for algorithmic decisions\n\n\nBerrien\n60,379\nSafe for algorithmic decisions\n\n\nBranch\n60,600\nSafe for algorithmic decisions\n\n\nMackinac\n60,620\nUse with caution - monitor outcomes\n\n\nOceana\n60,691\nSafe for algorithmic decisions\n\n\nMason\n60,744\nSafe for algorithmic decisions\n\n\nMontcalm\n61,250\nSafe for algorithmic decisions\n\n\nMuskegon\n61,347\nSafe for algorithmic decisions\n\n\nSt. Joseph\n62,281\nSafe for algorithmic decisions\n\n\nShiawassee\n62,498\nSafe for algorithmic decisions\n\n\nIngham\n62,548\nSafe for algorithmic decisions\n\n\nJackson\n62,581\nSafe for algorithmic decisions\n\n\nOtsego\n62,865\nUse with caution - monitor outcomes\n\n\nMarquette\n63,115\nSafe for algorithmic decisions\n\n\nCass\n65,183\nSafe for algorithmic decisions\n\n\nLenawee\n65,484\nSafe for algorithmic decisions\n\n\nVan Buren\n65,531\nUse with caution - monitor outcomes\n\n\nSt. Clair\n66,887\nSafe for algorithmic decisions\n\n\nKalamazoo\n67,905\nSafe for algorithmic decisions\n\n\nAntrim\n68,850\nSafe for algorithmic decisions\n\n\nEmmet\n69,690\nUse with caution - monitor outcomes\n\n\nCharlevoix\n69,764\nSafe for algorithmic decisions\n\n\nBenzie\n71,327\nUse with caution - monitor outcomes\n\n\nIonia\n71,720\nSafe for algorithmic decisions\n\n\nMonroe\n72,573\nSafe for algorithmic decisions\n\n\nMidland\n73,643\nSafe for algorithmic decisions\n\n\nMacomb\n73,876\nSafe for algorithmic decisions\n\n\nBarry\n75,182\nSafe for algorithmic decisions\n\n\nLapeer\n75,402\nSafe for algorithmic decisions\n\n\nAllegan\n75,543\nSafe for algorithmic decisions\n\n\nGrand Traverse\n75,553\nSafe for algorithmic decisions\n\n\nKent\n76,247\nSafe for algorithmic decisions\n\n\nEaton\n77,158\nSafe for algorithmic decisions\n\n\nLeelanau\n82,345\nUse with caution - monitor outcomes\n\n\nClinton\n82,594\nSafe for algorithmic decisions\n\n\nOttawa\n83,932\nSafe for algorithmic decisions\n\n\nWashtenaw\n84,245\nSafe for algorithmic decisions\n\n\nOakland\n92,620\nSafe for algorithmic decisions\n\n\nLivingston\n96,135\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation:\n\nThe following counties have reliable median income estimates, because the margin of error of the estimate is less than 5% of the estimate itself:\n1 Alcona\n2 Allegan\n3 Alpena\n4 Antrim\n5 Arenac\n6 Barry\n7 Bay\n8 Berrien\n9 Branch\n10 Calhoun\n11 Cass\n12 Charlevoix\n13 Cheboygan\n14 Chippewa\n15 Clare\n16 Clinton\n17 Dickinson\n18 Eaton\n19 Genesee\n20 Gladwin\n21 Grand Traverse 22 Gratiot\n23 Hillsdale\n24 Houghton\n25 Huron\n26 Ingham\n27 Ionia\n28 Isabella\n29 Jackson\n30 Kalamazoo\n31 Kent\n32 Lapeer\n33 Lenawee\n34 Livingston\n35 Macomb\n36 Manistee\n37 Marquette\n38 Mason\n39 Midland\n40 Monroe\n41 Montcalm\n42 Muskegon\n43 Newaygo\n44 Oakland\n45 Oceana\n46 Ogemaw\n47 Osceola\n48 Ottawa\n49 Saginaw\n50 St. Clair\n51 St. Joseph\n52 Sanilac\n53 Shiawassee\n54 Tuscola\n55 Washtenaw\n56 Wayne\n\nCounties requiring additional oversight:\n\nThe following counties’ median income estimates should be used with caution, given that the margin of error is 5 - 10% of the estimate. Any algorithmic decisions based on these estimates should account for the possibility that the median income is up to 10% higher or lower.\n1 Alger\n2 Baraga\n3 Benzie\n4 Crawford\n5 Delta\n6 Emmet\n7 Gogebic\n8 Iosco\n9 Iron\n10 Kalkaska\n11 Lake\n12 Leelanau\n13 Luce\n14 Mackinac\n15 Mecosta\n16 Menominee\n17 Missaukee\n18 Montmorency 19 Ontonagon\n20 Oscoda\n21 Otsego\n22 Presque Isle 23 Roscommon\n24 Van Buren\n25 Wexford\n\nCounties needing alternative approaches:\n\nThe following counties’ median income estimates are low confidence. No algorithmic decisions should be based on them without manual review or additional surveying to insure that the populations are adequately represented.\n1 Keweenaw\n2 Schoolcraft"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#questions-for-further-investigation",
    "href": "labs/lab1/scripts/assignment1_kk.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\nHow come even majority Black census tracts in Kalamazoo have a low confidence estimate compared to a majority White census tract?\nHow racially segregated are census tracts in Michigan in general – i.e. how many tracts are at least 3/4 one race? How does this compare to other states? How does it compare to the past?"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#submission-checklist",
    "href": "labs/lab1/scripts/assignment1_kk.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes",
    "section": "",
    "text": "Anscombe’s Quartet\n\nJust looking at summary can hide important details about trends in the data – all 4 graphs have the same variances, same correlation, and same regression line … wouldn’t realize how different they are unless looking at the graph\nPolicy Implications - policy decisions made based on misunderstood data\n\n73% of planners don’t warn user about data unreliability – violating ethics arguably\n\nCommon problems in data presentation:\n\nmisleading scales, cherry-picking, missing context\nhiding uncertainty (not communicating MOEs) – verrrryyyyyyy tempting to use census block groups with ACS5 because you can … but MOEs can be quite high at this level\n\n\n\n\n\n\n\nData -&gt; Aesthetics -&gt; Geometries -&gt; Visual\n\nData = dataset, df or tibble\nAesthetics = what variables map to visual properties\nGeometries = how to display the data (as points? bars? lines?) and how to decorate/stylize\nadditional layers = scales, themes, facets, annotations\n\nSyntax: -g &lt;- ggplot(data = acs_data) = “I would like to make this plot” … if after running this code you ran g, you would just get a black canvas in viewer\n\nAdding aesthetics: g &lt;- ggplot(data = acs_data) + aes(x=income, y=%_bachelor)\nAdding geometries: g &lt;- ggplot(...) + aes(...) + geom_points(decorate...colors...size...)\n\n\n\n\n\n\nEDA = Exploratory Data Analysis … take the time to get to know your data\n\nLoad and inspect – dimensions, variables types, missing data\nAssess reliability – examine MOEs and calculate coeffs of variation\nvisualize distributions …\n\nFirst steps:\n\nHistogram – distribution of data\nBox plot – finding outliers\n\n\n\n\n\n\nLeft join – preserves ‘left’ tables all ids from identifying category, add data from ‘right’ table based on identifying category but will skip mismatched ids\n\nright join is silly…just use left join and switch order\n\nFull join – combined all ids from each tables’ id category\nInner Join – only retain matching ids from each table\nCan join if columns have different names as long as they have matching values, but matching fields must have same data type"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "title": "Week 3 Notes - Course Introduction",
    "section": "",
    "text": "Main concepts:\nTechnical skills covered:"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#notes",
    "href": "weekly-notes/week-03-notes.html#notes",
    "title": "Week 3 Notes",
    "section": "",
    "text": "Anscombe’s Quartet\n\nJust looking at summary can hide important details about trends in the data – all 4 graphs have the same variances, same correlation, and same regression line … wouldn’t realize how different they are unless looking at the graph\nPolicy Implications - policy decisions made based on misunderstood data\n\n73% of planners don’t warn user about data unreliability – violating ethics arguably\n\nCommon problems in data presentation:\n\nmisleading scales, cherry-picking, missing context\nhiding uncertainty (not communicating MOEs) – verrrryyyyyyy tempting to use census block groups with ACS5 because you can … but MOEs can be quite high at this level\n\n\n\n\n\n\n\nData -&gt; Aesthetics -&gt; Geometries -&gt; Visual\n\nData = dataset, df or tibble\nAesthetics = what variables map to visual properties\nGeometries = how to display the data (as points? bars? lines?) and how to decorate/stylize\nadditional layers = scales, themes, facets, annotations\n\nSyntax: -g &lt;- ggplot(data = acs_data) = “I would like to make this plot” … if after running this code you ran g, you would just get a black canvas in viewer\n\nAdding aesthetics: g &lt;- ggplot(data = acs_data) + aes(x=income, y=%_bachelor)\nAdding geometries: g &lt;- ggplot(...) + aes(...) + geom_points(decorate...colors...size...)\n\n\n\n\n\n\nEDA = Exploratory Data Analysis … take the time to get to know your data\n\nLoad and inspect – dimensions, variables types, missing data\nAssess reliability – examine MOEs and calculate coeffs of variation\nvisualize distributions …\n\nFirst steps:\n\nHistogram – distribution of data\nBox plot – finding outliers\n\n\n\n\n\n\nLeft join – preserves ‘left’ tables all ids from identifying category, add data from ‘right’ table based on identifying category but will skip mismatched ids\n\nright join is silly…just use left join and switch order\n\nFull join – combined all ids from each tables’ id category\nInner Join – only retain matching ids from each table\nCan join if columns have different names as long as they have matching values, but matching fields must have same data type"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nGood pattern for finding census variables :\n\nfilter(str_detect(field, \"str\"))\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\nin the load variables table, “concept” column corresponds to tables"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#given-that-i-chose-three-relatively-low-population-counties-in-a-state-with-a-vary-large-white-majority-the-conclusions-made-about-the-link-between-racial-homogeneity-and-reliability-of-minority-estimates-is-a-limited-conclusion.",
    "href": "labs/lab1/scripts/assignment1_kk.html#given-that-i-chose-three-relatively-low-population-counties-in-a-state-with-a-vary-large-white-majority-the-conclusions-made-about-the-link-between-racial-homogeneity-and-reliability-of-minority-estimates-is-a-limited-conclusion.",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Given that I chose three relatively low population counties in a state with a vary large White majority, the conclusions made about the link between racial homogeneity and reliability of minority estimates is a limited conclusion.",
    "text": "Given that I chose three relatively low population counties in a state with a vary large White majority, the conclusions made about the link between racial homogeneity and reliability of minority estimates is a limited conclusion."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 Notes",
    "section": "",
    "text": "WHERE are these patterns occurring matters\n\nGeographic clustering of problems\nSpatial relationships between communities\nAccess to services and resources is often based on geography\n\n\n\n\n\n\nTwo families of representing world 2-dimensionally\n\nVector\n\ndiscrete objects – things that have definite boundaries\n\nRaster\n\nPixels\nContinuous data\n\n\n\nVector Data Representation - Three basic types of geometric representations - points - lines - polygons - (kinda like illustrator)\nCommon Spatial Data File Formats - Shapefile - Developed by ESRI - Three fundamental objects (need all three to render accurately): - .shp - stores info about the geometry - .shx - shape index - .dbf - names of things - Integrates with tidyverse, follows inernational standards - GeoJSON - All one file ! Little more - KML - Google Earth - Database connections (PostGIS)\nSimple Features - Multi-shapes – like a broken bridge – think multiple shapes for one row of a table - Think Hawaii - -multiple shapes that are all “Hawaii”\nTidyCensus gives you characteristics but no shapes, tigris gives you shapes but no characteristics – must combine\nImportant syntax:\n\n==ggplot geom for mapping: geom_sf()==\n==ggplot use theme_void() to get rid of graph backdrop==\n==st_filter() is like filter() but for spatial filtering rather than simple df`\nst_union() works like arcgis “dissolve”\n\nSpatial Subsetting: - preserves original shapes, not for doing something like finding part of a county. Rather for doing things like finding a county within a list of counties, or all counties that touch a specific county\nneighbors &lt;- pa_counties %&gt;%   st_filter(allegheny, .predicate = st_touches)\n\nIn this example, the terminology “st_touches” is common across all GIS\n.predicate tells st_filter() what kind of relationship to look for, if nothing specified, then st_intersect is default\n\ncheat sheet:\n\n\n\ncheat-sheet\n\n\n\n\n\n\nbasic problem : world is round, maps are flat, projecting from 3d to 2d will necessarily cause some kind of distortion\nEarth is also not perfectly round, but a geoid ( a lil bumpy ), but geoid is mathematically inconvenient, so the Step 1 in projection is we project from he geoid to an ==ellipsoid==, which doesn’t perfectly represent the geoid, but good enough, and smooth\n\nThis means there are multiple ellipsoids that fit the earth, and might fit certain areas better than others\n\n\n\n\n\ncrs\n\n\n\nStep 2 – tie the ellipsoid to the real earth to create a Geographic (Geodetic) Coordinate System i.e. Lat Longs\n\nClark, 1866, uses ‘flattening’ to make a nice lil ellipsoid that is particularly well-fitted to North America – Meades Ranch, Kansas, is where the ellipsoid and geoid smooch :-*\n==North American Datum 1927== or ==NAD27==\nProgress made, made a better ellipsoid than Clarke (“We’re not in Kansas anymore”), based on Earth Center instead of NA.. this one called ==GRS80==\n==WGS84== – yet another used by GPS systems\n\nStep 3 – take the 3d ellipsoid points and project onto 2d surface\n\nCylindrical Projections\n\n\n\n\n\ncylindical-projections\n\n\n    -   Line of tangency -- where the ellipsoid and the projected\n        surface smooch, all other locations will be distorted\n    -   Mercator is prime example\n    -   Bad at preserving sizes of things on 2d surface, but good at\n        preserving angles\n-   Transverse Cylindrical Projection\n    -   Good at preserving up and down (good for Chile, for example)\n    \n\n\n\ntransverse\n\n\n-   Conical Projection\n    -   Good for areas concentrated in a segment of the ellipsoid,\n        like North America\n        \n\n\n\nconical\n\n\n-   Projected Coordinate System\n    -   Localized coordinate system built on a regular,\n        non-distorted grid\n    -   ==UTM== --\n\n\n\nUTM\n\n\n        -   Need to know what zone you are in -- all numbers are\n            based on grid system originating in the lower left\n            corner of the zone\n        -   No negatives\n        -   Who uses UTM ? depends on state -- if looking at a long\n            skinny area like Idaho for example\n    -   ==State Plane== (SPC)\n        -   Used in PA - Two state plane grids in PA, North and\n            South\n\n\n\nstate-plane\n\n\n        -   Some states use Feet, others use Meters -- make sure to\n            check !\n-   SADD -- can't have all 4 in a projection\n    -   Shape\n    -   Area\n    -   Distance\n    -   Direction\n\nst_crs() to check CRS for a dataset\nst_set_crs(data, ####) – set CRS – ONLY IF CRS MISSING, do NOT use to transform CRS (path of despair) – this will not change the numbers themselves but rather how the computer interprets the numbers, only use if st_crs() returns &lt;unknown&gt;\nst_transform() – this will actually recalculate the coordinates from one CRS to another\nIn arcGIS – this is the like the difference between “define projection” and “project”"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#notes",
    "href": "weekly-notes/week-04-notes.html#notes",
    "title": "Week 4 Notes",
    "section": "",
    "text": "WHERE are these patterns occurring matters\n\nGeographic clustering of problems\nSpatial relationships between communities\nAccess to services and resources is often based on geography\n\n\n\n\n\n\nTwo families of representing world 2-dimensionally\n\nVector\n\ndiscrete objects – things that have definite boundaries\n\nRaster\n\nPixels\nContinuous data\n\n\n\nVector Data Representation - Three basic types of geometric representations - points - lines - polygons - (kinda like illustrator)\nCommon Spatial Data File Formats - Shapefile - Developed by ESRI - Three fundamental objects (need all three to render accurately): - .shp - stores info about the geometry - .shx - shape index - .dbf - names of things - Integrates with tidyverse, follows inernational standards - GeoJSON - All one file ! Little more - KML - Google Earth - Database connections (PostGIS)\nSimple Features - Multi-shapes – like a broken bridge – think multiple shapes for one row of a table - Think Hawaii - -multiple shapes that are all “Hawaii”\nTidyCensus gives you characteristics but no shapes, tigris gives you shapes but no characteristics – must combine\nImportant syntax:\n\n==ggplot geom for mapping: geom_sf()==\n==ggplot use theme_void() to get rid of graph backdrop==\n==st_filter() is like filter() but for spatial filtering rather than simple df`\nst_union() works like arcgis “dissolve”\n\nSpatial Subsetting: - preserves original shapes, not for doing something like finding part of a county. Rather for doing things like finding a county within a list of counties, or all counties that touch a specific county\nneighbors &lt;- pa_counties %&gt;%   st_filter(allegheny, .predicate = st_touches)\n\nIn this example, the terminology “st_touches” is common across all GIS\n.predicate tells st_filter() what kind of relationship to look for, if nothing specified, then st_intersect is default\n\ncheat sheet:\n\n\n\ncheat-sheet\n\n\n\n\n\n\nbasic problem : world is round, maps are flat, projecting from 3d to 2d will necessarily cause some kind of distortion\nEarth is also not perfectly round, but a geoid ( a lil bumpy ), but geoid is mathematically inconvenient, so the Step 1 in projection is we project from he geoid to an ==ellipsoid==, which doesn’t perfectly represent the geoid, but good enough, and smooth\n\nThis means there are multiple ellipsoids that fit the earth, and might fit certain areas better than others\n\n\n\n\n\ncrs\n\n\n\nStep 2 – tie the ellipsoid to the real earth to create a Geographic (Geodetic) Coordinate System i.e. Lat Longs\n\nClark, 1866, uses ‘flattening’ to make a nice lil ellipsoid that is particularly well-fitted to North America – Meades Ranch, Kansas, is where the ellipsoid and geoid smooch :-*\n==North American Datum 1927== or ==NAD27==\nProgress made, made a better ellipsoid than Clarke (“We’re not in Kansas anymore”), based on Earth Center instead of NA.. this one called ==GRS80==\n==WGS84== – yet another used by GPS systems\n\nStep 3 – take the 3d ellipsoid points and project onto 2d surface\n\nCylindrical Projections\n\n\n\n\n\ncylindical-projections\n\n\n    -   Line of tangency -- where the ellipsoid and the projected\n        surface smooch, all other locations will be distorted\n    -   Mercator is prime example\n    -   Bad at preserving sizes of things on 2d surface, but good at\n        preserving angles\n-   Transverse Cylindrical Projection\n    -   Good at preserving up and down (good for Chile, for example)\n    \n\n\n\ntransverse\n\n\n-   Conical Projection\n    -   Good for areas concentrated in a segment of the ellipsoid,\n        like North America\n        \n\n\n\nconical\n\n\n-   Projected Coordinate System\n    -   Localized coordinate system built on a regular,\n        non-distorted grid\n    -   ==UTM== --\n\n\n\nUTM\n\n\n        -   Need to know what zone you are in -- all numbers are\n            based on grid system originating in the lower left\n            corner of the zone\n        -   No negatives\n        -   Who uses UTM ? depends on state -- if looking at a long\n            skinny area like Idaho for example\n    -   ==State Plane== (SPC)\n        -   Used in PA - Two state plane grids in PA, North and\n            South\n\n\n\nstate-plane\n\n\n        -   Some states use Feet, others use Meters -- make sure to\n            check !\n-   SADD -- can't have all 4 in a projection\n    -   Shape\n    -   Area\n    -   Distance\n    -   Direction\n\nst_crs() to check CRS for a dataset\nst_set_crs(data, ####) – set CRS – ONLY IF CRS MISSING, do NOT use to transform CRS (path of despair) – this will not change the numbers themselves but rather how the computer interprets the numbers, only use if st_crs() returns &lt;unknown&gt;\nst_transform() – this will actually recalculate the coordinates from one CRS to another\nIn arcGIS – this is the like the difference between “define projection” and “project”"
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html",
    "href": "labs/lab2/assignment2_kk.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives:\n\nApply spatial operations to answer policy-relevant research questions\nIntegrate census demographic data with spatial analysis\nCreate publication-quality visualizations and maps\nWork with spatial data from multiple sources\nCommunicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html#assignment-overview",
    "href": "labs/lab2/assignment2_kk.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives:\n\nApply spatial operations to answer policy-relevant research questions\nIntegrate census demographic data with spatial analysis\nCreate publication-quality visualizations and maps\nWork with spatial data from multiple sources\nCommunicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "labs/lab2/assignment2_kk.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data:\n\nPennsylvania county boundaries\nPennsylvania hospitals (from lecture data)\nPennsylvania census tracts\n\nYour Task:\n\n# Load spatial data\npa_counties &lt;- st_read(\"data/Pennsylvania_County_Boundaries.shp\")\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `C:\\Users\\knoxk\\OneDrive\\Documents\\musa-5080\\portfolio-setup-kkxix\\labs\\lab2\\data\\Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nhospitals &lt;- st_read(\"data/hospitals.geojson\")\n\nReading layer `hospitals' from data source \n  `C:\\Users\\knoxk\\OneDrive\\Documents\\musa-5080\\portfolio-setup-kkxix\\labs\\lab2\\data\\hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\ncensus_tracts &lt;- st_transform(census_tracts, st_crs(pa_counties))\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\n\n# Check that all data loaded correctly\nglimpse(pa_counties)\n\nRows: 67\nColumns: 20\n$ OBJECTID   &lt;int&gt; 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347,…\n$ MSLINK     &lt;int&gt; 46, 8, 9, 58, 59, 60, 62, 63, 42, 43, 44, 47, 48, 49, 50, 5…\n$ COUNTY_NAM &lt;chr&gt; \"MONTGOMERY\", \"BRADFORD\", \"BUCKS\", \"TIOGA\", \"UNION\", \"VENAN…\n$ COUNTY_NUM &lt;chr&gt; \"46\", \"08\", \"09\", \"58\", \"59\", \"60\", \"62\", \"63\", \"42\", \"43\",…\n$ FIPS_COUNT &lt;chr&gt; \"091\", \"015\", \"017\", \"117\", \"119\", \"121\", \"125\", \"127\", \"08…\n$ COUNTY_ARE &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ COUNTY_PER &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ NUMERIC_LA &lt;int&gt; 5, 2, 5, 2, 2, 3, 1, 2, 1, 3, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1,…\n$ COUNTY_N_1 &lt;int&gt; 46, 8, 9, 58, 59, 60, 62, 63, 42, 43, 44, 47, 48, 49, 50, 5…\n$ AREA_SQ_MI &lt;dbl&gt; 487.4271, 1161.3379, 622.0836, 1137.2480, 319.1893, 683.367…\n$ SOUND      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ SPREAD_SHE &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ IMAGE_NAME &lt;chr&gt; \"poll.bmp\", \"poll.bmp\", \"poll.bmp\", \"poll.bmp\", \"poll.bmp\",…\n$ NOTE_FILE  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ VIDEO      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ DISTRICT_N &lt;chr&gt; \"06\", \"03\", \"06\", \"03\", \"03\", \"01\", \"12\", \"04\", \"02\", \"01\",…\n$ PA_CTY_COD &lt;chr&gt; \"46\", \"08\", \"09\", \"59\", \"60\", \"61\", \"63\", \"64\", \"42\", \"43\",…\n$ MAINT_CTY_ &lt;chr&gt; \"4\", \"9\", \"1\", \"7\", \"8\", \"5\", \"4\", \"6\", \"5\", \"4\", \"7\", \"3\",…\n$ DISTRICT_O &lt;chr&gt; \"6-4\", \"3-9\", \"6-1\", \"3-7\", \"3-8\", \"1-5\", \"12-4\", \"4-6\", \"2…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((-8398884 48..., MULTIPOLYGON (…\n\nglimpse(census_tracts)\n\nRows: 3,445\nColumns: 14\n$ STATEFP    &lt;chr&gt; \"42\", \"42\", \"42\", \"42\", \"42\", \"42\", \"42\", \"42\", \"42\", \"42\",…\n$ COUNTYFP   &lt;chr&gt; \"001\", \"013\", \"013\", \"013\", \"013\", \"011\", \"011\", \"011\", \"01…\n$ TRACTCE    &lt;chr&gt; \"031101\", \"100400\", \"100500\", \"100800\", \"101900\", \"011200\",…\n$ GEOIDFQ    &lt;chr&gt; \"1400000US42001031101\", \"1400000US42013100400\", \"1400000US4…\n$ GEOID      &lt;chr&gt; \"42001031101\", \"42013100400\", \"42013100500\", \"42013100800\",…\n$ NAME       &lt;chr&gt; \"311.01\", \"1004\", \"1005\", \"1008\", \"1019\", \"112\", \"2\", \"115\"…\n$ NAMELSAD   &lt;chr&gt; \"Census Tract 311.01\", \"Census Tract 1004\", \"Census Tract 1…\n$ STUSPS     &lt;chr&gt; \"PA\", \"PA\", \"PA\", \"PA\", \"PA\", \"PA\", \"PA\", \"PA\", \"PA\", \"PA\",…\n$ NAMELSADCO &lt;chr&gt; \"Adams County\", \"Blair County\", \"Blair County\", \"Blair Coun…\n$ STATE_NAME &lt;chr&gt; \"Pennsylvania\", \"Pennsylvania\", \"Pennsylvania\", \"Pennsylvan…\n$ LSAD       &lt;chr&gt; \"CT\", \"CT\", \"CT\", \"CT\", \"CT\", \"CT\", \"CT\", \"CT\", \"CT\", \"CT\",…\n$ ALAND      &lt;dbl&gt; 3043185, 993724, 1130204, 996553, 573726, 1539365, 1949529,…\n$ AWATER     &lt;dbl&gt; 0, 0, 0, 0, 0, 9308, 159015, 12469, 0, 0, 0, 1271, 6352, 74…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((-8575060 48..., MULTIPOLYGON (…\n\nglimpse(hospitals)\n\nRows: 223\nColumns: 12\n$ CHIEF_EXEC &lt;chr&gt; \"Peter J Adamo\", \"Autumn DeShields\", \"Shawn Parekh\", \"DIANE…\n$ CHIEF_EX_1 &lt;chr&gt; \"President\", \"Chief Executive Officer\", \"Chief Executive Of…\n$ FACILITY_U &lt;chr&gt; \"https://www.phhealthcare.org\", \"https://www.malvernbh.com\"…\n$ LONGITUDE  &lt;dbl&gt; -79.91131, -75.17005, -75.20963, -80.27907, -79.02513, -75.…\n$ COUNTY     &lt;chr&gt; \"Washington\", \"Philadelphia\", \"Philadelphia\", \"Washington\",…\n$ FACILITY_N &lt;chr&gt; \"Penn Highlands Mon Valley\", \"MALVERN BEHAVIORAL HEALTH\", \"…\n$ STREET     &lt;chr&gt; \"1163 Country Club Road\", \"1930 South Broad Street Unit 4\",…\n$ CITY_OR_BO &lt;chr&gt; \"Monongahela\", \"Philadelphia\", \"Philadelphia\", \"WASHINGTON\"…\n$ LATITUDE   &lt;dbl&gt; 40.18193, 39.92619, 40.02869, 40.15655, 39.80913, 40.24273,…\n$ TELEPHONE_ &lt;chr&gt; \"724-258-1000\", \"610-480-8919\", \"215-483-9900\", \"7248840710…\n$ ZIP_CODE   &lt;chr&gt; \"15063\", \"19145\", \"19128\", \"15301\", \"15552\", \"19464\", \"1776…\n$ geometry   &lt;POINT [m]&gt; POINT (-8895686 4892415), POINT (-8367892 4855223), P…\n\n# With ggplot2\np1 &lt;- ggplot(pa_counties) +\n  geom_sf() +\n  theme_void()\n\np2 &lt;- ggplot(census_tracts) +\n  geom_sf() +\n  theme_void()\n\np3 &lt;- ggplot(hospitals) +\n  geom_sf() +\n  theme_void()\n\np1 | p2 | p3\n\n\n\n\n\n\n\n\nQuestions to answer:\n\nHow many hospitals are in your dataset?\n\n223\n\nHow many census tracts?\n\n3445\n\nWhat coordinate reference system is each dataset in?\n\nI transformed hospitals and census tracts to PA counties CRS, which is WGS84. Before that, hospitals were also in WGS84, but census tracts were in NAD83.\n\n\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables:\n\nTotal population\nMedian household income\nPopulation 65 years and over (you may need to sum multiple age categories)\n\nYour Task:\n\n# Get demographic data from ACS\npa_tracts_data &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    median_income = \"B19013_001\",\n    total_pop = \"B01003_001\",\n    over_65 = \"B01001_020\"  # Population 65 years and over\n  ),\n  state = \"PA\",\n  year = 2023,\n  output = \"wide\"\n)\n\n# Join to tract boundaries\ncensus_tracts &lt;- census_tracts %&gt;%\n  left_join(pa_tracts_data, by= \"GEOID\")\n\nsummary(census_tracts)\n\n   STATEFP            COUNTYFP           TRACTCE            GEOIDFQ         \n Length:3445        Length:3445        Length:3445        Length:3445       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    GEOID              NAME.x            NAMELSAD            STUSPS         \n Length:3445        Length:3445        Length:3445        Length:3445       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  NAMELSADCO         STATE_NAME            LSAD               ALAND          \n Length:3445        Length:3445        Length:3445        Min.   :7.224e+04  \n Class :character   Class :character   Class :character   1st Qu.:1.320e+06  \n Mode  :character   Mode  :character   Mode  :character   Median :4.183e+06  \n                                                          Mean   :3.364e+07  \n                                                          3rd Qu.:2.858e+07  \n                                                          Max.   :1.024e+09  \n                                                                             \n     AWATER            NAME.y          median_incomeE   median_incomeM  \n Min.   :       0   Length:3445        Min.   : 13307   Min.   :   472  \n 1st Qu.:       0   Class :character   1st Qu.: 57864   1st Qu.:  9324  \n Median :   13905   Mode  :character   Median : 72944   Median : 13780  \n Mean   :  433440                      Mean   : 80731   Mean   : 16219  \n 3rd Qu.:  240806                      3rd Qu.: 96691   3rd Qu.: 20141  \n Max.   :29792870                      Max.   :250001   Max.   :208341  \n                                       NA's   :65       NA's   :73      \n   total_popE      total_popM        over_65E         over_65M     \n Min.   :    0   Min.   :   3.0   Min.   :  0.00   Min.   :  2.00  \n 1st Qu.: 2616   1st Qu.: 248.0   1st Qu.: 19.00   1st Qu.: 19.00  \n Median : 3640   Median : 375.0   Median : 39.00   Median : 31.00  \n Mean   : 3770   Mean   : 402.7   Mean   : 47.59   Mean   : 39.76  \n 3rd Qu.: 4810   3rd Qu.: 527.0   3rd Qu.: 66.00   3rd Qu.: 51.00  \n Max.   :10406   Max.   :2091.0   Max.   :324.00   Max.   :342.00  \n                                                                   \n          geometry   \n MULTIPOLYGON :3445  \n epsg:3857    :   0  \n +proj=merc...:   0  \n                     \n                     \n                     \n                     \n\n\nQuestions to answer:\n\nWhat year of ACS data are you using?\n\n2019-2023 5-year ACS data\n\nHow many tracts have missing income data?\n\n65\n\nWhat is the median income across all PA census tracts?\n\n$72,944\n\n\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria:\n\nLow median household income (choose an appropriate threshold)\nSignificant elderly population (choose an appropriate threshold)\n\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\nvulnerable_tracts &lt;- census_tracts %&gt;%\n  filter(median_incomeE &lt; 42398 | (over_65E/total_popE) &gt; .017)\n\nQuestions to answer:\n\nWhat income threshold did you choose and why?\n\nI looked at the 2023 poverty guidelines, the average household size in PA in 2023, which was 2.4, and then looked into eligibility for various programs, which is anywhere from 133% FPIG to 215%. I am going with vulnerable household is 215% or less of the 2-person household FPIG, or $42,398.\n\nWhat elderly population threshold did you choose and why?\n\nIn order to control for overall population size, instead of looking at the raw number of elderly, I looked at the over 65 as percent of total population. I defined vulnerable as any tract in the 75th percentile of elderly percent of tract population (1.7% or above).\n\nHow many tracts meet your vulnerability criteria?\n\n1,145\n\nWhat percentage of PA census tracts are considered vulnerable by your definition?\n\n33.2% – about a third.\n\n\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n#convert to Albers\nvulnerable_tracts &lt;- st_transform(vulnerable_tracts, crs = 5070)\nhospitals &lt;- st_transform(hospitals, crs = 5070)\n\n# Calculate distance from each tract centroid to nearest hospital\ntract_centroids &lt;- st_centroid(vulnerable_tracts)\nnearest_hospital &lt;- st_nearest_feature(tract_centroids, hospitals)\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(nearest_hospital_geom = hospitals$geometry[nearest_hospital])\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(distance_to_nearst_hospital = set_units(st_distance(tract_centroids, vulnerable_tracts$nearest_hospital_geom, by_element = TRUE), \"mi\"))\n\nsummary(vulnerable_tracts$distance_to_nearst_hospital) \n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 0.02107  1.21850  2.75761  4.37494  6.02931 29.75156 \n\n\nRequirements:\n\nUse an appropriate projected coordinate system for Pennsylvania\nCalculate distances in miles\nExplain why you chose your projection\n\nI chose Albers Conical Equal Area, NAD 83 projection based on this PA document from 2011’s recommendation.\nQuestions to answer:\n\nWhat is the average distance to the nearest hospital for vulnerable tracts?\n\n4.4 miles\n\nWhat is the maximum distance?\n\n29.8 miles\n\nHow many vulnerable tracts are more than 15 miles from the nearest hospital?\n\n33\n\n\n\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n# Create underserved variable\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(underserved = ifelse(as.numeric(distance_to_nearst_hospital) &gt; 15,\"Underserved\",\"\"))\n\nvulnerable_tracts %&gt;%\n  group_by(underserved)%&gt;%\n  summarise(n())\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1272296 ymin: 1971050 xmax: 1776268 ymax: 2294525\nProjected CRS: NAD83 / Conus Albers\n# A tibble: 2 × 3\n  underserved   `n()`                                                   geometry\n  &lt;chr&gt;         &lt;int&gt;                                         &lt;MULTIPOLYGON [m]&gt;\n1 \"\"             1112 (((1324165 2015512, 1324049 2015276, 1323967 2015195, 132…\n2 \"Underserved\"    33 (((1391776 2000059, 1392091 2000026, 1392335 1999901, 139…\n\n\nQuestions to answer:\n\nHow many tracts are under-served?\n\n33\n\nWhat percentage of vulnerable tracts are under-served?\n\n3%\n\nDoes this surprise you? Why or why not?\n\nUnfortunately, this does not surprise me, given how rural certain areas of Pennsylvania are and knowing how rural healthcare is a persistent issue in the US in general. However, it would surprise me if some of these tracts are in urban areas.\n\n\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\n# Spatial join tracts to counties\npa_counties &lt;- pa_counties %&gt;% st_transform(st_crs(vulnerable_tracts))\n\nvulnerable_per_county &lt;- vulnerable_tracts %&gt;%\n  st_join(pa_counties) %&gt;%\n  st_drop_geometry()\n\n# Aggregate statistics by county\nvulnerable_per_county_stats &lt;- vulnerable_per_county %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarise(\n    num_vulnerable_tracts = n(),\n    num_underserved_tracts = sum(underserved == \"Underserved\"),\n    percent_underserved = sum(underserved == \"Underserved\")/n(),\n    avg_distance_to_nearst_hospital = mean(distance_to_nearst_hospital),\n    total_pop = sum(total_popE),\n    total_underserved_pop = sum(ifelse(underserved == \"Underserved\", total_popE, 0))\n  )\n\nRequired county-level statistics:\n\nNumber of vulnerable tracts\nNumber of underserved tracts\n\nPercentage of vulnerable tracts that are underserved\nAverage distance to nearest hospital for vulnerable tracts\nTotal vulnerable population\n\nQuestions to answer:\n\nWhich 5 counties have the highest percentage of underserved vulnerable tracts?\n\nCameron\nJuniata\nPotter\nSnyder\nSullivan\n\nWhich counties have the most vulnerable people living far from hospitals?\n\nClearfield\nChester\nJuniata\nSnyder\nPike\n\nAre there any patterns in where underserved counties are located?\n\nThere are a lot of quite rural underserved counties, like Sullivan, Juniata, and Clearfield, but there are also suburban counties of major PA cities like Chester County outside of Philadelphia and Cumberland County outside Harrisburg.\n\n\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n# Create and format priority counties table\nvulnerable_per_county_stats %&gt;% \n  arrange(desc(total_underserved_pop)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  select(\n    c(\"COUNTY_NAM\", \"total_underserved_pop\", \"percent_underserved\", \"avg_distance_to_nearst_hospital\")\n  ) %&gt;%\n  mutate(\n    avg_distance_to_nearst_hospital = as.numeric(avg_distance_to_nearst_hospital),\n    percent_underserved = paste0(round(percent_underserved*100, 2), \"%\")\n  )%&gt;%\n  kable(\n    col.names = c(\"County\", \"Total Underserved Population\", \"Percent of Vulnerable Tracts that are Underserved\", \"Average Distance of to Nearest Hospital\"),\n    digit = 1,\n    format.args = list(big.mark = \",\"),\n    align = \"l\",\n    caption = \"10 Counties in PA with the Highest Absolute Population more than 15 miles from Nearest Hospital\"\n  )\n\n\n10 Counties in PA with the Highest Absolute Population more than 15 miles from Nearest Hospital\n\n\n\n\n\n\n\n\nCounty\nTotal Underserved Population\nPercent of Vulnerable Tracts that are Underserved\nAverage Distance of to Nearest Hospital\n\n\n\n\nCLEARFIELD\n17,027\n28.57%\n12.4\n\n\nCHESTER\n15,467\n7.5%\n6.1\n\n\nJUNIATA\n13,955\n50%\n15.4\n\n\nSNYDER\n12,073\n50%\n15.1\n\n\nPIKE\n10,292\n35.29%\n15.4\n\n\nDAUPHIN\n8,815\n8.33%\n5.3\n\n\nSCHUYLKILL\n8,815\n7.41%\n7.0\n\n\nPERRY\n8,761\n22.22%\n12.3\n\n\nLANCASTER\n8,055\n5.71%\n5.7\n\n\nCENTRE\n6,843\n5.88%\n6.2\n\n\n\n\n\nRequirements:\n\nUse knitr::kable() or similar for formatting\nInclude descriptive column names\nFormat numbers appropriately (commas for population, percentages, etc.)\nAdd an informative caption\nSort by priority (you decide the metric)"
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html#part-2-comprehensive-visualization",
    "href": "labs/lab2/assignment2_kk.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\n# Create county-level access map\nvulnerable_per_county_stats &lt;- vulnerable_per_county_stats %&gt;%\n  left_join(pa_counties, by=\"COUNTY_NAM\")\n\nvulnerable_per_county_stats &lt;- vulnerable_per_county_stats %&gt;%\n  st_as_sf() %&gt;% \n  st_transform(st_crs(census_tracts)) %&gt;%\n  mutate(percent_underserved = percent_underserved*100\n  )\n\nhospitals &lt;- hospitals %&gt;%\n  mutate(type = \"Hospital\")\n\nggplot(vulnerable_per_county_stats) +\n  geom_sf(aes(fill = percent_underserved)) +\n  scale_fill_gradient(\n    low = \"#C3CDFE\",   \n    high = \"#485EFE\",   \n    name = \"% Underserved Tracts\"\n  ) +\n  new_scale_fill()+\n  geom_sf(\n    data = hospitals, \n    aes(fill=type),      \n    size = 3, \n    shape = 21, \n    color = \"#FF8600\",     \n    stroke = 0.8\n  ) +\n  scale_fill_manual(\n    values = c(\"Hospital\" = \"#FF8600\"),\n    name = NULL\n  )+\n  labs(\n    title = \"Underserved Tracts per Counties in Pennsylvania\",\n    subtitle = str_wrap(\"Where an underserved tract is one where the center is at least 15 miles from the nearest hospital\",50),\n    caption =\"Data Sources: US Census Data, OpenDataPhilly\"\n  ) +\n  annotation_north_arrow(             \n    location = \"tr\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    legend.direction = \"vertical\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\nRequirements: - Fill counties by percentage of vulnerable tracts that are underserved - Include hospital locations as points - Use an appropriate color scheme - Include clear title, subtitle, and caption - Use theme_void() or similar clean theme - Add a legend with formatted labels\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\n# Create detailed tract-level map\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  st_as_sf() %&gt;% \n  st_transform(st_crs(census_tracts))\n\nunderserved_tracts &lt;- vulnerable_tracts[vulnerable_tracts$underserved==\"Underserved\",]\n\npa_counties$legend &lt;- \"County Boundary\"\n\nggplot(census_tracts) +\n  geom_sf(\n    color=\"darkgrey\",\n    size=.5\n  )+\n  geom_sf(\n    data=underserved_tracts,\n    aes(fill = underserved)\n  )+\n  geom_sf(\n    data=pa_counties,\n    aes(color=legend),     \n    linewidth = .8,\n    fill=NA\n  )+\n  geom_sf(\n    data=hospitals,\n    aes(fill = type),      \n    size = 1, \n    shape = 21, \n    color = \"#FF8600\",     \n    stroke = 0.8\n  )+\n  scale_fill_manual(\n    values = c(\"Underserved\" = \"#485EFE\",\n               \"Hospital\" = \"#FF8600\"),\n    name = NULL  # or \"Legend\" if you want a title\n  ) +\n  scale_color_manual(\n    values = c(\"County Boundary\" = \"black\"),\n    name = NULL\n  ) +\n  annotation_north_arrow(             \n    location = \"tr\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void()+\n  labs(\n    title = \"Underserved Tracts in Pennsylvania\",\n    subtitle = str_wrap(\"Where an underserved tract's center is at least 15 miles from the nearest hospital\",50),\n    caption =\"Data Sources: US Census Data, OpenDataPhilly\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\nRequirements: - Show underserved vulnerable tracts in a contrasting color - Include county boundaries for context - Show hospital locations - Use appropriate visual hierarchy (what should stand out?) - Include informative title and subtitle\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create distribution visualization\n\ndep_regions&lt;- st_read(\"data/DEPRegions2024_03.shp\")\n\nReading layer `DEPRegions2024_03' from data source \n  `C:\\Users\\knoxk\\OneDrive\\Documents\\musa-5080\\portfolio-setup-kkxix\\labs\\lab2\\data\\DEPRegions2024_03.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -8963379 ymin: 4825316 xmax: -8314404 ymax: 5201420\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ndep_regions &lt;- dep_regions %&gt;% st_transform(st_crs(vulnerable_tracts))\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;% \n  mutate(\n      urban_rural = case_when(\n      total_popE &gt;= 5000 ~ \"Urban\",\n      TRUE ~ \"Rural\"\n      )\n  )\n\nvulnerable_tracts_with_regions &lt;- vulnerable_tracts %&gt;%\n  st_join(dep_regions) %&gt;%\n  st_drop_geometry()\n\nvulnerable_tracts_with_regions &lt;- vulnerable_tracts_with_regions %&gt;%\n  mutate(\n    Region = case_when(\n      SNAME == \"NCRO\"~\"North Central\",\n      SNAME ==\"NERO\"~\"North East\",\n      SNAME==\"NWRO\"~\"North West\",\n      SNAME==\"SCRO\"~\"South Central\",\n      SNAME==\"SERO\"~\"South East\",\n      SNAME==\"SWRO\"~\"South West\"\n    )\n  )\n  \n  \nggplot(vulnerable_tracts_with_regions)+\n  geom_boxplot(\n    aes(x=Region, y=distance_to_nearst_hospital)\n  )+\n  labs(\n    title=\"Distance of Vulnerable Tracts to Nearest Hospital by Region\",\n    caption=str_wrap(\"Where a vulnerable tract is in the top 25% of concentration of elderly population or 215% of the Federal Poverty Line for 2-person household. Data sources: US Census, Pennsylvania Spatial Data Access  -- Department of Environmental Protection Regions\",100),\n    y=str_wrap(\"Distance from Center of Tract to Nearest Hospital\",50),\n    x=\"Region of Tract\"\n  )+\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\nNorthern Pennsylvania vulnerable populations generally are location farther from hospitals than Southern Pennsylvania, with North Central PA having the highest median and upper quartiles of distance to the nearest hospital of all 6 regions. However another trend we can see is that the highest number of outliers fall within the South East and South West regions, home to the two biggest cities in PA, Philadelphia and Pittsburgh, respectively. That shows that these regions have more tracts that are facing very different needs that the middle 50% of the region.\nSuggested chart types: - Histogram or density plot of distances - Box plot comparing distances across regions - Bar chart of underserved tracts by county - Scatter plot of distance vs. vulnerable population size\nRequirements: - Clear axes labels with units - Appropriate title - Professional formatting - Brief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html#part-3-bring-your-own-data-analysis",
    "href": "labs/lab2/assignment2_kk.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\n\n\n\nYour Analysis\n\nDigital Justice\nOption C: Digital Equity\n\nData: Census Broadband access, device access, and income, Philadelphia free wifi spots from OpenDataPhilly\nQuestion: “Do digitally disadvantaged neighborhoods have equitable access to city internet?”\nOperations: Buffer free wifi spots with a computer (10-minute walk = 0.5 mile), calculate connectivity by tract, determine digitally vulnerable tracts from census data and underserved tracts by distance from buffers.\nPolicy relevance: Digital equity, broadband infrastructure, internet-connected device access\n\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load your additional dataset\nv21 &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n\n# Search for broadband and computer-related variables\nbb_vars &lt;- v21 %&gt;% filter(str_detect(label, \"broadband\"))\ncomp_vars &lt;-v21 %&gt;% filter(str_detect(label, \"computer\"))\n\n#B28003_002 = total with computer\n#B28002_004 = total with broadband internet subscription\n\nphilly_digital_access &lt;-  get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    has_computer = \"B28003_002\", \n    has_broadband = \"B28002_004\"\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2023,\n  output = \"wide\"\n)\n\nfree_wifi_spots &lt;- st_read(\"data/free_city_wifi_locations.shp\")%&gt;%st_transform(2272)\n\nReading layer `free_city_wifi_locations' from data source \n  `C:\\Users\\knoxk\\OneDrive\\Documents\\musa-5080\\portfolio-setup-kkxix\\labs\\lab2\\data\\free_city_wifi_locations.shp' \n  using driver `ESRI Shapefile'\nreplacing null geometries with empty geometries\nSimple feature collection with 254 features and 47 fields (with 1 geometry empty)\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8378350 ymin: 4852065 xmax: -8345555 ymax: 4883809\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nphilly_digital_access &lt;- left_join(\n  philly_digital_access,\n  census_tracts,\n  by=\"GEOID\"\n) %&gt;%\n  st_as_sf() %&gt;%\n  st_transform(2272)\n\nggplot(philly_digital_access)+\n  geom_sf()+\n  geom_sf(\n    data=free_wifi_spots\n  )+\n  theme_void()\n\n\n\n\n\n\n\n\nQuestions to answer:\n\nWhat dataset did you choose and why?\n\nI choose the ACS variables showing whether a household has broadband access and whether they have a computer, as well as OpenDataPhilly’s database of free wi-fi locations, which inlcudes a variable showing how many public use computers are available at the location.\n\nWhat is the data source and date?\n\nThe ACS data is 5-year data from 2018 - 2023, and the wi-fi locations are from 2024, so the home broadband and computer access data may be slightly outdated. Also important to note is 2018 - 2023 range includes the pandemic, a period in which there was unprecedented investment in home internet subsidies that have expired in 2024 .\n\nHow many features does it contain?\n\nThere are 254 free-wifi locations, and 408 census tracts in Philadelphia.\n\nWhat CRS is it in? Did you need to transform it?\n\nBefore transformation, both datasets were in web mercator, EPSG 3857, and since Philly falls within southern pennsylvania, I transformed it to Pennsylvania South state plane projection, EPSG 2272.\n\n\n\nPose a research question"
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "labs/lab2/assignment2_kk.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nTake a few moments to clean up your markdown document and then write a line or two or three about how you may have incorporated feedback that you recieved after your first assignment."
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html#submission-requirements",
    "href": "labs/lab2/assignment2_kk.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 5 Notes",
    "section": "",
    "text": "General Problem: We observe data and we believe there’s some relationship between these variables. Statistical Learning = a set of approaches for estimating that relationship.\nFormalizing the Relationship: For response Y and predicators X_1, X_2,...X_n: Y = f(X) + E where E is error term.\nWhat is f()? – the true relationship between predictors and outcome, fixed but unknown\n\nWe want to know f() not only to predict Y, but to learn about/infer relationship between Xs\nTwo broad approaches:\n\nParametric\n\nmake and assumption about the form, i.e. predefine the shape the the slope, easier to interpret, reduces problem to estimating a few parameters\nunintuitively perhaps, modern AI is parametric, just with an insane number of parameters\n\nNon-parametric\n\ndoesn’t assume a form, it’s about find the best bit line, requires more data and is a bit harder to interpret\n\n\n\n\n\n\n\nAssumption: Relationship between x and y is linear\nTask : What are coefficients B in Y = B_1X_1 + B_2X_2 ...\nMethod: Ordinary Least Squares\nWhy? Simple, well-understood\nIn R : lm(Y ~ X, data = data)\n\nInterpreting coefficients:\n\nB_0 is intercept, i.e. what is dependent variable when independent is 0\nB_1 is slope\nRecall, all stats is based on sample, we never have full universe of data.\nTrue relationship is unknowable, but based on different samples, we may get relationships more or less close to unknowable true relationship.\nNull hypothesis: The relationship actually 0, we get absolutely no information about Y from X.\nSo, “statistical significance” is saying there is enough evidence to disprove null hypothesis\nHow we do this: how weird would the data sample have to be for null hypothesis to be true?\nIs relationship real? p-value is the probability that the null hypothesis is true given the sample. We are looking for small p-values.\n\n\n\n\n\nFor inference, how explanatory is model relationship between X and Y – R-squared tells us how fitting the line is to sample.\nFor prediction, how well would model fit new data? R-squared alone does not tell us if model is trustworthy\noverfitting – model is too good at predicting one sample, R-squared is crazy good, “following noise” essentially, won’t generalize well though to other samples\nunderfitting – model ignores the relationship entirely, low R-squared\nperfect fit – “the gist of things”\n\n\n\n\n\nSolution: hold out some data to test predictions\n\nsay we have 67 counties in PA, train model on 70% of sample, test on 30%\n\nset.seed() will preserve random selection if you run code multiple times\n\n\n\n\n\nRoot means square errors, RMSE – i.e. “our predictions are off the real observations by RMSE units” – is this good enough?\n\n\n\n\n\nhow well does models apply to new datasets?\nK-fold cross-validation - fold the 30/70 split at different segments and run train/test on each one, average RMSE across all\nin R: library(caret) functions trainControl() sets a cross-validation method with number of folds, train() creates mode based on data and the object returned from trainControl()\n\n\n\n\n\nLinear regression makes assumptions, if violated:\n\ncoefficients may be biased\nSE may be wrong\n\nheteroscedasticity – variance changes across X\nformal test: Breusch-Pagan test, p&lt;.05 bad\n\n\n\n\n\n\nheteroscedasticity\n\n\npredictions unreliable\nNormality of residuals - Solution Q-Q Plot\nno multicollinearity - vif()\noutliers\n\n\n\n\nadd more features/variables\ncreate categorical variables\n\n\n\n\n\nunderstand the framework\nvisualize first\nfit the model\nevaluate performance (cross validation)\ncheck assumptions\nimprove model if needed\nconsider ethics (who could be harmed?)"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#notes",
    "href": "weekly-notes/week-05-notes.html#notes",
    "title": "Week 5 Notes",
    "section": "",
    "text": "General Problem: We observe data and we believe there’s some relationship between these variables. Statistical Learning = a set of approaches for estimating that relationship.\nFormalizing the Relationship: For response Y and predicators X_1, X_2,...X_n: Y = f(X) + E where E is error term.\nWhat is f()? – the true relationship between predictors and outcome, fixed but unknown\n\nWe want to know f() not only to predict Y, but to learn about/infer relationship between Xs\nTwo broad approaches:\n\nParametric\n\nmake and assumption about the form, i.e. predefine the shape the the slope, easier to interpret, reduces problem to estimating a few parameters\nunintuitively perhaps, modern AI is parametric, just with an insane number of parameters\n\nNon-parametric\n\ndoesn’t assume a form, it’s about find the best bit line, requires more data and is a bit harder to interpret\n\n\n\n\n\n\n\nAssumption: Relationship between x and y is linear\nTask : What are coefficients B in Y = B_1X_1 + B_2X_2 ...\nMethod: Ordinary Least Squares\nWhy? Simple, well-understood\nIn R : lm(Y ~ X, data = data)\n\nInterpreting coefficients:\n\nB_0 is intercept, i.e. what is dependent variable when independent is 0\nB_1 is slope\nRecall, all stats is based on sample, we never have full universe of data.\nTrue relationship is unknowable, but based on different samples, we may get relationships more or less close to unknowable true relationship.\nNull hypothesis: The relationship actually 0, we get absolutely no information about Y from X.\nSo, “statistical significance” is saying there is enough evidence to disprove null hypothesis\nHow we do this: how weird would the data sample have to be for null hypothesis to be true?\nIs relationship real? p-value is the probability that the null hypothesis is true given the sample. We are looking for small p-values.\n\n\n\n\n\nFor inference, how explanatory is model relationship between X and Y – R-squared tells us how fitting the line is to sample.\nFor prediction, how well would model fit new data? R-squared alone does not tell us if model is trustworthy\noverfitting – model is too good at predicting one sample, R-squared is crazy good, “following noise” essentially, won’t generalize well though to other samples\nunderfitting – model ignores the relationship entirely, low R-squared\nperfect fit – “the gist of things”\n\n\n\n\n\nSolution: hold out some data to test predictions\n\nsay we have 67 counties in PA, train model on 70% of sample, test on 30%\n\nset.seed() will preserve random selection if you run code multiple times\n\n\n\n\n\nRoot means square errors, RMSE – i.e. “our predictions are off the real observations by RMSE units” – is this good enough?\n\n\n\n\n\nhow well does models apply to new datasets?\nK-fold cross-validation - fold the 30/70 split at different segments and run train/test on each one, average RMSE across all\nin R: library(caret) functions trainControl() sets a cross-validation method with number of folds, train() creates mode based on data and the object returned from trainControl()\n\n\n\n\n\nLinear regression makes assumptions, if violated:\n\ncoefficients may be biased\nSE may be wrong\n\nheteroscedasticity – variance changes across X\nformal test: Breusch-Pagan test, p&lt;.05 bad\n\n\n\n\n\n\nheteroscedasticity\n\n\npredictions unreliable\nNormality of residuals - Solution Q-Q Plot\nno multicollinearity - vif()\noutliers\n\n\n\n\nadd more features/variables\ncreate categorical variables\n\n\n\n\n\nunderstand the framework\nvisualize first\nfit the model\nevaluate performance (cross validation)\ncheck assumptions\nimprove model if needed\nconsider ethics (who could be harmed?)"
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html#do-digitally-vulnerable-tracts-have-adequate-access-to-free-wifi-spots-with-computers",
    "href": "labs/lab2/assignment2_kk.html#do-digitally-vulnerable-tracts-have-adequate-access-to-free-wifi-spots-with-computers",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Do digitally vulnerable tracts have adequate access to free wifi spots with computers?",
    "text": "Do digitally vulnerable tracts have adequate access to free wifi spots with computers?\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+):\n\nBuffers\nSpatial joins\nSpatial filtering with predicates\nDistance calculations\nIntersections or unions\nPoint-in-polygon aggregation\n\nYour Task:\n\n# Your spatial analysis\n\n#Filter Philly census tracts by the most digitally vulnerable: in the top quartile of percent of tract that has no home broadband or top quartile of percent of tract with no computer. \nphilly_digital_access &lt;- philly_digital_access %&gt;%\n  mutate(\n    percent_no_bb = (1-has_broadbandE/total_popE.x)*100,\n    percent_no_comp = (1-has_computerE/total_popE.x)*100\n  )\n\nsummary(philly_digital_access$percent_no_bb)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  25.95   56.43   62.88   61.71   68.65  100.00      17 \n\nsummary(philly_digital_access$percent_no_comp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  25.20   54.34   60.27   59.29   65.62  100.00      17 \n\n\n\n#based on summary stats, the most digitally vulnerable are tract with 68.65% or more of households that do not have home broadband or 65.62% or more of households that do not have a computer. \n\ndigit_vulnerable_philly &lt;- philly_digital_access %&gt;%\n  filter(percent_no_bb&gt;=68.65 | percent_no_comp&gt;=65.62 )\n\n#visually examine digitally vulnerable tracts and free wifi spots with computers\nggplot(philly_digital_access)+\n  geom_sf()+\n  geom_sf(\n    data=digit_vulnerable_philly,\n    fill=\"purple\"\n  )+\n  geom_sf(data=free_wifi_spots[free_wifi_spots$computers_==\"Y\",])+\n  theme_void()\n\n\n\n\n\n\n\n\n\n# Find nearest free wifi spots with a computer to tract centroids \ndigit_vulnerable_centroids &lt;- st_centroid(digit_vulnerable_philly)\nnearest_wifi_spot_comp &lt;- st_nearest_feature(digit_vulnerable_centroids, free_wifi_spots[free_wifi_spots$computers_==\"Y\",])\n\ndigit_vulnerable_philly &lt;- digit_vulnerable_philly %&gt;%\n  mutate(nearest_wifi_spot_geom = free_wifi_spots$geometry[nearest_wifi_spot_comp])\n\ndigit_vulnerable_philly &lt;- digit_vulnerable_philly %&gt;%\n  mutate(distance_wifi_spot_comp = set_units(st_distance(digit_vulnerable_centroids, digit_vulnerable_philly$nearest_wifi_spot_geom, by_element = TRUE), \"mi\"))\n\nsummary(digit_vulnerable_philly$distance_wifi_spot_comp) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3979  3.3372  5.8429  6.0722  7.9510 13.7746 \n\n\n\n#Find walking distance buffers around free wifi spots with computers \nfree_wifi_buffers &lt;- free_wifi_spots %&gt;%\n  filter(computers_==\"Y\" & to_display==\"ACTIVE\")%&gt;%\n  st_buffer(dist = 2640)  # 2640 ft = .5 mi\n\n\n#dissolve overlapping buffers \nfree_wifi_buffers_dissolve &lt;- free_wifi_buffers%&gt;%\n  st_union() %&gt;%\n  st_cast(\"POLYGON\")%&gt;%\n  st_as_sf()%&gt;%\n    mutate(\n    legend= \".5 mi from Free Wifi Spot w Computer\"\n  )\n\n  \n#examine buffers and tracts overlap\nggplot(philly_digital_access)+\n  geom_sf(\n    color=\"darkgray\",\n    linewidth=.5,\n    alpha=.5\n  )+\n  geom_sf(\n    data=digit_vulnerable_philly,\n    aes(fill=percent_no_bb+percent_no_comp)\n  )+\n  scale_fill_gradient(\n    low=\"#FFCF33\",\n    high=\"#F53D00\",\n    name=\"% No broadband + % No Computer\"\n  )+\n  new_scale_fill()+\n  geom_sf( \n    data=free_wifi_buffers_dissolve,\n    aes(fill=legend),\n    color=\"black\",\n    linewidth=.8,\n    alpha=0.4\n  )+\n  scale_color_manual(\n    values = c(\".5 mi from Free Wifi Spot w Computer\" =  \"#F5A6E6\") ,\n    name=NULL\n  )+\n  theme_void()\n\n\n\n\n\n\n\n\n\n#Define underserved tract as a tract that is digitally vulnerable and outside walking distance to a free wifi spot with a computer. \n\noverlap &lt;- st_intersects(digit_vulnerable_philly, free_wifi_buffers)\n\nno_overlap &lt;- lengths(overlap) == 0\n\ndigit_underserved &lt;- digit_vulnerable_philly[no_overlap, ]\n\nsummary(digit_underserved)\n\n    GEOID               NAME            total_popE.x   total_popM.x   \n Length:30          Length:30          Min.   :1027   Min.   :  82.0  \n Class :character   Class :character   1st Qu.:3868   1st Qu.: 624.0  \n Mode  :character   Mode  :character   Median :4604   Median : 808.0  \n                                       Mean   :4793   Mean   : 927.2  \n                                       3rd Qu.:6202   3rd Qu.:1139.5  \n                                       Max.   :8145   Max.   :2091.0  \n                                                                      \n has_computerE  has_computerM   has_broadbandE has_broadbandM \n Min.   :   0   Min.   : 11.0   Min.   :   0   Min.   : 11.0  \n 1st Qu.:1121   1st Qu.:155.8   1st Qu.:1103   1st Qu.:165.0  \n Median :1515   Median :226.0   Median :1328   Median :229.0  \n Mean   :1444   Mean   :223.7   Mean   :1346   Mean   :227.7  \n 3rd Qu.:1785   3rd Qu.:291.0   3rd Qu.:1677   3rd Qu.:284.0  \n Max.   :2528   Max.   :431.0   Max.   :2247   Max.   :444.0  \n                                                              \n   STATEFP            COUNTYFP           TRACTCE            GEOIDFQ         \n Length:30          Length:30          Length:30          Length:30         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    NAME.x            NAMELSAD            STUSPS           NAMELSADCO       \n Length:30          Length:30          Length:30          Length:30         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  STATE_NAME            LSAD               ALAND             AWATER       \n Length:30          Length:30          Min.   : 184674   Min.   :      0  \n Class :character   Class :character   1st Qu.: 491366   1st Qu.:      0  \n Mode  :character   Mode  :character   Median : 613069   Median :      0  \n                                       Mean   : 863564   Mean   :  46550  \n                                       3rd Qu.: 854698   3rd Qu.:      0  \n                                       Max.   :4488319   Max.   :1362058  \n                                                                          \n    NAME.y          median_incomeE   median_incomeM   total_popE.y \n Length:30          Min.   : 13721   Min.   : 2945   Min.   :1027  \n Class :character   1st Qu.: 38976   1st Qu.:11549   1st Qu.:3868  \n Mode  :character   Median : 54871   Median :14980   Median :4604  \n                    Mean   : 54849   Mean   :17043   Mean   :4793  \n                    3rd Qu.: 62857   3rd Qu.:22100   3rd Qu.:6202  \n                    Max.   :101094   Max.   :43870   Max.   :8145  \n                    NA's   :1        NA's   :1                     \n  total_popM.y       over_65E         over_65M               geometry \n Min.   :  82.0   Min.   :  0.00   Min.   :  3.00   MULTIPOLYGON :30  \n 1st Qu.: 624.0   1st Qu.:  6.00   1st Qu.: 13.00   epsg:2272    : 0  \n Median : 808.0   Median : 23.00   Median : 28.50   +proj=lcc ...: 0  \n Mean   : 927.2   Mean   : 31.63   Mean   : 36.50                     \n 3rd Qu.:1139.5   3rd Qu.: 50.50   3rd Qu.: 50.75                     \n Max.   :2091.0   Max.   :123.00   Max.   :121.00                     \n                                                                      \n percent_no_bb    percent_no_comp    nearest_wifi_spot_geom\n Min.   : 67.08   Min.   : 63.23   POINT        :30        \n 1st Qu.: 69.48   1st Qu.: 66.97   epsg:2272    : 0        \n Median : 70.93   Median : 68.08   +proj=lcc ...: 0        \n Mean   : 72.34   Mean   : 70.55                           \n 3rd Qu.: 73.19   3rd Qu.: 71.73                           \n Max.   :100.00   Max.   :100.00                           \n                                                           \n distance_wifi_spot_comp\n Min.   : 2.465         \n 1st Qu.: 5.471         \n Median : 6.020         \n Mean   : 7.138         \n 3rd Qu.: 9.588         \n Max.   :13.775         \n                        \n\n#what are characteristics of top 10 digitally underserved tracts by absolute number of those with no broadband + those with no computer? \n\n#note, the sum of estiamated households without computer and without broadband is not an accurate estimate of reality because those without one are likely overlapping with the other. This metric is just being used to create a metric showing reflecting broadband and computer need in one. \ndigit_underserved&lt;-digit_underserved%&gt;%\n  mutate(\n    digit_need_heuristic = (total_popE.x - has_computerE)+(total_popE.x - has_broadbandE)\n  )\n\ntop_10_digit_underserved &lt;- digit_underserved%&gt;%\n  arrange(desc(digit_need_heuristic))%&gt;%\n  slice_head(n = 10)\n \ntop_10_digit_underserved %&gt;%\n  st_drop_geometry()%&gt;%\n  select(c(\n    \"percent_no_bb\",\n    \"percent_no_comp\",\n    \"median_incomeE\",\n    \"distance_wifi_spot_comp\"\n  )) %&gt;%\n  kable(\n    col.names = c(\"Percent w/o Broadband\",\n                  \"Percent w/o a Computer\", \n                  \"Median Income\", \n                  \"Distance to Free Wi-Fi spot w/ Computer\"),\n    digits = 1,\n    format.args = list(big.mark = \",\"),\n    align = \"l\",\n    caption = \"10 Census Tracts in Philadelphia that are most Digitally Vulnerable and Distance to Nearest Free Wi-Fi Spot with a Public Computer\"\n  )\n\n\n10 Census Tracts in Philadelphia that are most Digitally Vulnerable and Distance to Nearest Free Wi-Fi Spot with a Public Computer\n\n\n\n\n\n\n\n\nPercent w/o Broadband\nPercent w/o a Computer\nMedian Income\nDistance to Free Wi-Fi spot w/ Computer\n\n\n\n\n82.4\n79.6\n38,976\n2.464926 [mi]\n\n\n73.3\n70.4\n13,721\n7.369100 [mi]\n\n\n75.5\n71.6\n56,818\n10.021851 [mi]\n\n\n73.6\n71.7\n68,652\n9.960189 [mi]\n\n\n70.9\n67.3\n33,419\n5.459912 [mi]\n\n\n76.1\n74.2\n50,245\n5.314348 [mi]\n\n\n69.9\n67.5\n100,857\n13.774589 [mi]\n\n\n67.8\n67.5\n71,551\n3.367629 [mi]\n\n\n67.6\n67.1\n54,871\n8.256588 [mi]\n\n\n68.1\n66.5\n93,300\n12.009875 [mi]\n\n\n\n\n\n\n#Map the underserved tracts and wifi spots buffers \ndigit_underserved&lt;-digit_underserved%&gt;%\n  mutate(\n    legend=\"Digitally Underserved Tract\"\n  )\n\nggplot(philly_digital_access)+\n  geom_sf(\n    color=\"darkgray\",\n    linewidth=.5,\n    alpha=.5\n  )+\n  geom_sf(\n    data=digit_underserved,\n    aes(fill=\"Digitally Underserved Tract\")\n  )+\n  scale_fill_manual(\n    values = c(\"Digitally Underserved Tract\" = \"#3F88C5\"),\n    name=NULL\n  )+\n  new_scale_fill()+\n  geom_sf( \n    data=free_wifi_buffers_dissolve,\n    aes(fill=\".5 mi from Free Wifi Spot w Computer\"),\n    color=\"black\",\n    linewidth=.8,\n    alpha=0.4\n  )+\n  scale_color_manual(\n    values = c(\".5 mi from Free Wifi Spot w Computer\" =  \"#F5A6E6\") \n  )+\n  guides(fill = guide_legend(title = NULL))+\n  theme_void()\n\n\n\n\n\n\n\n\nAnalysis requirements: - Clear code comments explaining each step - Appropriate CRS transformations - Summary statistics or counts - At least one map showing your findings - Brief interpretation of results (3-5 sentences)\nYour interpretation:\n[Write your findings here]"
  },
  {
    "objectID": "labs/lab2/Knox_Katie_Assignment2.html",
    "href": "labs/lab2/Knox_Katie_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives:\n\nApply spatial operations to answer policy-relevant research questions\nIntegrate census demographic data with spatial analysis\nCreate publication-quality visualizations and maps\nWork with spatial data from multiple sources\nCommunicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/lab2/Knox_Katie_Assignment2.html#assignment-overview",
    "href": "labs/lab2/Knox_Katie_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives:\n\nApply spatial operations to answer policy-relevant research questions\nIntegrate census demographic data with spatial analysis\nCreate publication-quality visualizations and maps\nWork with spatial data from multiple sources\nCommunicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/lab2/Knox_Katie_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "labs/lab2/Knox_Katie_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data:\n\nPennsylvania county boundaries\nPennsylvania hospitals (from lecture data)\nPennsylvania census tracts\n\nYour Task:\n\n# Load spatial data\npa_counties &lt;- st_read(\"data/Pennsylvania_County_Boundaries.shp\")\nhospitals &lt;- st_read(\"data/hospitals.geojson\")\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n\n# Check that all data loaded correctly\nglimpse(pa_counties)\nglimpse(census_tracts)\nglimpse(hospitals)\n\n\ncensus_tracts &lt;- st_transform(census_tracts, st_crs(pa_counties))\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\n\n\n# With ggplot2\np1 &lt;- ggplot(pa_counties) +\n  geom_sf() +\n  theme_void()\n\np2 &lt;- ggplot(census_tracts) +\n  geom_sf() +\n  theme_void()\n\np3 &lt;- ggplot(hospitals) +\n  geom_sf() +\n  theme_void()\n\np1 | p2 | p3\n\n\n\n\n\n\n\n\nQuestions to answer:\n\nHow many hospitals are in your dataset?\n\n223\n\nHow many census tracts?\n\n3445\n\nWhat coordinate reference system is each dataset in?\n\nI transformed hospitals and census tracts to PA counties CRS, which is WGS84. Before that, hospitals were also in WGS84, but census tracts were in NAD83.\n\n\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables:\n\nTotal population\nMedian household income\nPopulation 65 years and over (you may need to sum multiple age categories)\n\nYour Task:\n\n# Get demographic data from ACS\npa_tracts_data &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    median_income = \"B19013_001\",\n    total_pop = \"B01003_001\",\n    over_65 = \"B01001_020\"  # Population 65 years and over\n  ),\n  state = \"PA\",\n  year = 2023,\n  output = \"wide\"\n)\n\n# Join to tract boundaries\ncensus_tracts &lt;- census_tracts %&gt;%\n  left_join(pa_tracts_data, by= \"GEOID\")\n\nsummary(census_tracts)\n\nQuestions to answer:\n\nWhat year of ACS data are you using?\n\n2019-2023 5-year ACS data\n\nHow many tracts have missing income data?\n\n65\n\nWhat is the median income across all PA census tracts?\n\n$72,944\n\n\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria:\n\nLow median household income (choose an appropriate threshold)\nSignificant elderly population (choose an appropriate threshold)\n\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\nvulnerable_tracts &lt;- census_tracts %&gt;%\n  filter(median_incomeE &lt; 42398 | (over_65E/total_popE) &gt; .017)\n\nQuestions to answer:\n\nWhat income threshold did you choose and why?\n\nI looked at the 2023 poverty guidelines, the average household size in PA in 2023, which was 2.4, and then looked into eligibility for various programs, which is anywhere from 133% FPIG to 215%. I am going with vulnerable household is 215% or less of the 2-person household FPIG, or $42,398.\n\nWhat elderly population threshold did you choose and why?\n\nIn order to control for overall population size, instead of looking at the raw number of elderly, I looked at the over 65 as percent of total population. I defined vulnerable as any tract in the 75th percentile of elderly percent of tract population (1.7% or above).\n\nHow many tracts meet your vulnerability criteria?\n\n1,145\n\nWhat percentage of PA census tracts are considered vulnerable by your definition?\n\n33.2% – about a third.\n\n\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n#convert to Albers\nvulnerable_tracts &lt;- st_transform(vulnerable_tracts, crs = 5070)\nhospitals &lt;- st_transform(hospitals, crs = 5070)\n\n# Calculate distance from each tract centroid to nearest hospital\ntract_centroids &lt;- st_centroid(vulnerable_tracts)\nnearest_hospital &lt;- st_nearest_feature(tract_centroids, hospitals)\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(nearest_hospital_geom = hospitals$geometry[nearest_hospital])\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(distance_to_nearst_hospital = set_units(st_distance(tract_centroids, vulnerable_tracts$nearest_hospital_geom, by_element = TRUE), \"mi\"))\n\nsummary(vulnerable_tracts$distance_to_nearst_hospital) \n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 0.02107  1.21850  2.75761  4.37494  6.02931 29.75156 \n\n\nRequirements:\n\nUse an appropriate projected coordinate system for Pennsylvania\nCalculate distances in miles\nExplain why you chose your projection\n\nI chose Albers Conical Equal Area, NAD 83 projection based on this PA document from 2011’s recommendation.\nQuestions to answer:\n\nWhat is the average distance to the nearest hospital for vulnerable tracts?\n\n4.4 miles\n\nWhat is the maximum distance?\n\n29.8 miles\n\nHow many vulnerable tracts are more than 15 miles from the nearest hospital?\n\n33\n\n\n\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n# Create underserved variable\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(underserved = ifelse(as.numeric(distance_to_nearst_hospital) &gt; 15,\"Underserved\",\"\"))\n\nvulnerable_tracts %&gt;%\n  group_by(underserved)%&gt;%\n  summarise(n())\n\nQuestions to answer:\n\nHow many tracts are under-served?\n\n33\n\nWhat percentage of vulnerable tracts are under-served?\n\n3%\n\nDoes this surprise you? Why or why not?\n\nUnfortunately, this does not surprise me, given how rural certain areas of Pennsylvania are and knowing how rural healthcare is a persistent issue in the US in general. However, it would surprise me if some of these tracts are in urban areas.\n\n\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\n# Spatial join tracts to counties\npa_counties &lt;- pa_counties %&gt;% st_transform(st_crs(vulnerable_tracts))\n\nvulnerable_per_county &lt;- vulnerable_tracts %&gt;%\n  st_join(pa_counties) %&gt;%\n  st_drop_geometry()\n\n# Aggregate statistics by county\nvulnerable_per_county_stats &lt;- vulnerable_per_county %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarise(\n    num_vulnerable_tracts = n(),\n    num_underserved_tracts = sum(underserved == \"Underserved\"),\n    percent_underserved = sum(underserved == \"Underserved\")/n(),\n    avg_distance_to_nearst_hospital = mean(distance_to_nearst_hospital),\n    total_pop = sum(total_popE),\n    total_underserved_pop = sum(ifelse(underserved == \"Underserved\", total_popE, 0))\n  )\n\nRequired county-level statistics:\n\nNumber of vulnerable tracts\nNumber of underserved tracts\n\nPercentage of vulnerable tracts that are underserved\nAverage distance to nearest hospital for vulnerable tracts\nTotal vulnerable population\n\nQuestions to answer:\n\nWhich 5 counties have the highest percentage of underserved vulnerable tracts?\n\nCameron\nJuniata\nPotter\nSnyder\nSullivan\n\nWhich counties have the most vulnerable people living far from hospitals?\n\nClearfield\nChester\nJuniata\nSnyder\nPike\n\nAre there any patterns in where underserved counties are located?\n\nThere are a lot of quite rural underserved counties, like Sullivan, Juniata, and Clearfield, but there are also suburban counties of major PA cities like Chester County outside of Philadelphia and Cumberland County outside Harrisburg.\n\n\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n# Create and format priority counties table\nvulnerable_per_county_stats %&gt;% \n  arrange(desc(total_underserved_pop)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  select(\n    c(\"COUNTY_NAM\", \"total_underserved_pop\", \"percent_underserved\", \"avg_distance_to_nearst_hospital\")\n  ) %&gt;%\n  mutate(\n    avg_distance_to_nearst_hospital = as.numeric(avg_distance_to_nearst_hospital),\n    percent_underserved = paste0(round(percent_underserved*100, 2), \"%\")\n  )%&gt;%\n  kable(\n    col.names = c(\"County\", \"Total Underserved Population\", \"Percent of Vulnerable Tracts that are Underserved\", \"Average Distance of to Nearest Hospital\"),\n    digit = 1,\n    format.args = list(big.mark = \",\"),\n    align = \"l\",\n    caption = \"10 Counties in PA with the Highest Absolute Population more than 15 miles from Nearest Hospital\"\n  )\n\n\n10 Counties in PA with the Highest Absolute Population more than 15 miles from Nearest Hospital\n\n\n\n\n\n\n\n\nCounty\nTotal Underserved Population\nPercent of Vulnerable Tracts that are Underserved\nAverage Distance of to Nearest Hospital\n\n\n\n\nCLEARFIELD\n17,027\n28.57%\n12.4\n\n\nCHESTER\n15,467\n7.5%\n6.1\n\n\nJUNIATA\n13,955\n50%\n15.4\n\n\nSNYDER\n12,073\n50%\n15.1\n\n\nPIKE\n10,292\n35.29%\n15.4\n\n\nDAUPHIN\n8,815\n8.33%\n5.3\n\n\nSCHUYLKILL\n8,815\n7.41%\n7.0\n\n\nPERRY\n8,761\n22.22%\n12.3\n\n\nLANCASTER\n8,055\n5.71%\n5.7\n\n\nCENTRE\n6,843\n5.88%\n6.2\n\n\n\n\n\nRequirements:\n\nUse knitr::kable() or similar for formatting\nInclude descriptive column names\nFormat numbers appropriately (commas for population, percentages, etc.)\nAdd an informative caption\nSort by priority (you decide the metric)"
  },
  {
    "objectID": "labs/lab2/Knox_Katie_Assignment2.html#part-2-comprehensive-visualization",
    "href": "labs/lab2/Knox_Katie_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\n# Create county-level access map\nvulnerable_per_county_stats &lt;- vulnerable_per_county_stats %&gt;%\n  left_join(pa_counties, by=\"COUNTY_NAM\")\n\nvulnerable_per_county_stats &lt;- vulnerable_per_county_stats %&gt;%\n  st_as_sf() %&gt;% \n  st_transform(st_crs(census_tracts)) %&gt;%\n  mutate(percent_underserved = percent_underserved*100\n  )\n\nhospitals &lt;- hospitals %&gt;%\n  mutate(type = \"Hospital\")\n\nggplot(vulnerable_per_county_stats) +\n  geom_sf(aes(fill = percent_underserved)) +\n  scale_fill_gradient(\n    low = \"#C3CDFE\",   \n    high = \"#485EFE\",   \n    name = \"% Underserved Tracts\"\n  ) +\n  new_scale_fill()+\n  geom_sf(\n    data = hospitals, \n    aes(fill=type),      \n    size = 3, \n    shape = 21, \n    color = \"#FF8600\",     \n    stroke = 0.8\n  ) +\n  scale_fill_manual(\n    values = c(\"Hospital\" = \"#FF8600\"),\n    name = NULL\n  )+\n  labs(\n    title = \"Underserved Tracts per Counties in Pennsylvania\",\n    subtitle = str_wrap(\"Where an underserved tract is one where the center is at least 15 miles from the nearest hospital\",50),\n    caption =\"Data Sources: US Census Data, OpenDataPhilly\"\n  ) +\n  annotation_north_arrow(             \n    location = \"tr\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    legend.direction = \"vertical\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\nRequirements:\n\nFill counties by percentage of vulnerable tracts that are underserved\nInclude hospital locations as points\nUse an appropriate color scheme\nInclude clear title, subtitle, and caption\nUse theme_void() or similar clean theme\nAdd a legend with formatted labels\n\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\n# Create detailed tract-level map\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  st_as_sf() %&gt;% \n  st_transform(st_crs(census_tracts))\n\nunderserved_tracts &lt;- vulnerable_tracts[vulnerable_tracts$underserved==\"Underserved\",]\n\npa_counties$legend &lt;- \"County Boundary\"\n\nggplot(census_tracts) +\n  geom_sf(\n    color=\"darkgrey\",\n    alpha=.5,\n    size=.5\n  )+\n  geom_sf(\n    data=underserved_tracts,\n    aes(fill = underserved)\n  )+\n  geom_sf(\n    data=pa_counties,\n    aes(color=legend),     \n    linewidth = .8,\n    fill=NA\n  )+\n  geom_sf(\n    data=hospitals,\n    aes(fill = type),      \n    size = 1, \n    shape = 21, \n    color = \"#FF8600\",     \n    stroke = 0.8\n  )+\n  scale_fill_manual(\n    values = c(\"Underserved\" = \"#485EFE\",\n               \"Hospital\" = \"#FF8600\"),\n    name = NULL  \n  ) +\n  scale_color_manual(\n    values = c(\"County Boundary\" = \"black\"),\n    name = NULL\n  ) +\n  annotation_north_arrow(             \n    location = \"tr\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void()+\n  labs(\n    title = \"Underserved Tracts in Pennsylvania\",\n    subtitle = str_wrap(\"Where an underserved tract's center is at least 15 miles from the nearest hospital\",50),\n    caption =\"Data Sources: US Census Data, Pennsylvania Spatial Data Access\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\nRequirements:\n\nShow underserved vulnerable tracts in a contrasting color\nInclude county boundaries for context\nShow hospital locations\nUse appropriate visual hierarchy (what should stand out?)\nInclude informative title and subtitle\n\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create distribution visualization\n\n#Load regions\ndep_regions&lt;- st_read(\"data/DEPRegions2024_03.shp\")\n\n\ndep_regions &lt;- dep_regions %&gt;% st_transform(st_crs(vulnerable_tracts))\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;% \n  mutate(\n      urban_rural = case_when(\n      total_popE &gt;= 5000 ~ \"Urban\",\n      TRUE ~ \"Rural\"\n      )\n  )\n\nvulnerable_tracts_with_regions &lt;- vulnerable_tracts %&gt;%\n  st_join(dep_regions) %&gt;%\n  st_drop_geometry()\n\nvulnerable_tracts_with_regions &lt;- vulnerable_tracts_with_regions %&gt;%\n  mutate(\n    Region = case_when(\n      SNAME == \"NCRO\"~\"North Central\",\n      SNAME ==\"NERO\"~\"North East\",\n      SNAME==\"NWRO\"~\"North West\",\n      SNAME==\"SCRO\"~\"South Central\",\n      SNAME==\"SERO\"~\"South East\",\n      SNAME==\"SWRO\"~\"South West\"\n    )\n  )\n  \n  \nggplot(vulnerable_tracts_with_regions)+\n  geom_boxplot(\n    aes(x=Region, y=distance_to_nearst_hospital)\n  )+\n  labs(\n    title=\"Distance of Vulnerable Tracts to Nearest Hospital by Region\",\n    caption=str_wrap(\"Where a vulnerable tract is in the top 25% of concentration of elderly population or 215% of the Federal Poverty Line for 2-person household. Data sources: US Census, Pennsylvania Spatial Data Access, Department of Environmental Protection Regions\",100),\n    y=str_wrap(\"Distance from Center of Tract to Nearest Hospital\",50),\n    x=\"Region of Tract\"\n  )+\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\nNorthern Pennsylvania vulnerable populations generally are location farther from hospitals than Southern Pennsylvania, with North Central PA having the highest median and upper quartiles of distance to the nearest hospital of all 6 regions. However another trend we can see is that the highest number of outliers fall within the South East and South West regions, home to the two biggest cities in PA, Philadelphia and Pittsburgh, respectively. That shows that these regions have more tracts that are facing very different needs that the middle 50% of the region.\nSuggested chart types:\n\nHistogram or density plot of distances\nBox plot comparing distances across regions\nBar chart of underserved tracts by county\nScatter plot of distance vs. vulnerable population size\n\nRequirements:\n\nClear axes labels with units\nAppropriate title\nProfessional formatting\nBrief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "labs/lab2/Knox_Katie_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "labs/lab2/Knox_Katie_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\n\n\n\nYour Analysis\n\nDigital Justice\n\nData: Census Broadband access, device access, and income, Philadelphia free wifi spots from OpenDataPhilly\nQuestion: “Do digitally disadvantaged neighborhoods have equitable access to city internet?”\nOperations: Buffer free wifi spots with a computer (10-minute walk = 0.5 mile), calculate connectivity by tract, determine digitally vulnerable tracts from census data and underserved tracts by distance from buffers.\nPolicy relevance: Digital equity, broadband infrastructure, internet-connected device access\n\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load your additional dataset\nv21 &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n\n# Search for broadband and computer-related variables\nbb_vars &lt;- v21 %&gt;% filter(str_detect(label, \"broadband\"))\ncomp_vars &lt;-v21 %&gt;% filter(str_detect(label, \"computer\"))\n\n#B28003_002 = total with computer\n#B28002_004 = total with broadband internet subscription\n\nphilly_digital_access &lt;-  get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    has_computer = \"B28003_002\", \n    has_broadband = \"B28002_004\"\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2023,\n  output = \"wide\"\n)\n\nfree_wifi_spots &lt;- st_read(\"data/free_city_wifi_locations.shp\")%&gt;%st_transform(2272)\n\nphilly_digital_access &lt;- left_join(\n  philly_digital_access,\n  census_tracts,\n  by=\"GEOID\"\n) %&gt;%\n  st_as_sf() %&gt;%\n  st_transform(2272)\n\nggplot(philly_digital_access)+\n  geom_sf()+\n  geom_sf(\n    data=free_wifi_spots\n  )+\n  theme_void()+\n  labs(\n    title = \"Free Wi-Fi Spots in Philadelphia\",\n    caption =\"Data Sources: US Census Data, OpenDataPhilly\"\n  ) +\n  annotation_north_arrow(             \n    location = \"br\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    legend.direction = \"vertical\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\nQuestions to answer:\n\nWhat dataset did you choose and why?\n\nI choose the ACS variables showing whether a household has broadband access and whether they have a computer, as well as OpenDataPhilly’s database of free wi-fi locations, which inlcudes a variable showing how many public use computers are available at the location.\n\nWhat is the data source and date?\n\nThe ACS data is 5-year data from 2018 - 2023, and the wi-fi locations are from 2024, so the home broadband and computer access data may be slightly outdated. Also important to note is 2018 - 2023 range includes the pandemic, a period in which there was unprecedented investment in home internet subsidies that have expired in 2024 Affordable Connectivity Program.\n\nHow many features does it contain?\n\nThere are 254 free-wifi locations, and 408 census tracts in Philadelphia.\n\nWhat CRS is it in? Did you need to transform it?\n\nBefore transformation, both datasets were in web mercator, EPSG 3857, and since Philly falls within southern pennsylvania, I transformed it to Pennsylvania South state plane projection, EPSG 2272.\n\n\n\n\nPose a research question\n\nDo digitally vulnerable tracts have adequate access to free wifi spots with computers?\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+):\n\nBuffers\nSpatial joins\nSpatial filtering with predicates\nDistance calculations\nIntersections or unions\nPoint-in-polygon aggregation\n\nYour Task:\n\n# Your spatial analysis\n\n#Filter Philly census tracts by the most digitally vulnerable: in the top quartile of percent of tract that has no home broadband or top quartile of percent of tract with no computer. \nphilly_digital_access &lt;- philly_digital_access %&gt;%\n  mutate(\n    percent_no_bb = (1-has_broadbandE/total_popE.x)*100,\n    percent_no_comp = (1-has_computerE/total_popE.x)*100\n  )\n\nsummary(philly_digital_access$percent_no_bb)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  25.95   56.43   62.88   61.71   68.65  100.00      17 \n\nsummary(philly_digital_access$percent_no_comp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  25.20   54.34   60.27   59.29   65.62  100.00      17 \n\n\n\n#based on summary stats, the most digitally vulnerable are tract with 68.65% or more of households that do not have home broadband or 65.62% or more of households that do not have a computer. \n\ndigit_vulnerable_philly &lt;- philly_digital_access %&gt;%\n  filter(percent_no_bb&gt;=68.65 | percent_no_comp&gt;=65.62 )%&gt;%\n  mutate(Vulnerable=\"Digtally Vulnerable Tract\")\n\n#visually examine digitally vulnerable tracts and free wifi spots with computers\nggplot(philly_digital_access)+\n  geom_sf()+\n  geom_sf(\n    data=digit_vulnerable_philly,\n    aes(fill=Vulnerable)\n  )+\n  scale_fill_manual(\n    values=c(\"Digtally Vulnerable Tract\" = \"purple\")\n  )+\n  geom_sf(data=free_wifi_spots[free_wifi_spots$computers_==\"Y\",])+\n  theme_void()+\n  labs(\n    title = str_wrap(\"Digitally Vulnerable Tracts and Free Wifi Spots with a Computer in Philadelphia\",70),\n    subtitle = str_wrap(\"Where a vulnerable tract is one where 68.65% or more of households do not have home broadband or 65.62% or more of households do not have a computer\",60),\n    caption =\"Data Sources: US Census Data, OpenDataPhilly\"\n  ) +\n  annotation_north_arrow(             \n    location = \"br\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    legend.direction = \"vertical\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\n\n# Find nearest free wifi spots with a computer to tract centroids \ndigit_vulnerable_centroids &lt;- st_centroid(digit_vulnerable_philly)\nnearest_wifi_spot_comp &lt;- st_nearest_feature(digit_vulnerable_centroids, free_wifi_spots[free_wifi_spots$computers_==\"Y\",])\n\ndigit_vulnerable_philly &lt;- digit_vulnerable_philly %&gt;%\n  mutate(nearest_wifi_spot_geom = free_wifi_spots$geometry[nearest_wifi_spot_comp])\n\ndigit_vulnerable_philly &lt;- digit_vulnerable_philly %&gt;%\n  mutate(distance_wifi_spot_comp = set_units(st_distance(digit_vulnerable_centroids, digit_vulnerable_philly$nearest_wifi_spot_geom, by_element = TRUE), \"mi\"))\n\nsummary(digit_vulnerable_philly$distance_wifi_spot_comp) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3979  3.3372  5.8429  6.0722  7.9510 13.7746 \n\n\n\n#Find walking distance buffers around free wifi spots with computers \nfree_wifi_buffers &lt;- free_wifi_spots %&gt;%\n  filter(computers_==\"Y\" & to_display==\"ACTIVE\")%&gt;%\n  st_buffer(dist = 2640)  # 2640 ft = .5 mi\n\n\n#dissolve overlapping buffers \nfree_wifi_buffers_dissolve &lt;- free_wifi_buffers%&gt;%\n  st_union() %&gt;%\n  st_cast(\"POLYGON\")%&gt;%\n  st_as_sf()%&gt;%\n    mutate(\n    legend= \".5 mi from Free Wifi Spot w Computer\"\n  )\n\n  \n#examine buffers and tracts overlap\nggplot(philly_digital_access)+\n  geom_sf(\n    color=\"darkgray\",\n    linewidth=.5,\n    alpha=.5\n  )+\n  geom_sf(\n    data=digit_vulnerable_philly,\n    aes(fill=percent_no_bb+percent_no_comp)\n  )+\n  scale_fill_gradient(\n    low=\"#FFCF33\",\n    high=\"#F53D00\",\n    name=str_wrap(\"Digital Vulnerability (higher number is more vulnerable)\",30)\n  )+\n  new_scale_fill()+\n  geom_sf( \n    data=free_wifi_buffers_dissolve,\n    aes(fill=legend),\n    color=\"black\",\n    linewidth=.8,\n    alpha=0.4\n  )+\n  scale_color_manual(\n    values = c(\".5 mi from Free Wifi Spot w Computer\" =  \"#F5A6E6\") ,\n    name=NULL\n  )+\n  theme_void()+\n  labs(\n    title = str_wrap(\"Degree of Digital Vulnerability and Walking Distance to Public WiFi & Computer\",50),\n    caption =\"Data Sources: US Census Data, OpenDataPhilly\"\n  ) +\n  annotation_north_arrow(             \n    location = \"br\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void() +\n  guides(fill = guide_legend(title = NULL))+\n  theme(\n    legend.position = \"right\",\n    legend.direction = \"vertical\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\n\n#Define underserved tract as a tract that is digitally vulnerable and outside walking distance to a free wifi spot with a computer. \n\noverlap &lt;- st_intersects(digit_vulnerable_philly, free_wifi_buffers)\n\nno_overlap &lt;- lengths(overlap) == 0\n\ndigit_underserved &lt;- digit_vulnerable_philly[no_overlap, ]\n\nsummary(digit_underserved)\n\n\n#what are characteristics of top 10 digitally underserved tracts by absolute number of those with no broadband + those with no computer? \n\n#note, the sum of estiamated households without computer and without broadband is not an accurate estimate of reality because those without one are likely overlapping with the other. This metric is just being used to create a metric showing reflecting broadband and computer need in one. \ndigit_underserved&lt;-digit_underserved%&gt;%\n  mutate(\n    digit_need_heuristic = (total_popE.x - has_computerE)+(total_popE.x - has_broadbandE)\n  )\n\ntop_10_digit_underserved &lt;- digit_underserved%&gt;%\n  arrange(desc(digit_need_heuristic))%&gt;%\n  slice_head(n = 10)\n \ntop_10_digit_underserved %&gt;%\n  st_drop_geometry()%&gt;%\n  select(c(\n    \"NAMELSAD\",\n    \"percent_no_bb\",\n    \"percent_no_comp\",\n    \"median_incomeE\",\n    \"distance_wifi_spot_comp\"\n  )) %&gt;%\n  kable(\n    col.names = c(\"Census Tract\",\n                  \"Percent w/o Broadband\",\n                  \"Percent w/o a Computer\", \n                  \"Median Income\", \n                  \"Distance to Free Wi-Fi spot w/ Computer\"),\n    digits = 1,\n    format.args = list(big.mark = \",\"),\n    align = \"l\",\n    caption = \"10 Census Tracts in Philadelphia that have Highest Digitally Vulnerable Population\"\n  )\n\n\n10 Census Tracts in Philadelphia that have Highest Digitally Vulnerable Population\n\n\n\n\n\n\n\n\n\nCensus Tract\nPercent w/o Broadband\nPercent w/o a Computer\nMedian Income\nDistance to Free Wi-Fi spot w/ Computer\n\n\n\n\nCensus Tract 90\n82.4\n79.6\n38,976\n2.464926 [mi]\n\n\nCensus Tract 192\n73.3\n70.4\n13,721\n7.369100 [mi]\n\n\nCensus Tract 314.01\n75.5\n71.6\n56,818\n10.021851 [mi]\n\n\nCensus Tract 310\n73.6\n71.7\n68,652\n9.960189 [mi]\n\n\nCensus Tract 300\n70.9\n67.3\n33,419\n5.459912 [mi]\n\n\nCensus Tract 305.02\n76.1\n74.2\n50,245\n5.314348 [mi]\n\n\nCensus Tract 363.03\n69.9\n67.5\n100,857\n13.774589 [mi]\n\n\nCensus Tract 274.02\n67.8\n67.5\n71,551\n3.367629 [mi]\n\n\nCensus Tract 100\n67.6\n67.1\n54,871\n8.256588 [mi]\n\n\nCensus Tract 362.01\n68.1\n66.5\n93,300\n12.009875 [mi]\n\n\n\n\n\n\n#Map the underserved tracts and wifi spots buffers \ndigit_underserved&lt;-digit_underserved%&gt;%\n  mutate(\n    legend=\"Digitally Underserved Tract\"\n  )\n\nggplot(philly_digital_access)+\n  geom_sf(\n    color=\"darkgray\",\n    linewidth=.5,\n    alpha=.5\n  )+\n  geom_sf(\n    data=digit_underserved,\n    aes(fill=\"Digitally Underserved Tract\")\n  )+\n  scale_fill_manual(\n    values = c(\"Digitally Underserved Tract\" = \"#3F88C5\"),\n    name=NULL\n  )+\n  new_scale_fill()+\n  geom_sf( \n    data=free_wifi_buffers_dissolve,\n    aes(fill=\".5 mi from Free Wifi Spot w Computer\"),\n    color=\"black\",\n    linewidth=.8,\n    alpha=0.4\n  )+\n  scale_color_manual(\n    values = c(\".5 mi from Free Wifi Spot w Computer\" =  \"#F5A6E6\") \n  )+\n  guides(fill = guide_legend(title = NULL))+\n  theme_void()+\n    labs(\n    title = str_wrap(\"Digitally Underserved Tracts in Philadelphia \",40),\n    caption =\"Data Sources: US Census Data, OpenDataPhilly\"\n  ) +\n  annotation_north_arrow(             \n    location = \"br\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void() +\n  guides(fill = guide_legend(title = NULL))+\n  theme(\n    legend.position = \"right\",\n    legend.direction = \"vertical\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\nAnalysis requirements:\n\nClear code comments explaining each step\nAppropriate CRS transformations\nSummary statistics or counts\nAt least one map showing your findings\nBrief interpretation of results (3-5 sentences)\n\nYour interpretation:\nWhile the Free Wi-Fi Spots in Philadelphia are a very promising program to help bridge the gap for those without internet and a computer at home, a lot of the centers with computers are outside walking distance for the most digitally vulnerable in Philadelphia. There are 30 total tracts in Philly that completely fall outside walking distance to a Wi-Fi computer center, mostly concentrated in North Philadelphia. The estimated number of households in these 30 tracts without broadband is 103,418 and without a computer is 100,478. The average distance from these underserved tracts to the nearest free Wi-Fi and computer center is 7 miles.\nThe internet has become indispensable to most educational, workforce, social, and civic needs, and thus policy should primarily aim to enable every person to have home broadband and a computer. However, while that may be a long-term goal, free Wi-Fi and computer centers help bridge the gap. Thus they should prioritize locations as close as possible to the most digitally vulnerable areas of Philadelphia, seeing as a longer commute to these centers will compound the barriers already faced by not having broadband or a computer at home."
  },
  {
    "objectID": "labs/lab2/Knox_Katie_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "labs/lab2/Knox_Katie_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nTaking feedback into account, I have hidden sensitive code blocks and hidden lengthy console output that is not essential to interpreting my work!"
  },
  {
    "objectID": "labs/lab2/Knox_Katie_Assignment2.html#submission-requirements",
    "href": "labs/lab2/Knox_Katie_Assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "I have chosen to look at “All street lights out” violations, because it is more cozy to commit crimes when the lights are out.\n\nchi_boundary&lt;-chi_boundary%&gt;%st_transform(st_crs(all_lights_out))\n\nchi_boundary&lt;-chi_boundary%&gt;%st_transform(26971)\nall_lights_out&lt;-all_lights_out%&gt;%st_transform(26971)\n\nall_lights_out%&gt;%\n  ggplot()+\n  geom_sf(alpha = 0.1, color = \"lightblue\")+\n  geom_sf(data = chi_boundary, fill = NA, color = \"darkgreen\")+\n  theme_void()\n\n\n\n\n\n\n\n\nThe 311 complaints of all street lights out are seemingly very ubiquitous across Chicago! Every part of the city is very visual dense with instances. The only empty areas are where the airport is located, and where there are bodies of water and parks."
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-1-collect-311-violation-data",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-1-collect-311-violation-data",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "I have chosen to look at “All street lights out” violations, because it is more cozy to commit crimes when the lights are out.\n\nchi_boundary&lt;-chi_boundary%&gt;%st_transform(st_crs(all_lights_out))\n\nchi_boundary&lt;-chi_boundary%&gt;%st_transform(26971)\nall_lights_out&lt;-all_lights_out%&gt;%st_transform(26971)\n\nall_lights_out%&gt;%\n  ggplot()+\n  geom_sf(alpha = 0.1, color = \"lightblue\")+\n  geom_sf(data = chi_boundary, fill = NA, color = \"darkgreen\")+\n  theme_void()\n\n\n\n\n\n\n\n\nThe 311 complaints of all street lights out are seemingly very ubiquitous across Chicago! Every part of the city is very visual dense with instances. The only empty areas are where the airport is located, and where there are bodies of water and parks."
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-2-fishnet-grid",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-2-fishnet-grid",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 2: Fishnet Grid",
    "text": "Part 2: Fishnet Grid\n\n# Step 1: Define cell size (in map units - meters for this projection)\ncell_size &lt;- 500  # 500m x 500m cells\n\n# Step 2: Create grid over study area\nfishnet &lt;- st_make_grid(\n  chi_boundary,\n  cellsize = cell_size,\n  square = TRUE,\n  what = \"polygons\"\n) %&gt;%\n  st_sf() %&gt;%\n  mutate(uniqueID = row_number())\n\n# Step 3: Clip to study area (remove cells outside boundary)\nfishnet &lt;- fishnet[chi_boundary, ]\n\n# Check results\nnrow(fishnet)  # Number of cells\n\n[1] 2640\n\nst_area(fishnet[1, ])  # Area of one cell (should be 250,000 m²)\n\n250000 [m^2]\n\nfishnet %&gt;%\n  ggplot()+\n  geom_sf()+\n  geom_sf(data = chi_boundary, fill=NA, color=\"darkgreen\")+\n  theme_void()\n\n\n\n\n\n\n\n\n\n# Count lights out per cell\nlights_out_count &lt;- st_join(all_lights_out, fishnet) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(count = n())\n\n# Join back to fishnet\nfishnet &lt;- fishnet %&gt;%\n  left_join(lights_out_count, by = \"uniqueID\") %&gt;%\n  mutate(count_lights_out = replace_na(count, 0))\n\n# Summary\nsummary(fishnet$count_lights_out)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0    24.0    95.5   112.3   167.0   673.0"
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-3-spatial-features",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-3-spatial-features",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 3: Spatial Features",
    "text": "Part 3: Spatial Features\nFind “Nearest Neighbor”\n\n# Calculate distance to nearest complaint of all street lights out\nnn_dist &lt;- get.knnx(\n  data = st_coordinates(all_lights_out),      # \"To\" locations\n  query = st_coordinates(st_centroid(fishnet)), # \"From\" locations\n  k = 1                                          # Nearest 1\n)\n\n# Extract distances\nfishnet$lights_out_dists &lt;- nn_dist$nn.dist[, 1]\nsummary(fishnet$lights_out_dists)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n   0.9545   33.3966   54.0120  194.0979  124.7172 4603.3141 \n\n\nThe median distance from a random 500x500 area in Chicago to a reported incident of all street lights out is 54 meters, and the 3rd quantile is 124 meters. This means the majority of ares in Chicago are very near one such complaint (and likely more than one given the distribution of incident counts per grid cell from the previous step). The mean is higher that the 3rd quantile, showing that max distance is an extreme outlier, likely in one of the park or airport areas where there probably aren’t streets in a park let alone street lights.\nFind Local Moran’s I\n\n# Step 1: Create spatial object\nfishnet_sp &lt;- as_Spatial(fishnet)\n\n# Step 2: Define neighbors (Queen contiguity)\nneighbors &lt;- poly2nb(fishnet_sp, queen = TRUE)\n\n# Step 3: Create spatial weights (row-standardized)\nweights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n\n# Step 4: Calculate Local Moran's I\nlocal_moran &lt;- localmoran(\n  fishnet$count_lights_out,  # Variable of interest\n  weights,                  # Spatial weights\n  zero.policy = TRUE       # Handle cells with no neighbors\n)\n\n# Step 5: Extract components\nfishnet$local_I &lt;- local_moran[, \"Ii\"]      # Local I statistic\nfishnet$p_value &lt;- local_moran[, \"Pr(z != E(Ii))\"]  # P-value\nfishnet$z_score &lt;- local_moran[, \"Z.Ii\"]    # Z-score\n\n\n# Standardize the variable for quadrant classification\nfishnet$standardized_value &lt;- scale(fishnet$count_lights_out)\n\n# Calculate spatial lag (weighted mean of neighbors)\nfishnet$spatial_lag &lt;- lag.listw(weights, fishnet$count_lights_out)\nfishnet$standardized_lag &lt;- scale(fishnet$spatial_lag)\n\n# Identify High-High clusters\nfishnet$hotspot &lt;- 0  # Default: not a hotspot\n\n# Criteria: \n# 1. Value above mean (standardized &gt; 0)\n# 2. Neighbors above mean (spatial lag &gt; 0)\n# 3. Statistically significant (p &lt; 0.05)\n\nfishnet$hotspot[\n  fishnet$standardized_value &gt; 0 & \n  fishnet$standardized_lag &gt; 0 & \n  fishnet$p_value &lt; 0.05\n] &lt;- 1\n\n# Count hotspots\nsum(fishnet$hotspot)\n\n[1] 418\n\n\nIn this step, first each cell’s count of incidents is standardized to measure how much it deviates from the mean, and a spatial lag is calculated to summarize the average counts in neighboring cells based on the intution that areas with high incident counts tend to be near other areas with high counts, so clustering in space can reveal meaningful hotspots of activity. Cells are classified as High-High clusters (hotspots) if they have above-average counts, are surrounded by neighbors with above-average counts, and are statistically significant based on Local Moran’s I (p &lt; 0.05). The fact that the sum of hotspots is 418 indicates that 418 grid cells are part of statistically significant clusters of high incident density.\n\nfishnet%&gt;%filter(hotspot==1)%&gt;%\n  ggplot()+\n  geom_sf()+\n  theme_void()+\n  geom_sf(data = chi_boundary, fill = NA, color = \"darkgreen\")\n\n\n\n\n\n\n\n\nLooking at a map of these hotspots reveals they are concentrated in areas west and south of the main center city core.\nNext, I find distance of each grid cell to hotspot.\n\n# Step 1: Identify hotspots \nhotspot_cells &lt;- filter(fishnet, hotspot == 1)\n\n# Step 2: Calculate distances\nhotspot_dist &lt;- get.knnx(\n  data = st_coordinates(st_centroid(hotspot_cells)),\n  query = st_coordinates(st_centroid(fishnet)),\n  k = 1\n)\n\nfishnet$hotspot_nn &lt;- hotspot_dist$nn.dist[, 1]\n\n\n# Create comparison maps\np1 &lt;- ggplot(fishnet) +\n  geom_sf(aes(fill = lights_out_dists), color = NA) +\n  scale_fill_viridis_c(name = \"Distance (m)\", option = \"plasma\",  direction = -1) +\n  labs(title = \"Distance to Nearest Complaint\\nof All Street Lights Out\") +\n  theme_void()\n\n\np2 &lt;- ggplot(fishnet) +\n  geom_sf(aes(fill = hotspot_nn), color = NA) +\n  scale_fill_viridis_c(name = \"Distance (m)\", option = \"plasma\",  direction = -1) +\n  labs(title = \"Distance to Nearest Hotspot\") +\n  theme_void()\n\np1 + p2 + plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\nThe distribution of distances shows that individual complaints are much more spatially heterogeneous than the hotspots. While the nearest-complaint distances vary widely across the city, the nearest-hotspot distances are more uniform. This indicates that 311 service requests are concentrated in clusters, and once these clusters are aggregated into hotspots, the spatial variation smooths out. In other words, individual incidents occur in scattered patterns, but the resulting hotspots capture the broader areas of high activity, providing a clearer picture of concentrated problem areas."
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-4",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-4",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 4",
    "text": "Part 4"
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-5",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-5",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 5",
    "text": "Part 5"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html",
    "href": "weekly-notes/week-09-notes.html",
    "title": "Week 9 Notes",
    "section": "",
    "text": "PREDPOL, Palantir, Hunch\nDirty Data\nMore likely to predict what police are already doing rather than preventing novel crimes\nA branch of “technological solutionism”\n\n\n\n\nRevisiting Spatial Lag - e.g. adding a term in housing prediction model that takes into account average neighboring house prices - Problem when using for prediction – you don’t have future neighbor prices… so problematic to include in model - if you use current neighbor sales… risk over emphasizing past prices - if you use predicted future neighbor sale values… problem of circularity\n\n\n\n\nSetup & Data\n\n(dirty) burglary data from police\n(dirty) 311 calls about abandoned cars\nCreate fishnet (500m x 500m grid)\n\na common unit of analysis\n\naggregate burglaries to cells\n\nBaseline Comparison\n\nKernel Density Estimation (KDE)\n\nDoes our model perform better than the KDE?\n\nSimple spatial smoothing\nWhat we need to beat\n\nFeature engineering\n\n\nk-Nearest Neighbors\n\nLISA (Local Moran’s I)\n\nCount Regression Models\nSpatial Cross-Validation\n\nSpecific CV to spatial applications\n\nModel Comparison\n\nCore Logic: “Broken Windows Theory” - i.e. visual signs of ‘disorder’ lead to criminal activity\nSpatial Weights Matrix - Global Moran’s I gives us a value from [-1, 1] - Positive is similar values cluster - Negative is dissimilar values cluster - 0 is no clustering - Local Moran’s I - Test for statistical significance - so every estimate gets a p-value - Moran Scatterplot - High-high = hotspot - High-low = outliers - Low-low = coldspot - Low-high = outliers\nPoisson Distribution - Appropriate for count data - Poisson assumption: Variance = mean\nPoisson Regression - Log terms - In R, use glm() instead of lm() - Test for overdispersion - Use glm.nb() if there is overdispersion\nSpatial Cross-Validation - Standard CV might fail for spatial data because nearby observations are correlated; Spatial leakage, or model learns from neighbors of test set - Instead group together and remove neighbors together in a fold\nKernel Density Estimation - create a kernel out of different densities of observations"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#notes",
    "href": "weekly-notes/week-09-notes.html#notes",
    "title": "Week 9 Notes",
    "section": "",
    "text": "PREDPOL, Palantir, Hunch\nDirty Data\nMore likely to predict what police are already doing rather than preventing novel crimes\nA branch of “technological solutionism”\n\n\n\n\nRevisiting Spatial Lag - e.g. adding a term in housing prediction model that takes into account average neighboring house prices - Problem when using for prediction – you don’t have future neighbor prices… so problematic to include in model - if you use current neighbor sales… risk over emphasizing past prices - if you use predicted future neighbor sale values… problem of circularity\n\n\n\n\nSetup & Data\n\n(dirty) burglary data from police\n(dirty) 311 calls about abandoned cars\nCreate fishnet (500m x 500m grid)\n\na common unit of analysis\n\naggregate burglaries to cells\n\nBaseline Comparison\n\nKernel Density Estimation (KDE)\n\nDoes our model perform better than the KDE?\n\nSimple spatial smoothing\nWhat we need to beat\n\nFeature engineering\n\n\nk-Nearest Neighbors\n\nLISA (Local Moran’s I)\n\nCount Regression Models\nSpatial Cross-Validation\n\nSpecific CV to spatial applications\n\nModel Comparison\n\nCore Logic: “Broken Windows Theory” - i.e. visual signs of ‘disorder’ lead to criminal activity\nSpatial Weights Matrix - Global Moran’s I gives us a value from [-1, 1] - Positive is similar values cluster - Negative is dissimilar values cluster - 0 is no clustering - Local Moran’s I - Test for statistical significance - so every estimate gets a p-value - Moran Scatterplot - High-high = hotspot - High-low = outliers - Low-low = coldspot - Low-high = outliers\nPoisson Distribution - Appropriate for count data - Poisson assumption: Variance = mean\nPoisson Regression - Log terms - In R, use glm() instead of lm() - Test for overdispersion - Use glm.nb() if there is overdispersion\nSpatial Cross-Validation - Standard CV might fail for spatial data because nearby observations are correlated; Spatial leakage, or model learns from neighbors of test set - Instead group together and remove neighbors together in a fold\nKernel Density Estimation - create a kernel out of different densities of observations"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html",
    "href": "weekly-notes/week-06-notes.html",
    "title": "Week 6 Notes",
    "section": "",
    "text": "transforming csv or other such data into SF st_as_af() - option: if your data has coordinates, then coords = c(\"Name of Long column\", \"name of lat column\") – and set the CRS appropriately (e.g. crs = 4326) based on the coordinate system, but then transform into something more appropriate like state plane.\nChange categorical variable (such as neighborhood name) as a numerical factor that can be used in model - mutate(name = as.factor(name)) - first category alphabetically is chosen as reference variable, and the coefficients for the rest are in comparison to that\n\n\n\nwhen the effect of one variable on the model depends on another variable (i.e. square footage matters more to price in a fancy neighborhood than nonfancy neighborhood)\ninstead of ind_var_1 + ind_var_2 , for two variables you think interact, use ind_var_1 * ind_var_2\n\n\n\n\n\ncaution interpreting coefficients with quadratic terms\n\n\n\n\n3 tricks: 1. Buffer Aggregation - draw a buffer around the house, and calculate features that fall within the buffer (e.g. number of crimes in .5 mi radius) 2. k-Nearest Neighbors (kNN) - What is the average distance to the k-nearest features (e.g. what is average distance from house to 3 nearest crimes, low = close, high = far) 3. Distance to Specific Points - Straight-line distance to important features (e.g. distance from house to city hall)\n\n\n\n\nsome variable which summarizes other potential factors (e.g. neighborhood might stand in for a number of more specific variables that are harder to find data for perhaps)\n\n\n\n\n\nwant to make sure for instance that a category exists in as many instances as you have folds (e.g. if there are only 3 examples in Rittenhouse, 10-fold might have issues)\nsolution:\n\ndrop sparse categories"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#notes",
    "href": "weekly-notes/week-06-notes.html#notes",
    "title": "Week 6 Notes",
    "section": "",
    "text": "transforming csv or other such data into SF st_as_af() - option: if your data has coordinates, then coords = c(\"Name of Long column\", \"name of lat column\") – and set the CRS appropriately (e.g. crs = 4326) based on the coordinate system, but then transform into something more appropriate like state plane.\nChange categorical variable (such as neighborhood name) as a numerical factor that can be used in model - mutate(name = as.factor(name)) - first category alphabetically is chosen as reference variable, and the coefficients for the rest are in comparison to that\n\n\n\nwhen the effect of one variable on the model depends on another variable (i.e. square footage matters more to price in a fancy neighborhood than nonfancy neighborhood)\ninstead of ind_var_1 + ind_var_2 , for two variables you think interact, use ind_var_1 * ind_var_2\n\n\n\n\n\ncaution interpreting coefficients with quadratic terms\n\n\n\n\n3 tricks: 1. Buffer Aggregation - draw a buffer around the house, and calculate features that fall within the buffer (e.g. number of crimes in .5 mi radius) 2. k-Nearest Neighbors (kNN) - What is the average distance to the k-nearest features (e.g. what is average distance from house to 3 nearest crimes, low = close, high = far) 3. Distance to Specific Points - Straight-line distance to important features (e.g. distance from house to city hall)\n\n\n\n\nsome variable which summarizes other potential factors (e.g. neighborhood might stand in for a number of more specific variables that are harder to find data for perhaps)\n\n\n\n\n\nwant to make sure for instance that a category exists in as many instances as you have folds (e.g. if there are only 3 examples in Rittenhouse, 10-fold might have issues)\nsolution:\n\ndrop sparse categories"
  },
  {
    "objectID": "labs/lab_4/Assignment4_Instructions.html",
    "href": "labs/lab_4/Assignment4_Instructions.html",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "In this lab, you will apply the spatial predictive modeling techniques demonstrated in the class exercise to a 311 service request type of your choice. You will build a complete spatial predictive model, document your process, and interpret your results.\n\n\nDue Date: November 17, 2025, 10:00AM\nDeliverable: One rendered document, posted to your portfolio website.\n\n\n\nBy completing this assignment, you will demonstrate your ability to:\n\nAdapt example code to analyze a new dataset\nBuild spatial features for predictive modeling\nApply count regression techniques to spatial data\nImplement spatial cross-validation\nInterpret and communicate model results\nCritically evaluate model performance\n\n\n\n\n\n\n\n\nVisit the Chicago 311 Service Requests dataset:\nhttps://data.cityofchicago.org/stories/s/311-Dataset-Changes-12-11-2018/d7nq-5g7t\nBrowse the available service request types (e.g., Graffiti Removal, Pothole Repair, Street Light Out, Sanitation Code Violations, etc.) and choose one violation type that interests you.\n\n\n\n\n\n\n\n\n\n\nUsing the class exercise as your template, adapt the code to analyze your chosen 311 violation type.\n\n\nWork through all major sections of the analysis:\n\n\n\nLoad your 311 data and Chicago spatial boundaries\nCreate visualizations showing the spatial distribution of your violation type\nDescribe patterns you observe\n\n\n\n\n\nCreate a 500m x 500m fishnet grid\nAggregate your violations to grid cells\nVisualize the count distribution\n\n\n\n\n\nCalculate k-nearest neighbor features\nPerform Local Moran’s I analysis\nIdentify hot spots and cold spots\nCreate distance-to-hotspot measures\nJoin any additional contextual data if you are looking for more to do and really get into this (e.g., demographics, land use)\n\n\n\n\n\nFit Poisson regression\nFit Negative Binomial regression\nCompare model fit (AIC)\n\n\n\n\n\nImplement Leave-One-Group-Out cross-validation on 2017 data\nCalculate and report error metrics (MAE, RMSE)\n\n\n\n\n\nGenerate final predictions for both years\nCompare to KDE baseline\nMap prediction errors for 2018\nAssess model performance across time\n\n\n\n\n\nDownload 2018 crimes https://data.cityofchicago.org/Public-Safety/Crimes-2018/3i3m-jwuy/about_data\nBe sure to filter for BURGLARIES (FORCED ENTRY only to match the 2017 data)\nAggregate 2018 violations to the same fishnet grid\nUse your 2017 model to predict 2018 counts\nCalculate temporal validation metrics\nCompare spatial vs. temporal validation performance\n\n\n\n\n\n\n\n\n\nFor each major section, you must explain in your own words:\n\nWhat you are doing in that step\nWhy this step is important for the analysis\nWhat you found or learned from the results\n\nDo not simply copy text from the example or from your AI friend. Think about the purpose of each technique and articulate it in your own words.\n\n\n\n\n\n\n\nYour knitted HTML document should be professional and easy to read:\n\n\n\nRemove unnecessary code, comments, or debugging lines\nKeep only essential code chunks\nUse code-fold: show in your YAML header\n\n\n\n\n\nUse headers to organize sections\nInclude a table of contents\nAdd your name and date\n\n\n\n\n\nEnsure all text renders properly (no broken markdown)\nCheck that headers appear as headers (not plain text)\nUse proper markdown formatting\n\n\n\n\n\nAll plots should have titles and labels\nMaps should be readable\nUse consistent color schemes\n\n\n\n\n\nProofread for typos\nRemove any “Your answer here” placeholders\nMake sure all code runs without errors\nMake IT NEAT. WE GOTTA LOOK GOOD HERE.\n\n\n\n\n\n\n\nBefore you submit, verify that your document includes:\n\n\n\nIntroduction: Brief description of your chosen 311 violation type and why you selected it\nData Exploration: Maps and summary statistics of your violation data\nFishnet Grid: Successfully created and visualized grid with aggregated counts\nSpatial Features: At least 2-3 spatial features calculated (k-NN, distance measures, etc.)\nLocal Moran’s I: Spatial autocorrelation analysis with interpretation\nCount Regression: Both Poisson and Negative Binomial models with comparison\nSpatial Cross-Validation: LOGO CV implemented on 2017 data with reported metrics\nTemporal Validation: 2018 data aggregated and tested with 2017 model(OPTIONAL.)\nValidation Comparison: Comparison of spatial vs. temporal validation performance\nModel Comparison: Your model vs. KDE baseline for 2018 predictions\nError Analysis: Maps and discussion of where model performs well/poorly\nWritten Explanations: Your own words explaining each step\n\n\n\n\n\nDocument knits to HTML without errors\nCode is clean and well-organized\nAll visualizations display properly\nHeaders and formatting render correctly\nFile is named successfully posted to your website"
  },
  {
    "objectID": "labs/lab_4/Assignment4_Instructions.html#assignment-overview",
    "href": "labs/lab_4/Assignment4_Instructions.html#assignment-overview",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "In this lab, you will apply the spatial predictive modeling techniques demonstrated in the class exercise to a 311 service request type of your choice. You will build a complete spatial predictive model, document your process, and interpret your results.\n\n\nDue Date: November 17, 2025, 10:00AM\nDeliverable: One rendered document, posted to your portfolio website.\n\n\n\nBy completing this assignment, you will demonstrate your ability to:\n\nAdapt example code to analyze a new dataset\nBuild spatial features for predictive modeling\nApply count regression techniques to spatial data\nImplement spatial cross-validation\nInterpret and communicate model results\nCritically evaluate model performance"
  },
  {
    "objectID": "labs/lab_4/Assignment4_Instructions.html#step-1-choose-your-311-violation-type",
    "href": "labs/lab_4/Assignment4_Instructions.html#step-1-choose-your-311-violation-type",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "Visit the Chicago 311 Service Requests dataset:\nhttps://data.cityofchicago.org/stories/s/311-Dataset-Changes-12-11-2018/d7nq-5g7t\nBrowse the available service request types (e.g., Graffiti Removal, Pothole Repair, Street Light Out, Sanitation Code Violations, etc.) and choose one violation type that interests you."
  },
  {
    "objectID": "labs/lab_4/Assignment4_Instructions.html#step-2-complete-the-analysis",
    "href": "labs/lab_4/Assignment4_Instructions.html#step-2-complete-the-analysis",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "Using the class exercise as your template, adapt the code to analyze your chosen 311 violation type.\n\n\nWork through all major sections of the analysis:\n\n\n\nLoad your 311 data and Chicago spatial boundaries\nCreate visualizations showing the spatial distribution of your violation type\nDescribe patterns you observe\n\n\n\n\n\nCreate a 500m x 500m fishnet grid\nAggregate your violations to grid cells\nVisualize the count distribution\n\n\n\n\n\nCalculate k-nearest neighbor features\nPerform Local Moran’s I analysis\nIdentify hot spots and cold spots\nCreate distance-to-hotspot measures\nJoin any additional contextual data if you are looking for more to do and really get into this (e.g., demographics, land use)\n\n\n\n\n\nFit Poisson regression\nFit Negative Binomial regression\nCompare model fit (AIC)\n\n\n\n\n\nImplement Leave-One-Group-Out cross-validation on 2017 data\nCalculate and report error metrics (MAE, RMSE)\n\n\n\n\n\nGenerate final predictions for both years\nCompare to KDE baseline\nMap prediction errors for 2018\nAssess model performance across time\n\n\n\n\n\nDownload 2018 crimes https://data.cityofchicago.org/Public-Safety/Crimes-2018/3i3m-jwuy/about_data\nBe sure to filter for BURGLARIES (FORCED ENTRY only to match the 2017 data)\nAggregate 2018 violations to the same fishnet grid\nUse your 2017 model to predict 2018 counts\nCalculate temporal validation metrics\nCompare spatial vs. temporal validation performance"
  },
  {
    "objectID": "labs/lab_4/Assignment4_Instructions.html#step-3-write-your-analysis",
    "href": "labs/lab_4/Assignment4_Instructions.html#step-3-write-your-analysis",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "For each major section, you must explain in your own words:\n\nWhat you are doing in that step\nWhy this step is important for the analysis\nWhat you found or learned from the results\n\nDo not simply copy text from the example or from your AI friend. Think about the purpose of each technique and articulate it in your own words."
  },
  {
    "objectID": "labs/lab_4/Assignment4_Instructions.html#step-4-format-your-document",
    "href": "labs/lab_4/Assignment4_Instructions.html#step-4-format-your-document",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "Your knitted HTML document should be professional and easy to read:\n\n\n\nRemove unnecessary code, comments, or debugging lines\nKeep only essential code chunks\nUse code-fold: show in your YAML header\n\n\n\n\n\nUse headers to organize sections\nInclude a table of contents\nAdd your name and date\n\n\n\n\n\nEnsure all text renders properly (no broken markdown)\nCheck that headers appear as headers (not plain text)\nUse proper markdown formatting\n\n\n\n\n\nAll plots should have titles and labels\nMaps should be readable\nUse consistent color schemes\n\n\n\n\n\nProofread for typos\nRemove any “Your answer here” placeholders\nMake sure all code runs without errors\nMake IT NEAT. WE GOTTA LOOK GOOD HERE."
  },
  {
    "objectID": "labs/lab_4/Assignment4_Instructions.html#submission-checklist",
    "href": "labs/lab_4/Assignment4_Instructions.html#submission-checklist",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "Before you submit, verify that your document includes:\n\n\n\nIntroduction: Brief description of your chosen 311 violation type and why you selected it\nData Exploration: Maps and summary statistics of your violation data\nFishnet Grid: Successfully created and visualized grid with aggregated counts\nSpatial Features: At least 2-3 spatial features calculated (k-NN, distance measures, etc.)\nLocal Moran’s I: Spatial autocorrelation analysis with interpretation\nCount Regression: Both Poisson and Negative Binomial models with comparison\nSpatial Cross-Validation: LOGO CV implemented on 2017 data with reported metrics\nTemporal Validation: 2018 data aggregated and tested with 2017 model(OPTIONAL.)\nValidation Comparison: Comparison of spatial vs. temporal validation performance\nModel Comparison: Your model vs. KDE baseline for 2018 predictions\nError Analysis: Maps and discussion of where model performs well/poorly\nWritten Explanations: Your own words explaining each step\n\n\n\n\n\nDocument knits to HTML without errors\nCode is clean and well-organized\nAll visualizations display properly\nHeaders and formatting render correctly\nFile is named successfully posted to your website"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html",
    "href": "weekly-notes/week-07-notes.html",
    "title": "Week 7 Notes",
    "section": "",
    "text": "random errors are good, clustered errors are bad\none tool to investigate - spatial autocorrelation of errors\n\nTobler’s First Law - Everything is related to everything, but near things are more related than distanct things - Is there a spatial aspect/clustering of higher vs lower errors? –&gt; Start by mapping residuals\nMoran’s I Measure of spatial autocorrelation – range -1 to +1 where -1 is perfect negative correlation (dispersion) and +1 is perfect positive correlation (clustering)\nDefining neighbors - Contiguity – polygons that share a border (queen in chess) - Distance – all within X meters, fixed threshol - k-Nearest - closest k points, adaptive distance – use this for point data like houses – r function is knn2nk() then use the results of that in nb2listw() which creates a binary matrix of whether two points are neighbors\nComputing Moran’s I - pass the weights into moran.mc() then moran_test$statistic is the Moran’s I value - moran_test$p.value shows significance - if p.value &lt; 0.05, very significant clustering! bad!\nWhat Moran’s I tells you –&gt; 1. Add more spatial features 2. Try spatial fixed effects 3. Consider spatial regression models"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#notes",
    "href": "weekly-notes/week-07-notes.html#notes",
    "title": "Week 7 Notes",
    "section": "",
    "text": "random errors are good, clustered errors are bad\none tool to investigate - spatial autocorrelation of errors\n\nTobler’s First Law - Everything is related to everything, but near things are more related than distanct things - Is there a spatial aspect/clustering of higher vs lower errors? –&gt; Start by mapping residuals\nMoran’s I Measure of spatial autocorrelation – range -1 to +1 where -1 is perfect negative correlation (dispersion) and +1 is perfect positive correlation (clustering)\nDefining neighbors - Contiguity – polygons that share a border (queen in chess) - Distance – all within X meters, fixed threshol - k-Nearest - closest k points, adaptive distance – use this for point data like houses – r function is knn2nk() then use the results of that in nb2listw() which creates a binary matrix of whether two points are neighbors\nComputing Moran’s I - pass the weights into moran.mc() then moran_test$statistic is the Moran’s I value - moran_test$p.value shows significance - if p.value &lt; 0.05, very significant clustering! bad!\nWhat Moran’s I tells you –&gt; 1. Add more spatial features 2. Try spatial fixed effects 3. Consider spatial regression models"
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-4-count-regression-models",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-4-count-regression-models",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 4: Count Regression Models",
    "text": "Part 4: Count Regression Models\nCollect crimes data and join crime counts to fishnet\n\ncrimes &lt;- read.csv(\"C:/Users/knoxk/Downloads/Crimes_-_2018_20251117.csv\")\ncrimes &lt;- crimes%&gt;%filter(!is.na(Latitude) & !is.na(Longitude))\ncrimes_sf &lt;- crimes %&gt;% st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) \ncrimes_sf &lt;- crimes_sf %&gt;% st_transform(26971)\n\ncrimes_sf%&gt;%\n  ggplot()+\n  geom_sf(alpha = 0.1, color = \"violetred\")+\n  geom_sf(data = chi_boundary, fill = NA, color = \"darkgreen\")+\n  theme_void()\n\n\n\n\n\n\n\n\n\n# Count lights out per cell\ncrime_count &lt;- st_join(crimes_sf, fishnet) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(crime_count = n())\n\n# Join back to fishnet\nfishnet &lt;- fishnet %&gt;%\n  left_join(crime_count, by = \"uniqueID\") %&gt;%\n  mutate(crime_count = replace_na(crime_count, 0))\n\n# Summary\nsummary(fishnet$crime_count)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   11.00   60.00   99.82  136.00 2188.00 \n\n\nThis shows the distribution of crimes per fishnet grid.\n\n# Fit Poisson model\nmodel_poisson &lt;- glm(\n  crime_count ~ count_lights_out + lights_out_dists,\n  data = fishnet,\n  family = poisson(link = \"log\")\n)\n\n# View results\nsummary(model_poisson)\n\n\nCall:\nglm(formula = crime_count ~ count_lights_out + lights_out_dists, \n    family = poisson(link = \"log\"), data = fishnet)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       4.436e+00  4.602e-03   963.9   &lt;2e-16 ***\ncount_lights_out  3.404e-03  1.706e-05   199.5   &lt;2e-16 ***\nlights_out_dists -3.982e-03  3.557e-05  -111.9   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 351602  on 2639  degrees of freedom\nResidual deviance: 231205  on 2637  degrees of freedom\nAIC: 244686\n\nNumber of Fisher Scoring iterations: 9\n\n# Exponentiate coefficients for interpretation\nexp(coef(model_poisson))\n\n     (Intercept) count_lights_out lights_out_dists \n      84.4003005        1.0034096        0.9960263 \n\n\nUsing a Poisson Regression where the dependent variable is the number of crimes in a grid cell and the dependent variables are the count of complaints of lights out and distance from grid center to nearest complaint of lights out. The model shows that areas with more “lights out” complaints tend to have slightly higher crime counts. Specifically, each additional street light reported as out in a grid cell is associated with a small increase in the expected number of crimes. Conversely, the farther a location is from the nearest street light outage, the slightly lower the expected crime count. In other words, crimes tend to be more common in areas where street lights are out or clustered, suggesting a very very small link between lighting conditions and crime risk.\n\n# Calculate dispersion parameter\nmodel_pois &lt;- glm(\n  crime_count ~ count_lights_out + lights_out_dists,\n  data = fishnet,\n  family = poisson\n)\n\ndispersion &lt;- sum(residuals(model_pois, type = \"pearson\")^2) / model_pois$df.residual\n\ncat(\"Dispersion parameter:\", round(dispersion, 3), \"\\n\")\n\nDispersion parameter: 19091.61 \n\n\nThis dispersion is quite enormous, and shows that the variance is significantly higher than the mean, which violates an assumption of the Poisson Model.\n\n# Fit Negative Binomial model\nmodel_nb &lt;- glm.nb(\n  crime_count ~ count_lights_out + lights_out_dists,\n  data = fishnet,\n  init.theta = 1,      # starting value for dispersion\n  control = glm.control(epsilon = 1e-8, maxit = 50)\n)\n\n# View results\nsummary(model_nb)\n\n\nCall:\nglm.nb(formula = crime_count ~ count_lights_out + lights_out_dists, \n    data = fishnet, control = glm.control(epsilon = 1e-08, maxit = 50), \n    init.theta = 0.6218860388, link = log)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       3.563e+00  4.293e-02   82.99   &lt;2e-16 ***\ncount_lights_out  7.790e-03  2.596e-04   30.01   &lt;2e-16 ***\nlights_out_dists -7.746e-04  6.132e-05  -12.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.6219) family taken to be 1)\n\n    Null deviance: 4221.3  on 2639  degrees of freedom\nResidual deviance: 3197.0  on 2637  degrees of freedom\nAIC: 27647\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.6219 \n          Std. Err.:  0.0168 \n\n 2 x log-likelihood:  -27638.6580 \n\n# Compare to Poisson\nAIC(model_poisson)\n\n[1] 244685.8\n\nAIC(model_nb) \n\n[1] 27646.66\n\n\nA Negative Binomial model is a type of count regression that estimates the relationship between predictor variables and a count outcome while accounting for overdispersion, making it appropriate when the variance of the counts exceeds the mean, as is the case here.\nEach additional street light reported as out is associated with an approximately 0.78% increase in expected crime counts, while each additional meter further from the nearest lights-out complaint is associated with an approximately 0.08% decrease. These effects are statistically significant, though quite small in magnitude, and the negative binomial model accounts for overdispersion in crime counts, providing more reliable estimates than the Poisson Model. This is supported by the NB model’s much lower AIC."
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-5-spatial-cross-validation",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-5-spatial-cross-validation",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 5: Spatial Cross-Validation",
    "text": "Part 5: Spatial Cross-Validation\nJoin districts to fishnet:\n\ncrime_count_dist &lt;- crimes_sf %&gt;%\n  st_join(fishnet) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID, District) %&gt;% \n  summarize(crime_count = n(), .groups = \"drop\")  \n\ncrime_count_dist_unique &lt;- crime_count_dist %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarise(\n    crime_count = sum(crime_count, na.rm = TRUE),\n    District = first(District)  # pick first district if multiple\n  )\n\n# Join back to fishnet\nfishnet_2018 &lt;- fishnet %&gt;%\n  left_join(crime_count_dist_unique, by = \"uniqueID\") %&gt;%\n  mutate(\n    crime_count = replace_na(fishnet$crime_count, 0),\n    District = District  \n  )%&gt;%filter(!is.na(District))\n\nfishnet_2018 &lt;- fishnet_2018 %&gt;% \n  filter(District != 1 & District != 31)\n\n\n# Get unique districts\ndistricts &lt;- unique(fishnet_2018$District)\n\n# Initialize results\ncv_results &lt;- list()\n\n# Loop through districts\nfor (dist in districts) {\n  # Split data\n  train_data &lt;- fishnet_2018 %&gt;% filter(District != dist)\n  test_data &lt;- fishnet_2018 %&gt;% filter(District == dist)\n  \n  # Fit model on training data\n  model_cv &lt;- glm.nb(\n  crime_count ~ count_lights_out + lights_out_dists,\n  data = train_data,\n  init.theta = .1,  # starting value for dispersion\n  control = glm.control(maxit = 50, epsilon = 1e-8)\n)\n  \n  # Predict on test data\n  test_data$prediction &lt;- predict(model_cv, test_data, type = \"response\")\n  \n  # Store results\n  cv_results[[dist]] &lt;- test_data\n}\n\n# Combine all predictions\nall_predictions &lt;- bind_rows(cv_results)\n\n# Calculate metrics by district\ncv_metrics &lt;- all_predictions %&gt;%\n  group_by(District) %&gt;%\n  summarize(\n    MAE = mean(abs(crime_count - prediction)),\n    RMSE = sqrt(mean((crime_count - prediction)^2)),\n    ME = mean(crime_count - prediction)\n  )\n\n\n# Map prediction errors\nall_predictions &lt;- all_predictions %&gt;%\n  mutate(\n    error = crime_count - prediction,\n    abs_error = abs(error),\n    pct_error = (prediction - crime_count) / (crime_count + 1) * 100\n  )\n\n# Visualize\nggplot(all_predictions) +\n  geom_sf(aes(fill = error), color = NA) +\n  scale_fill_gradient2(\n    low = \"blue\", mid = \"white\", high = \"red\",\n    midpoint = 0,\n    name = \"Error\"\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Red = Over-prediction, Blue = Under-prediction\") +\n  theme_void()\n\n\n\n\n\n\n\n\nThis analysis shows that under-predictions of crimes produced by our model are concentrated in south Chicago, whereas over-predilections are concentrated in more central areas. This indicates that street lights outtages are not able to account for the distribution of crime across the city."
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-6-model-evaluation",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-6-model-evaluation",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 6: Model Evaluation",
    "text": "Part 6: Model Evaluation\n\npreds &lt;- all_predictions %&gt;%\n  sf::st_drop_geometry() %&gt;%\n  dplyr::select(uniqueID, prediction)\n\nfishnet_2018 &lt;- fishnet_2018 %&gt;%\n  dplyr::left_join(preds, by = \"uniqueID\")\n\n# Step 1: Convert to point pattern (ppp) object\nlights_out_ppp &lt;- as.ppp(\n  X = st_coordinates(all_lights_out),\n  W = as.owin(st_bbox(chi_boundary))\n)\n\n# Step 2: Calculate KDE\nkde_surface &lt;- density.ppp(\n  lights_out_ppp,\n  sigma = 1000,  # Bandwidth in meters\n  edge = TRUE    # Edge correction\n)\n\n# Step 3: Extract values to fishnet cells\nfishnet_2018$kde_risk &lt;- raster::extract(\n  raster(kde_surface),\n  st_centroid(fishnet_2018)\n)\n\n# Standardize to 0-1 scale for comparison\nfishnet_2018$kde_risk &lt;- (fishnet_2018$kde_risk - min(fishnet_2018$kde_risk, na.rm=T)) / \n                     (max(fishnet_2018$kde_risk, na.rm=T) - min(fishnet_2018$kde_risk, na.rm=T))\n\n# Create quintiles (5 equal groups)\nfishnet_2018$model_risk_category &lt;- cut(\n  fishnet_2018$prediction,\n  breaks = quantile(fishnet_2018$prediction, probs = seq(0, 1, 0.2)),\n  labels = c(\"1st (Lowest)\", \"2nd\", \"3rd\", \"4th\", \"5th (Highest)\"),\n  include.lowest = TRUE\n)\n\nfishnet_2018$kde_risk_category &lt;- cut(\n  fishnet_2018$kde_risk,\n  breaks = quantile(fishnet_2018$kde_risk, probs = seq(0, 1, 0.2)),\n  labels = c(\"1st (Lowest)\", \"2nd\", \"3rd\", \"4th\", \"5th (Highest)\"),\n  include.lowest = TRUE\n)\n\n\np3 &lt;- ggplot(fishnet_2018) +\n  geom_sf(aes(fill = model_risk_category), color = NA) +\n  labs(title = \"Model Risk Quintiles\") +\n  theme_void()\n\np4 &lt;- ggplot(fishnet_2018) +\n  geom_sf(aes(fill = kde_risk_category), color = NA) +\n  labs(title = \"KDE Risk Quintiles\") +\n  theme_void()\n\np3 + p4\n\n\n\n\n\n\n\n\n\n# Spatial join: 2018 crimes to fishnet with risk categories\nresults_2018 &lt;- st_join(fishnet_2018, crimes_sf) %&gt;%\n  group_by(model_risk_category) %&gt;%\n  summarize(crimes_sf = n()) %&gt;%\n  mutate(pct_of_total = 100 * crimes_sf / sum(crimes_sf))%&gt;%\n  dplyr::select(\"model_risk_category\", \"pct_of_total\")\n\nkde_2018 &lt;- st_join(fishnet_2018, crimes_sf) %&gt;%\n  group_by(kde_risk_category) %&gt;%\n  summarize(crimes_sf = n()) %&gt;%\n  mutate(pct_of_total = 100 * crimes_sf / sum(crimes_sf))%&gt;%\n  dplyr::select(\"kde_risk_category\", \"pct_of_total\")\n\nkde_2018&lt;- kde_2018%&gt;%st_drop_geometry\nresults_2018&lt;- results_2018%&gt;%st_drop_geometry\n\nresults_2018_clean &lt;- results_2018 %&gt;%\n  rename(\n    `Model Risk Category` = model_risk_category,\n    `Percent of Crimes` = pct_of_total\n  ) %&gt;%\n  mutate(\n    `Percent of Crimes` = percent(`Percent of Crimes`/100, accuracy = 0.1)\n  )\n\nkde_2018_clean &lt;- kde_2018 %&gt;%\n  rename(\n    `KDE Risk Category` = kde_risk_category,\n    `Percent of Crimes` = pct_of_total\n  ) %&gt;%\n  mutate(\n    `Percent of Crimes` = percent(`Percent of Crimes`/100, accuracy = 0.1)\n  )\n\nkbl(\n  list(results_2018_clean, kde_2018_clean),\n  caption = \"Model Risk Areas vs KDE Risk Areas and Percentage of Crimes in Risk Areas\"\n) %&gt;% \n  kable_classic(full_width = FALSE)\n\n\nModel Risk Areas vs KDE Risk Areas and Percentage of Crimes in Risk Areas\n\n\n\n\n\n\n\n\n\nModel Risk Category\nPercent of Crimes\n\n\n\n\n1st (Lowest)\n5.2%\n\n\n2nd\n15.3%\n\n\n3rd\n20.3%\n\n\n4th\n24.9%\n\n\n5th (Highest)\n34.3%\n\n\n\n\n\n\nKDE Risk Category\nPercent of Crimes\n\n\n\n\n1st (Lowest)\n4.8%\n\n\n2nd\n16.2%\n\n\n3rd\n18.9%\n\n\n4th\n22.5%\n\n\n5th (Highest)\n37.7%\n\n\n\n\n\n\n\n\nAs this table shows, the highest risk area identified by the model account for a slightly smaller percentage of total crimes than the KDE highest risk area. This shows that the model based on street light outtage reports slightly underperforms simply examining peak crime location by historical data in order to account for higher proportions of crime."
  }
]