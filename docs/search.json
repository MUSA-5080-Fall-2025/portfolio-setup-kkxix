[
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes",
    "section": "",
    "text": "Main concepts:\n\nBest practices for coding in R\nWhat are algorithms\nCensus data foundations\nData Analytics are Subjective\n\nTechnical skills covered:\n\ndplyr basics practice\ntidycensus"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 Notes",
    "section": "",
    "text": "Main concepts:\n\nBest practices for coding in R\nWhat are algorithms\nCensus data foundations\nData Analytics are Subjective\n\nTechnical skills covered:\n\ndplyr basics practice\ntidycensus"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes",
    "section": "Reflection",
    "text": "Reflection"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\nHello! I am Katie Knox, a first-year MCP student at University of Pennsylvania. By undergraduate degree was in computer science and francophone studies at Swarthmore College, and for the past three years I have worked for National Digital Inclusion Alliance. I am taking this course because I want to learn how to apply data analysis skills towards public interest problems. I just moved from Toronto back to Philadelphia, so go birds! (Raptors and Eagles)\n\n\n\n\nEmail: knox1@upenn.edu\nGitHub: @kkxix"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Hello! I am Katie Knox, a first-year MCP student at University of Pennsylvania. By undergraduate degree was in computer science and francophone studies at Swarthmore College, and for the past three years I have worked for National Digital Inclusion Alliance. I am taking this course because I want to learn how to apply data analysis skills towards public interest problems. I just moved from Toronto back to Philadelphia, so go birds! (Raptors and Eagles)"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: knox1@upenn.edu\nGitHub: @kkxix"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Main concepts:\n\nBreakdown of syllabus (stay on toes for weekly quizzes)\n\nTechnical skills covered\n\nRStudio\nQuarto\nGit and Github"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Main concepts:\n\nBreakdown of syllabus (stay on toes for weekly quizzes)\n\nTechnical skills covered\n\nRStudio\nQuarto\nGit and Github"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nR functions:\n\nselect() - choose columns\nfilter() - choose rows\nmutate() - create new variables\nsummarize() - calculate statistics\ngroup_by() - operate on groups\nnames() - column names\nglimpse() - little glimpse of data\n\nQuarto features learned:\n\nQuarto is cool and hip :-)\nA way to show code snippets, run code blocks, share visuals, and share text as a webpage or slides (kinda like python notebook or observable)\n\n\ncommon syntax pattern: save_to &lt;- function(df, ...)\neverything except column: select(df, -col_to_exclude)\nanother common pattern: mutate(df, new_col = case_when(condition ~ code, TRUE ~ code2)\nIconic couple: group_by() and summarize() - group_by() – create categories out of values in a column - summarize() – summarize values in another column by a user-defined function - summarize(n=n()) – basic count of grouped categories\npiped syntax pattern: save_to &lt;- df %&gt;% function(...) %&gt;% funciton(...) ^^you don’t need df as first argument if first you state df and pass to functions through pipe"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWhy don’t you need to include df as argument when you are using piping syntax?"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nGot a preview of policy topics we will cover over semester:\n\nhousing\nrecidivism\nbikeshare equity\n\nWhat sets this apart from stats is emphasis on prediction"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nHype for this course!\nPhillies &gt; Mets"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html",
    "href": "labs/lab0/scripts/lab0_template.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you’ll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#data-structure-exploration",
    "href": "labs/lab0/scripts/lab0_template.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\nglimpse(car_data)\n\nRows: 50,000\nColumns: 7\n$ Manufacturer          &lt;chr&gt; \"Ford\", \"Porsche\", \"Ford\", \"Toyota\", \"VW\", \"Ford…\n$ Model                 &lt;chr&gt; \"Fiesta\", \"718 Cayman\", \"Mondeo\", \"RAV4\", \"Polo\"…\n$ `Engine size`         &lt;dbl&gt; 1.0, 4.0, 1.6, 1.8, 1.0, 1.4, 1.8, 1.4, 1.2, 2.0…\n$ `Fuel type`           &lt;chr&gt; \"Petrol\", \"Petrol\", \"Diesel\", \"Hybrid\", \"Petrol\"…\n$ `Year of manufacture` &lt;dbl&gt; 2002, 2016, 2014, 1988, 2006, 2018, 2010, 2015, …\n$ Mileage               &lt;dbl&gt; 127300, 57850, 39190, 210814, 127869, 33603, 866…\n$ Price                 &lt;dbl&gt; 3074, 49704, 24072, 1705, 4101, 29204, 14350, 30…\n\n# Check the column names\nnames(car_data)\n\n[1] \"Manufacturer\"        \"Model\"               \"Engine size\"        \n[4] \"Fuel type\"           \"Year of manufacture\" \"Mileage\"            \n[7] \"Price\"              \n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 × 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym…           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer: - How many rows and columns does the dataset have? - What types of variables do you see (numeric, character, etc.)? - Are there any column names that might cause problems? Why?\nYour answers:\n\nRows: 50k\nColumns: 7\n\nVariable types: There are both character and numeric (double) types of variables\nProblematic names: ‘Engine size’ ‘Fuel type’ and ‘Year of manufacture’ all have a space in them"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#tibble-vs-data-frame",
    "href": "labs/lab0/scripts/lab0_template.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 49,990 more rows\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\ncar_df &lt;- as.data.frame(car_data)\nhead(car_df)\n\n  Manufacturer      Model Engine size Fuel type Year of manufacture Mileage\n1         Ford     Fiesta         1.0    Petrol                2002  127300\n2      Porsche 718 Cayman         4.0    Petrol                2016   57850\n3         Ford     Mondeo         1.6    Diesel                2014   39190\n4       Toyota       RAV4         1.8    Hybrid                1988  210814\n5           VW       Polo         1.0    Petrol                2006  127869\n6         Ford      Focus         1.4    Petrol                2018   33603\n  Price\n1  3074\n2 49704\n3 24072\n4  1705\n5  4101\n6 29204\n\n\nQuestion: What differences do you notice in how they print?\nYour answer: The tibble automatically shows only 10 rows, and wraps the column names with spaces in them in ’’ marks. It also says what data type the different variables are. The data frame shows wayyyyyyy more rows by defaults and is formatted more consistently."
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#selecting-columns",
    "href": "labs/lab0/scripts/lab0_template.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\n# Select just Model and Mileage columns\ncar_df %&gt;% select(Model, Mileage) %&gt;% head()\n\n       Model Mileage\n1     Fiesta  127300\n2 718 Cayman   57850\n3     Mondeo   39190\n4       RAV4  210814\n5       Polo  127869\n6      Focus   33603\n\n# Select Manufacturer, Price, and Fuel type\ncar_df %&gt;% select(Manufacturer, Price, 'Fuel type') %&gt;% head()\n\n  Manufacturer Price Fuel type\n1         Ford  3074    Petrol\n2      Porsche 49704    Petrol\n3         Ford 24072    Diesel\n4       Toyota  1705    Hybrid\n5           VW  4101    Petrol\n6         Ford 29204    Petrol\n\n# Challenge: Select all columns EXCEPT Engine Size\ncar_df %&gt;% select(-'Engine size') %&gt;% head()\n\n  Manufacturer      Model Fuel type Year of manufacture Mileage Price\n1         Ford     Fiesta    Petrol                2002  127300  3074\n2      Porsche 718 Cayman    Petrol                2016   57850 49704\n3         Ford     Mondeo    Diesel                2014   39190 24072\n4       Toyota       RAV4    Hybrid                1988  210814  1705\n5           VW       Polo    Petrol                2006  127869  4101\n6         Ford      Focus    Petrol                2018   33603 29204"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#renaming-columns",
    "href": "labs/lab0/scripts/lab0_template.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet’s fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\ncar_data &lt;- rename(car_data, year = 'Year of manufacture')\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\" \"Model\"        \"Engine size\"  \"Fuel type\"    \"year\"        \n[6] \"Mileage\"      \"Price\"       \n\n\nQuestion: Why did we need backticks around Year of manufacture but not around year?\nYour answer: We don’t need backticks around year because it has no spaces."
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#calculate-car-age",
    "href": "labs/lab0/scripts/lab0_template.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create an 'age' column (2025 minus year of manufacture)\ncar_data &lt;- mutate(car_data, age = 2025-year)\n\n# Create a mileage_per_year column  \ncar_data &lt;- mutate(car_data, mileage_per_year = Mileage/age)\n\n# Look at your new columns\nselect(car_data, Model, year, age, Mileage, mileage_per_year)\n\n# A tibble: 50,000 × 5\n   Model       year   age Mileage mileage_per_year\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1 Fiesta      2002    23  127300            5535.\n 2 718 Cayman  2016     9   57850            6428.\n 3 Mondeo      2014    11   39190            3563.\n 4 RAV4        1988    37  210814            5698.\n 5 Polo        2006    19  127869            6730.\n 6 Focus       2018     7   33603            4800.\n 7 Mondeo      2010    15   86686            5779.\n 8 Prius       2015    10   30663            3066.\n 9 Polo        2012    13   73470            5652.\n10 Focus       1992    33  262514            7955.\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#categorize-cars",
    "href": "labs/lab0/scripts/lab0_template.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, its is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is mid-range (use case_when)\ncar_data &lt;- car_data %&gt;% mutate(price_category = case_when(\n  Price &lt; 15000 ~ \"budget\",\n  Price &gt;= 15000 & Price &lt; 30000 ~ \"midrange\",\n  Price &gt;= 30000 ~ \"luxury\"\n))\n\n\n# Check your categories select the new column and show it\nselect(car_data, price_category)\n\n# A tibble: 50,000 × 1\n   price_category\n   &lt;chr&gt;         \n 1 budget        \n 2 luxury        \n 3 midrange      \n 4 budget        \n 5 budget        \n 6 midrange      \n 7 budget        \n 8 luxury        \n 9 budget        \n10 budget        \n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#basic-filtering",
    "href": "labs/lab0/scripts/lab0_template.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\nfilter(car_data, Manufacturer == \"Toyota\")\n\n# A tibble: 12,554 × 10\n   Manufacturer Model `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4            1.8 Hybrid       1988  210814  1705    37\n 2 Toyota       Prius           1.4 Hybrid       2015   30663 30297    10\n 3 Toyota       RAV4            2.2 Petrol       2007   79393 16026    18\n 4 Toyota       Yaris           1.4 Petrol       1998   97286  4046    27\n 5 Toyota       RAV4            2.4 Hybrid       2003  117425 11667    22\n 6 Toyota       Yaris           1.2 Petrol       1992  245990   720    33\n 7 Toyota       RAV4            2   Hybrid       2018   28381 52671     7\n 8 Toyota       Prius           1   Hybrid       2003  115291  6512    22\n 9 Toyota       Prius           1   Hybrid       1990  238571   961    35\n10 Toyota       Prius           1.8 Hybrid       2017   31958 38961     8\n# ℹ 12,544 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with mileage less than 30,000\nfilter(car_data, Mileage&lt;30000)\n\n# A tibble: 5,402 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 Ford         Mondeo               1.6 Diesel       2015   21834 28435    10\n 9 VW           Passat               1.6 Diesel       2018   22122 36634     7\n10 VW           Passat               1.4 Diesel       2020   21413 39310     5\n# ℹ 5,392 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find luxury cars (from price category) with low mileage\ncar_data %&gt;% filter(Mileage&lt;30000) %&gt;% filter(price_category == \"luxury\")\n\n# A tibble: 3,257 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 VW           Passat               1.6 Diesel       2018   22122 36634     7\n 9 VW           Passat               1.4 Diesel       2020   21413 39310     5\n10 Toyota       RAV4                 2.4 Petrol       2021    6829 66031     4\n# ℹ 3,247 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#multiple-conditions",
    "href": "labs/lab0/scripts/lab0_template.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Honda OR Nissan\nfilter(car_data, Manufacturer == \"Honda\" | Manufacturer == \"Nissan\")\n\n# A tibble: 0 × 10\n# ℹ 10 variables: Manufacturer &lt;chr&gt;, Model &lt;chr&gt;, Engine size &lt;dbl&gt;,\n#   Fuel type &lt;chr&gt;, year &lt;dbl&gt;, Mileage &lt;dbl&gt;, Price &lt;dbl&gt;, age &lt;dbl&gt;,\n#   mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with price between $20,000 and $35,000\nfilter(car_data, Price &gt;20000 & Price &lt; 35000)\n\n# A tibble: 7,301 × 10\n   Manufacturer Model  `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Mondeo           1.6 Diesel       2014   39190 24072    11\n 2 Ford         Focus            1.4 Petrol       2018   33603 29204     7\n 3 Toyota       Prius            1.4 Hybrid       2015   30663 30297    10\n 4 Toyota       Prius            1.4 Hybrid       2016   43893 29946     9\n 5 Toyota       Prius            1.4 Hybrid       2016   43130 30085     9\n 6 VW           Passat           1.6 Petrol       2016   64344 23641     9\n 7 Ford         Mondeo           1.6 Diesel       2015   21834 28435    10\n 8 BMW          M5               4.4 Petrol       2008  109941 31711    17\n 9 BMW          Z4               2.2 Petrol       2014   61332 26084    11\n10 Porsche      911              3.5 Petrol       2003  107705 24378    22\n# ℹ 7,291 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find diesel cars less than 10 years old\ncar_data %&gt;% filter(`Fuel type` == \"Diesel\" & age &lt; 10)\n\n# A tibble: 2,040 × 10\n   Manufacturer Model   `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta            1   Diesel       2017   38370 16257     8\n 2 VW           Passat            1.6 Diesel       2018   22122 36634     7\n 3 VW           Passat            1.4 Diesel       2020   21413 39310     5\n 4 BMW          X3                2   Diesel       2018   27389 44018     7\n 5 Ford         Mondeo            2   Diesel       2016   51724 28482     9\n 6 Porsche      Cayenne           2.6 Diesel       2019   20147 76182     6\n 7 VW           Polo              1.2 Diesel       2018   37411 19649     7\n 8 Ford         Mondeo            1.8 Diesel       2016   29439 30886     9\n 9 Ford         Mondeo            1.4 Diesel       2020   18929 37720     5\n10 Ford         Mondeo            1.4 Diesel       2018   42017 28904     7\n# ℹ 2,030 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: 2040"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#basic-summaries",
    "href": "labs/lab0/scripts/lab0_template.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\navg_mileage_by_fuel_type &lt;- car_data %&gt;%\n  group_by(`Fuel type`) %&gt;%\n  summarize(avg_mileage = mean(Mileage, na.rm = TRUE))\n\navg_mileage_by_fuel_type\n\n# A tibble: 3 × 2\n  `Fuel type` avg_mileage\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Diesel          112667.\n2 Hybrid          111622.\n3 Petrol          112795.\n\n# Count cars by manufacturer\ncars_per_manufacturer &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(count = n())\n\ncars_per_manufacturer\n\n# A tibble: 5 × 2\n  Manufacturer count\n  &lt;chr&gt;        &lt;int&gt;\n1 BMW           4965\n2 Ford         14959\n3 Porsche       2609\n4 Toyota       12554\n5 VW           14913"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#categorical-summaries",
    "href": "labs/lab0/scripts/lab0_template.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories\ncar_data %&gt;% \n  group_by(price_category) %&gt;%\n  summarise(frequency = n())\n\n# A tibble: 3 × 2\n  price_category frequency\n  &lt;chr&gt;              &lt;int&gt;\n1 budget             34040\n2 luxury              6179\n3 midrange            9781"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#notes",
    "href": "weekly-notes/week-02-notes.html#notes",
    "title": "Week 2 Notes",
    "section": "Notes",
    "text": "Notes\n\nCopying labs from MUSA repo to portfolio repo\n\ntemplate goes to scripts folder, data goes to data folder\n\n\n\nLab0 / General workflow in R\n\nWant to avoid “hoarding” tibbles, try not to create a new tibble for every operation\nMay way to read in data and store as a ‘pristine’ version, but then create a working version\nUse piping to string together operations, no need to store in-between temporary tibbles\n\n\n\nAlgorithmic Decision-Making & Census Data\n\nAlgorithms = set of instructions for solving a problem/completing a task\nAlgorithmic decision-making in government\n\nSystems used to assist or replace human decision-making (humans may be inefficient, flawed/biased)\nDecisions are made based on predictions from models that process historical data relevant to the decision-making process:\n\nInputs = features = predictors = independent variables = x\nOutputs = labels - outcome = dependent variable = y\n\nExamples:\n\nRecidivism risk for bail and sentencing decisions\nMortgage lending and tenant screening\npatient care prioritization and health care resource allocation\n\n\nClarifying terms:\n\nData science -&gt; engineering, algorithms, and methods\nData analytic -&gt;applying data science to other disciplines\nMachine Learning -&gt; algorithms for classification and prediction/regression\nAI -&gt; Algorithms that adjust and improve across iterations (neural networks, deep learning)\n\nGov’t data collection historically\n\ncivic registrations systems\ncensus data\nadministration records\noperations research – post-WWII efforts to determine things like the best place for hospitals etc\n\nNew data sources\n\nMore data available from both official and ‘unofficial’ or ‘accidental’ sources like Instagram\nTurn from explanation to prediction\nMessier data\n\nWhy use algos in gov’t?\n\nEfficiency - process more cases faster\nConsistency - same process applied equally (In theory)\nObjectivity - remove human bias\nCost-saving - remove costly human labors\n\nBig But – Data Analytics are Subjective\n\nAt every step, there are human choices:\n\nData cleaning decisions\nRecoding/classification\nData collection biases\nImperfect proxies\nInterpreting results\nWhat variables you include in your model\n\n\nExample - Healthcare Algorithm Bias\n\nAlgorithm to identify high-risk patients systematically discriminated against Black patients, why?\n\nAlgo used healthcare cost as a proxy for need (high cost = more risk, less cost = low risk)\nProblem: black patients incur lower costs because of exclusion from health care and unequal access\n\nResult: Black patients were systematically not prioritized\n\nExample - COMPAS Recidivism Prediction\n\nAlgo flagged 2x as likely Black defendants as White as high risk of reoffending, why?\n\nData came from pigs\n\n\nExample - Dutch Welfare Fraud Detection\n\nAlgo disproportionately targeted vulnerable populations\n\n\n\n\nScenario\n\nAutomated traffic enforcements\n\nHighest traffic volume + Mots crashes as proxy for places needing more traffic enforcement\nBlind spot -&gt; unreported crashes (folks avoiding insurance fees for instance)\nHarm -&gt; Could under-enforce actually dangerous intersections, otherwise could be over-enforcement in over-policed areas\nSafeguard -&gt; Cap disparities across areas\n\n\n\n\nCensus Data Foundations\n\nDecennial census mandated by constitution for allocating representative governments\n\nOnly 9 questions on 10y census – age, race, sex, housing\n\nACS\n\n3% of households surveyed annually\nDetailed questions\n\n2020 Innovation: Differential Privacy\n\nadded noise to protect privacy, but this leads to weird edge cases like people living under water\n\nAccessing Census Dat ain R\n\ntidycensus !!! hooray\n\nStructure:\n\nData organized in tables\nEach table has multiple variables\n\n……E = estimate variable\n…..M = MOE variable\n\n\nRule of thumb\n\nLarge MOE : estimate means less reliable\nSmall MOE : estimate means more reliable\n\nTIGER/Line Files\n\nGeographic shape data not automatically included in ACS tables, need to add on\nNow released as shapefiles\n\nTwo types of census outputs:\n\nWide\n\nColumns: Tract, Variable_1, Variable_2…\ni.e. One row per tract\n\nLong\n\nColumns: Tract, Var, Value\ni.e. Tract * variable number of rows"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html",
    "href": "labs/lab1/scripts/assignment1_template.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Michigan Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#scenario",
    "href": "labs/lab1/scripts/assignment1_template.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Michigan Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#learning-objectives",
    "href": "labs/lab1/scripts/assignment1_template.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#submission-instructions",
    "href": "labs/lab1/scripts/assignment1_template.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#data-retrieval",
    "href": "labs/lab1/scripts/assignment1_template.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\n\n# Display the first few rows"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#data-quality-assessment",
    "href": "labs/lab1/scripts/assignment1_template.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#high-uncertainty-counties",
    "href": "labs/lab1/scripts/assignment1_template.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\n\n# Format as table with kable() - include appropriate column names and caption\n\nData Quality Commentary:\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#focus-area-selection",
    "href": "labs/lab1/scripts/assignment1_template.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nComment on the output: [write something :)]"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#tract-level-demographics",
    "href": "labs/lab1/scripts/assignment1_template.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\n\n# Add readable tract and county name columns using str_extract() or similar"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#demographic-analysis",
    "href": "labs/lab1/scripts/assignment1_template.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\n\n# Create a nicely formatted table of your results using kable()"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#moe-analysis-for-demographic-variables",
    "href": "labs/lab1/scripts/assignment1_template.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\n# Create summary statistics showing how many tracts have data quality issues"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#pattern-analysis",
    "href": "labs/lab1/scripts/assignment1_template.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nPattern Analysis: [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?]"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#analysis-integration-and-professional-summary",
    "href": "labs/lab1/scripts/assignment1_template.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\n[Your integrated 4-paragraph summary here]"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#specific-recommendations",
    "href": "labs/lab1/scripts/assignment1_template.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\n# Format as a professional table with kable()\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: [List counties with high confidence data and explain why they’re appropriate]\nCounties requiring additional oversight: [List counties with moderate confidence data and describe what kind of monitoring would be needed]\nCounties needing alternative approaches: [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.]"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#questions-for-further-investigation",
    "href": "labs/lab1/scripts/assignment1_template.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[List 2-3 questions that your analysis raised that you’d like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_template.html#submission-checklist",
    "href": "labs/lab1/scripts/assignment1_template.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html",
    "href": "labs/lab1/scripts/assignment1_kk.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Michigan Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#scenario",
    "href": "labs/lab1/scripts/assignment1_kk.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Michigan Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#learning-objectives",
    "href": "labs/lab1/scripts/assignment1_kk.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#submission-instructions",
    "href": "labs/lab1/scripts/assignment1_kk.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#data-retrieval",
    "href": "labs/lab1/scripts/assignment1_kk.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\nmi_county_pop_income &lt;- get_acs(\n  geography = \"county\", \n  variables = c(\n    median_household_income = \"B19013_001\",\n    total_pop = \"B01003_001\"),\n  state = \"MI\",\n  year = 2022,\n  output= \"wide\",\n  survey = \"acs5\")\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\nmi_county_pop_income &lt;- mi_county_pop_income %&gt;% \n  mutate(County_Name = str_remove(NAME, \" County, Michigan\")) \n\n# Display the first few rows\nhead(mi_county_pop_income)\n\n# A tibble: 6 × 7\n  GEOID NAME            median_household_inc…¹ median_household_inc…² total_popE\n  &lt;chr&gt; &lt;chr&gt;                            &lt;dbl&gt;                  &lt;dbl&gt;      &lt;dbl&gt;\n1 26001 Alcona County,…                  50295                   2243      10238\n2 26003 Alger County, …                  55528                   2912       8866\n3 26005 Allegan County…                  75543                   2369     120189\n4 26007 Alpena County,…                  49133                   2119      28911\n5 26009 Antrim County,…                  68850                   3115      23662\n6 26011 Arenac County,…                  53487                   2018      15031\n# ℹ abbreviated names: ¹​median_household_incomeE, ²​median_household_incomeM\n# ℹ 2 more variables: total_popM &lt;dbl&gt;, County_Name &lt;chr&gt;"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#data-quality-assessment",
    "href": "labs/lab1/scripts/assignment1_kk.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\nmi_county_pop_income &lt;- mi_county_pop_income %&gt;%\n  mutate(\n    income_moe_percent = (median_household_incomeM/ median_household_incomeE)*100\n      ) %&gt;%\n  mutate(\n    income_e_reliability = case_when(\n      income_moe_percent &lt; 5 ~ \"High Confidence\",\n      income_moe_percent &gt;=5 & income_moe_percent &lt;= 10 ~ \"Moderate Confidence\",\n      income_moe_percent &gt; 10 ~ \"Low Confidence\"\n    )\n  ) %&gt;%\n  mutate(\n    unreliable_income_e = case_when(\n      income_e_reliability == \"Low Confidence\" ~ TRUE,\n      TRUE ~ FALSE\n    )\n  )\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\nincome_reliability_summary &lt;- mi_county_pop_income %&gt;%\n  group_by(income_e_reliability) %&gt;%\n  summarise(num_counties = n()) %&gt;%\n  mutate(\n    percent_of_counties = (num_counties/ sum(num_counties)) *100\n  )"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#high-uncertainty-counties",
    "href": "labs/lab1/scripts/assignment1_kk.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\ntop_5_unreliable &lt;- mi_county_pop_income %&gt;%\n  arrange(desc(income_moe_percent)) %&gt;%\n  top_n(5, income_moe_percent) %&gt;%\n  select(NAME, median_household_incomeE, median_household_incomeM, income_moe_percent, income_e_reliability)\n  \n\n# Format as table with kable() - include appropriate column names and caption\ntop_5_unreliable_formatted &lt;- top_5_unreliable %&gt;%\n  mutate(\n    median_household_incomeE = paste0(\"$\", format(median_household_incomeE, big.mark = \",\", scientific = FALSE)),\n    median_household_incomeM = paste0(\"±\", \"$\", format(median_household_incomeM, big.mark = \",\", scientific = FALSE)),\n    income_moe_percent = paste0(round(income_moe_percent, 2), \"%\"),\n  )\n\n  kable(top_5_unreliable_formatted,\n      col.names = c(\"County Name\", \"Median Income\", \"MOE\", \"MOE Percent of Estimate\", \"Reliability of Estimate\"),\n      align = \"l\",\n      caption = \"5 Least Reliable Median Income Estimates of Counties in Michigan\",\n      digits = 2,\n      format.args = list(big.mark = \",\", scientific = FALSE))\n\n\n5 Least Reliable Median Income Estimates of Counties in Michigan\n\n\n\n\n\n\n\n\n\nCounty Name\nMedian Income\nMOE\nMOE Percent of Estimate\nReliability of Estimate\n\n\n\n\nKeweenaw County, Michigan\n$55,560\n±$7,301\n13.14%\nLow Confidence\n\n\nSchoolcraft County, Michigan\n$55,071\n±$6,328\n11.49%\nLow Confidence\n\n\nGogebic County, Michigan\n$47,913\n±$4,766\n9.95%\nModerate Confidence\n\n\nOtsego County, Michigan\n$62,865\n±$5,910\n9.4%\nModerate Confidence\n\n\nMontmorency County, Michigan\n$46,345\n±$3,796\n8.19%\nModerate Confidence\n\n\n\n\n\nData Quality Commentary:\nAssistance programs and policies aimed at community development may look at median income to identify priority areas. Areas with lower reliability may have a far lower or higher actual median income than what is reported in the census, and could affect the calculations of how resources are allocated to those areas. Additionally, higher unreliability is more likely in less population geographies because it is harder to get a representative sample, so smaller or less densely populated geographies, which are already often economically vulnerable, may be further marginalized by being overlooked in the prioritization of assistance programs based on these metrics."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#focus-area-selection",
    "href": "labs/lab1/scripts/assignment1_kk.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\ncounties &lt;- c(\"Kalamazoo\", \"Crawford\", \"Keweenaw\")\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\nselected_counties &lt;- filter(mi_county_pop_income, County_Name %in% counties)\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nselected_counties %&gt;%\n  select(NAME, median_household_incomeE, income_moe_percent, income_e_reliability) %&gt;%\n  mutate(\n    median_household_incomeE = paste0(\"$\", format(median_household_incomeE, big.mark = \",\", scientific = FALSE)),\n    income_moe_percent = paste0(round(income_moe_percent, 2), \"%\"),\n  ) %&gt;%\nkable(col.names = c(\"County Name\", \"Median Income\", \"MOE Percent of Estimate\", \"Reliability of Estimate\"),\n      align = \"l\",\n      caption = \"3 Selected Counties in Michigan\")\n\n\n3 Selected Counties in Michigan\n\n\n\n\n\n\n\n\nCounty Name\nMedian Income\nMOE Percent of Estimate\nReliability of Estimate\n\n\n\n\nCrawford County, Michigan\n$57,998\n5.93%\nModerate Confidence\n\n\nKalamazoo County, Michigan\n$67,905\n2.31%\nHigh Confidence\n\n\nKeweenaw County, Michigan\n$55,560\n13.14%\nLow Confidence\n\n\n\n\n\nComment on the output:\nI chose:\n\nKalamazoo as a high reliable county and a relatively bigger city in Western Michigan with multiple universities.\nCrawford is a moderate reliable county and where I used to go fishing with my grandpa :)\nKeweenaw is a low reliable county in the most northern tip of the upper penninsula with a very small population (I didn’t know anyone even lived there)"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#tract-level-demographics",
    "href": "labs/lab1/scripts/assignment1_kk.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements:\n\nGeography: tract level\nVariables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001)\nUse the same state and year as before\nOutput format: wide\n\nChallenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\ndemographics &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(White = \"B03002_003\", Black = \"B03002_004\", Latinx = \"B03002_012\", Total = \"B03002_001\"),\n  state = \"MI\", \n  county = counties,\n  year = 2022,\n  output=\"wide\"\n)\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\ndemographics &lt;- demographics %&gt;%\n  mutate(percent_black = if_else(TotalE == 0, 0, BlackE / TotalE * 100),\n         percent_white = if_else(TotalE == 0, 0, WhiteE / TotalE * 100),\n         percent_latin = if_else(TotalE == 0, 0, LatinxE / TotalE * 100) \n  )%&gt;%\n  mutate(percent_black_c = paste0(round(percent_black, 2), \"%\"),\n         percent_white_c = paste0(round(percent_white), \"%\"),\n         percent_latin_c = paste0(round(percent_latin, 2), \"%\")\n  )\n\n# Add readable tract and county name columns using str_extract() or similar\ndemographics &lt;- demographics %&gt;%\n  mutate(\n    County_Name = str_extract(NAME, \"(?&lt;=;\\\\s)[A-Za-z ]+(?=\\\\sCounty;)\"),\n    Tract = str_extract(NAME, \"(?&lt;=Census Tract )[0-9.]+\")\n  )"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#demographic-analysis",
    "href": "labs/lab1/scripts/assignment1_kk.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\ndemographics %&gt;% \n  filter(!is.nan(percent_latin)) %&gt;%\n  arrange(desc(percent_latin)) %&gt;% \n  slice(1) %&gt;%\n  select(County_Name, Tract, percent_latin_c) %&gt;%\n  kable(col.names = c(\"County\", \"Tract\", \"Percent Latin\"),\n        align = \"c\")\n\n\n\n\nCounty\nTract\nPercent Latin\n\n\n\n\nKalamazoo\n10.01\n25.44%\n\n\n\n\n\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\navg_demo_by_tract &lt;- demographics %&gt;%\n  group_by(County_Name) %&gt;%\n  summarize(avg_black = mean(percent_black),\n            avg_white = mean(percent_white),\n            avg_latin = mean(percent_latin))\n\n# Create a nicely formatted table of your results using kable()\navg_demo_by_tract %&gt;%\n  mutate(avg_black_c = paste0(round(avg_black, 2), \"%\"),\n         avg_white_c = paste0(round(avg_white, 2), \"%\"),\n         avg_latin_c = paste0(round(avg_latin, 2), \"%\")) %&gt;%\n  select(County_Name, avg_black_c, avg_white_c, avg_latin_c) %&gt;% \n  kable(col.names = c(\"County\", \"Avg % Black\", \"Avg % White\", \"Avg % Latin\"),\n        caption = \"Average Demographic Makeup of All Census Tracts in 3 MI Counties\",\n        align = \"r\")\n\n\nAverage Demographic Makeup of All Census Tracts in 3 MI Counties\n\n\nCounty\nAvg % Black\nAvg % White\nAvg % Latin\n\n\n\n\nCrawford\n0.28%\n92.25%\n2.19%\n\n\nKalamazoo\n12.7%\n73.33%\n5.81%\n\n\nKeweenaw\n0%\n65.28%\n0.53%"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#moe-analysis-for-demographic-variables",
    "href": "labs/lab1/scripts/assignment1_kk.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\ndemographics &lt;- demographics %&gt;%\n  mutate(White_percent_MOE = WhiteM/WhiteE*100,\n         Black_percent_MOE = BlackM/BlackE*100, \n         Latin_percent_MOE = LatinxM/LatinxE*100)\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\ndemographics &lt;- demographics %&gt;% \n  mutate(\n    flag = case_when(\n      White_percent_MOE &gt;=100 |\n      Black_percent_MOE &gt;=100 | \n      Latin_percent_MOE &gt;=100 ~\n      \"MOE Higher than Estimate\",\n      (White_percent_MOE &gt;=50 & White_percent_MOE &lt; 100) |\n      (Black_percent_MOE &gt;=50 & Black_percent_MOE &lt; 100) | \n      (Latin_percent_MOE &gt;=50 & Latin_percent_MOE &lt; 100) ~\n      \"Very Unreliable\",\n      (White_percent_MOE &gt;10 & White_percent_MOE &lt; 50) |\n      (Black_percent_MOE &gt;10 & Black_percent_MOE &lt; 50) | \n      (Latin_percent_MOE &gt;10 & Latin_percent_MOE &lt; 50) ~\n      \"Unreliable\", \n      White_percent_MOE &lt;=10 |\n      Black_percent_MOE &lt;=10 | \n      Latin_percent_MOE &lt;=10 ~\n      \"Reliable\"\n    )\n  )\n\n# Create summary statistics showing how many tracts have data quality issues\nreliability_summary &lt;- demographics %&gt;%\n  group_by(flag) %&gt;% \n  summarize(count = n())\n\nkable(reliability_summary,\n      col.names = c(\"Flag\", \"Number of Tracts\"),\n      align=\"c\")\n\n\n\n\nFlag\nNumber of Tracts\n\n\n\n\nMOE Higher than Estimate\n37\n\n\nUnreliable\n3\n\n\nVery Unreliable\n36"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#pattern-analysis",
    "href": "labs/lab1/scripts/assignment1_kk.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\nmoe_issues &lt;- demographics %&gt;% \n  group_by(flag) %&gt;%\n  summarise(\n    avg_pop_size = mean(TotalE),\n    avg_percent_black = mean(if_else(TotalE != 0, BlackE / TotalE * 100, NA_real_), na.rm = TRUE),\n    avg_percent_white = mean(if_else(TotalE != 0, WhiteE / TotalE * 100, NA_real_), na.rm = TRUE),\n    avg_percent_latin = mean(if_else(TotalE != 0, LatinxE / TotalE * 100, NA_real_), na.rm = TRUE)\n  )\n  \n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\nmoe_issues &lt;- moe_issues %&gt;%\n  arrange(desc(avg_percent_black)) %&gt;%\n  mutate(avg_percent_black_c = paste0(round(avg_percent_black, 2), \"%\"),\n         avg_percent_white_c = paste0(round(avg_percent_white, 2), \"%\"),\n         avg_percent_latin_c = paste0(round(avg_percent_latin, 2), \"%\")) \n\n\nmoe_issues %&gt;%\n  select(flag, avg_pop_size, avg_percent_black_c, avg_percent_white_c, avg_percent_latin_c) %&gt;% \n  kable(col.names = c(\"Reliability (Bad to Worst)\", \"Avg Tract Population Size\", \"Avg % Black\", \"Avg % White\", \"Avg % Latin\"),\n        caption = \"Average Demographic Makeup of All Census Tracts in 3 MI Counties\",\n        align = \"r\",\n        digits = 0)\n\n\nAverage Demographic Makeup of All Census Tracts in 3 MI Counties\n\n\n\n\n\n\n\n\n\nReliability (Bad to Worst)\nAvg Tract Population Size\nAvg % Black\nAvg % White\nAvg % Latin\n\n\n\n\nUnreliable\n2762\n21.99%\n52.73%\n19.85%\n\n\nVery Unreliable\n4112\n14.88%\n70.22%\n6.23%\n\n\nMOE Higher than Estimate\n3254\n7.32%\n82.16%\n3.43%\n\n\n\n\n\n\nmoe_issues_2 &lt;- demographics %&gt;% \n  group_by(County_Name) %&gt;%\n  summarise(\n    total_pop_size = sum(TotalE),\n    avg_moe_percent_black = mean(if_else(BlackE != 0, BlackM / BlackE * 100, NA_real_), na.rm = TRUE),\n    avg_moe_percent_white = mean(if_else(WhiteE != 0, WhiteM / WhiteE * 100, NA_real_), na.rm = TRUE),\n    avg_moe_percent_latin = mean(if_else(LatinxE != 0, LatinxM / LatinxE * 100, NA_real_), na.rm = TRUE)\n  )\n  \n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\nmoe_issues_2 &lt;- moe_issues_2 %&gt;%\n  mutate(avg_moe_percent_black_c = paste0(round(avg_moe_percent_black, 2), \"%\"),\n         avg_moe_percent_white_c = paste0(round(avg_moe_percent_white, 2), \"%\"),\n         avg_moe_percent_latin_c = paste0(round(avg_moe_percent_latin, 2), \"%\")) \n\nmoe_issues_2 %&gt;%\n  arrange(desc(total_pop_size))%&gt;%\n  select(County_Name, total_pop_size, avg_moe_percent_black_c, avg_moe_percent_white_c, avg_moe_percent_latin_c) %&gt;% \n  kable(col.names = c(\"County\", \"Total Population Size\", \"Avg Black MOE % of Estimate\", \"Avg White MOE % of Estimate\", \"Avg Latin MOE % of Estimate\"),\n        caption = \"Average MOE % of Estimate of Different Racial Groups by County\",\n        align = \"r\",\n        digits = 0)\n\n\nAverage MOE % of Estimate of Different Racial Groups by County\n\n\n\n\n\n\n\n\n\nCounty\nTotal Population Size\nAvg Black MOE % of Estimate\nAvg White MOE % of Estimate\nAvg Latin MOE % of Estimate\n\n\n\n\nKalamazoo\n261426\n70.69%\n17.82%\n86.12%\n\n\nCrawford\n13197\n124.44%\n11.33%\n88.48%\n\n\nKeweenaw\n2088\nNaN%\n25.15%\n12.12%\n\n\n\n\n\nPattern Analysis:\nGiven that there was not a single tract in the three counties where all three racial population MOEs were below 10% of the estimate itself, I decided to make categories “Unreliable” where MOE was up to 50% of estimate, “Very unreliable” if 50- 100 % of estimate, and then if it was over 100, that indicates the MOE is bigger than the estimate itself, or “Inf” shows that the estimate was 0, which can’t be a denominator. We can see that on this scale, the more racially homogenous an area is on average, the less reliable the estimates are for that area. I also looked at the averagve MOEs as percent of Estimate for the three different racial groups by county, and accross all three counties, the White population had the lowest average MOE % of Estimate (except in Keweenaw, but this is based on only one tract having any estimate Latinx population at all)."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#analysis-integration-and-professional-summary",
    "href": "labs/lab1/scripts/assignment1_kk.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements:\n\nOverall Pattern Identification: What are the systematic patterns across all your analyses?\nEquity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings?\nRoot Cause Analysis: What underlying factors drive both data quality issues and bias risk?\nStrategic Recommendations: What should the Department implement to address these systematic issues?\n\nExecutive Summary:\nAccross the data I analyzed, the clearest pattern that emerged is that the more homogenous a tract, the more unreliable the estimates. We also see that this unreliability is not evenly shared, but whatever population is in the minority faces more unreliable estimates, leading to a sort of double marginalization, where an already marginalized population is less accurately represented in data, and decisions made on these data could be very misguided.\nIn Michigan, Black and Latinx populations face the greatest risk of algorithmic bias, even in a county that is relatively diverse, like Kalamazoo, any given tract might have a much higher White majority and unreliable estimates of Black and Latinx populations.\nThe underlying issues that could be at play here are that when attaining a survey sample especially in a census tract with a relatively low population and very low diversity, there might be a handful of people of the demographic the census is trying to estimate the size of, causing issues in extrapolating from the sample to the overall makeup of the tract and the county. In Michigan specifically, the state is about 3/4 White, and there is a history of racialized segregration of living areas that persists today, so even in diverse areas, any given tract might have high unreliability because of a more homogenous makeup.\nThe Department could address this by supplementing census data with further sampling to add to the dataset, however this risks also overburdening minority populations with survey fatigue. The Department should at least be very cautious and skeptical of algorithmic decision-making based on data at the census tract level, and should set a higher minimum population size where data reliability of all racial/ethnic groups is adequately low for any allocations or programs that might be based on these estimates."
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#specific-recommendations",
    "href": "labs/lab1/scripts/assignment1_kk.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\nincome_summary &lt;- mi_county_pop_income %&gt;% select(County_Name, median_household_incomeE, income_moe_percent, income_e_reliability)\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\nincome_summary &lt;- income_summary %&gt;%\n  mutate(safe = case_when(\n    income_e_reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n    income_e_reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n    income_e_reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n  ))\n\n# Format as a professional table with kable()\nincome_summary %&gt;%\n  select(County_Name, median_household_incomeE, safe) %&gt;%\n  arrange(median_household_incomeE) %&gt;%\n  kable(\n    col.names = c(\"County Name\", \"Median Household Income\", \"Algorithm Recommendation\"),\n    digit = 0,\n    format.args = list(big.mark = \",\"),\n    align = \"l\",\n    caption = \"Median Income of Michigan Counties by How Safe they are to use for Algorithmic Decision-making\"\n  )\n\n\nMedian Income of Michigan Counties by How Safe they are to use for Algorithmic Decision-making\n\n\n\n\n\n\n\nCounty Name\nMedian Household Income\nAlgorithm Recommendation\n\n\n\n\nLake\n45,946\nUse with caution - monitor outcomes\n\n\nIosco\n46,224\nUse with caution - monitor outcomes\n\n\nMontmorency\n46,345\nUse with caution - monitor outcomes\n\n\nClare\n47,816\nSafe for algorithmic decisions\n\n\nGogebic\n47,913\nUse with caution - monitor outcomes\n\n\nOntonagon\n48,316\nUse with caution - monitor outcomes\n\n\nOscoda\n48,692\nUse with caution - monitor outcomes\n\n\nAlpena\n49,133\nSafe for algorithmic decisions\n\n\nRoscommon\n49,898\nUse with caution - monitor outcomes\n\n\nAlcona\n50,295\nSafe for algorithmic decisions\n\n\nOgemaw\n50,377\nSafe for algorithmic decisions\n\n\nLuce\n51,015\nUse with caution - monitor outcomes\n\n\nBaraga\n51,911\nUse with caution - monitor outcomes\n\n\nIron\n52,241\nUse with caution - monitor outcomes\n\n\nIsabella\n52,638\nSafe for algorithmic decisions\n\n\nHoughton\n52,736\nSafe for algorithmic decisions\n\n\nArenac\n53,487\nSafe for algorithmic decisions\n\n\nGladwin\n53,717\nSafe for algorithmic decisions\n\n\nDelta\n53,852\nUse with caution - monitor outcomes\n\n\nMenominee\n54,074\nUse with caution - monitor outcomes\n\n\nMecosta\n54,132\nUse with caution - monitor outcomes\n\n\nHuron\n54,475\nSafe for algorithmic decisions\n\n\nOsceola\n54,875\nSafe for algorithmic decisions\n\n\nSchoolcraft\n55,071\nRequires manual review or additional data\n\n\nAlger\n55,528\nUse with caution - monitor outcomes\n\n\nKeweenaw\n55,560\nRequires manual review or additional data\n\n\nSanilac\n55,740\nSafe for algorithmic decisions\n\n\nPresque Isle\n55,986\nUse with caution - monitor outcomes\n\n\nKalkaska\n56,380\nUse with caution - monitor outcomes\n\n\nSaginaw\n56,579\nSafe for algorithmic decisions\n\n\nWayne\n57,223\nSafe for algorithmic decisions\n\n\nMissaukee\n57,667\nUse with caution - monitor outcomes\n\n\nBay\n57,887\nSafe for algorithmic decisions\n\n\nGratiot\n57,934\nSafe for algorithmic decisions\n\n\nCrawford\n57,998\nUse with caution - monitor outcomes\n\n\nCalhoun\n58,191\nSafe for algorithmic decisions\n\n\nChippewa\n58,408\nSafe for algorithmic decisions\n\n\nGenesee\n58,594\nSafe for algorithmic decisions\n\n\nWexford\n58,652\nUse with caution - monitor outcomes\n\n\nNewaygo\n59,065\nSafe for algorithmic decisions\n\n\nHillsdale\n59,425\nSafe for algorithmic decisions\n\n\nManistee\n59,467\nSafe for algorithmic decisions\n\n\nCheboygan\n59,557\nSafe for algorithmic decisions\n\n\nDickinson\n59,651\nSafe for algorithmic decisions\n\n\nTuscola\n59,815\nSafe for algorithmic decisions\n\n\nBerrien\n60,379\nSafe for algorithmic decisions\n\n\nBranch\n60,600\nSafe for algorithmic decisions\n\n\nMackinac\n60,620\nUse with caution - monitor outcomes\n\n\nOceana\n60,691\nSafe for algorithmic decisions\n\n\nMason\n60,744\nSafe for algorithmic decisions\n\n\nMontcalm\n61,250\nSafe for algorithmic decisions\n\n\nMuskegon\n61,347\nSafe for algorithmic decisions\n\n\nSt. Joseph\n62,281\nSafe for algorithmic decisions\n\n\nShiawassee\n62,498\nSafe for algorithmic decisions\n\n\nIngham\n62,548\nSafe for algorithmic decisions\n\n\nJackson\n62,581\nSafe for algorithmic decisions\n\n\nOtsego\n62,865\nUse with caution - monitor outcomes\n\n\nMarquette\n63,115\nSafe for algorithmic decisions\n\n\nCass\n65,183\nSafe for algorithmic decisions\n\n\nLenawee\n65,484\nSafe for algorithmic decisions\n\n\nVan Buren\n65,531\nUse with caution - monitor outcomes\n\n\nSt. Clair\n66,887\nSafe for algorithmic decisions\n\n\nKalamazoo\n67,905\nSafe for algorithmic decisions\n\n\nAntrim\n68,850\nSafe for algorithmic decisions\n\n\nEmmet\n69,690\nUse with caution - monitor outcomes\n\n\nCharlevoix\n69,764\nSafe for algorithmic decisions\n\n\nBenzie\n71,327\nUse with caution - monitor outcomes\n\n\nIonia\n71,720\nSafe for algorithmic decisions\n\n\nMonroe\n72,573\nSafe for algorithmic decisions\n\n\nMidland\n73,643\nSafe for algorithmic decisions\n\n\nMacomb\n73,876\nSafe for algorithmic decisions\n\n\nBarry\n75,182\nSafe for algorithmic decisions\n\n\nLapeer\n75,402\nSafe for algorithmic decisions\n\n\nAllegan\n75,543\nSafe for algorithmic decisions\n\n\nGrand Traverse\n75,553\nSafe for algorithmic decisions\n\n\nKent\n76,247\nSafe for algorithmic decisions\n\n\nEaton\n77,158\nSafe for algorithmic decisions\n\n\nLeelanau\n82,345\nUse with caution - monitor outcomes\n\n\nClinton\n82,594\nSafe for algorithmic decisions\n\n\nOttawa\n83,932\nSafe for algorithmic decisions\n\n\nWashtenaw\n84,245\nSafe for algorithmic decisions\n\n\nOakland\n92,620\nSafe for algorithmic decisions\n\n\nLivingston\n96,135\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation:\n\nThe following counties have reliable median income estimates, because the margin of error of the estimate is less than 5% of the estimate itself:\n1 Alcona\n2 Allegan\n3 Alpena\n4 Antrim\n5 Arenac\n6 Barry\n7 Bay\n8 Berrien\n9 Branch\n10 Calhoun\n11 Cass\n12 Charlevoix\n13 Cheboygan\n14 Chippewa\n15 Clare\n16 Clinton\n17 Dickinson\n18 Eaton\n19 Genesee\n20 Gladwin\n21 Grand Traverse 22 Gratiot\n23 Hillsdale\n24 Houghton\n25 Huron\n26 Ingham\n27 Ionia\n28 Isabella\n29 Jackson\n30 Kalamazoo\n31 Kent\n32 Lapeer\n33 Lenawee\n34 Livingston\n35 Macomb\n36 Manistee\n37 Marquette\n38 Mason\n39 Midland\n40 Monroe\n41 Montcalm\n42 Muskegon\n43 Newaygo\n44 Oakland\n45 Oceana\n46 Ogemaw\n47 Osceola\n48 Ottawa\n49 Saginaw\n50 St. Clair\n51 St. Joseph\n52 Sanilac\n53 Shiawassee\n54 Tuscola\n55 Washtenaw\n56 Wayne\n\nCounties requiring additional oversight:\n\nThe following counties’ median income estimates should be used with caution, given that the margin of error is 5 - 10% of the estimate. Any algorithmic decisions based on these estimates should account for the possibility that the median income is up to 10% higher or lower.\n1 Alger\n2 Baraga\n3 Benzie\n4 Crawford\n5 Delta\n6 Emmet\n7 Gogebic\n8 Iosco\n9 Iron\n10 Kalkaska\n11 Lake\n12 Leelanau\n13 Luce\n14 Mackinac\n15 Mecosta\n16 Menominee\n17 Missaukee\n18 Montmorency 19 Ontonagon\n20 Oscoda\n21 Otsego\n22 Presque Isle 23 Roscommon\n24 Van Buren\n25 Wexford\n\nCounties needing alternative approaches:\n\nThe following counties’ median income estimates are low confidence. No algorithmic decisions should be based on them without manual review or additional surveying to insure that the populations are adequately represented.\n1 Keweenaw\n2 Schoolcraft"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#questions-for-further-investigation",
    "href": "labs/lab1/scripts/assignment1_kk.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\nHow come even majority Black census tracts in Kalamazoo have a low confidence estimate compared to a majority White census tract?\nHow racially segregated are census tracts in Michigan in general – i.e. how many tracts are at least 3/4 one race? How does this compare to other states? How does it compare to the past?"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#submission-checklist",
    "href": "labs/lab1/scripts/assignment1_kk.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes",
    "section": "",
    "text": "Anscombe’s Quartet\n\nJust looking at summary can hide important details about trends in the data – all 4 graphs have the same variances, same correlation, and same regression line … wouldn’t realize how different they are unless looking at the graph\nPolicy Implications - policy decisions made based on misunderstood data\n\n73% of planners don’t warn user about data unreliability – violating ethics arguably\n\nCommon problems in data presentation:\n\nmisleading scales, cherry-picking, missing context\nhiding uncertainty (not communicating MOEs) – verrrryyyyyyy tempting to use census block groups with ACS5 because you can … but MOEs can be quite high at this level\n\n\n\n\n\n\n\nData -&gt; Aesthetics -&gt; Geometries -&gt; Visual\n\nData = dataset, df or tibble\nAesthetics = what variables map to visual properties\nGeometries = how to display the data (as points? bars? lines?) and how to decorate/stylize\nadditional layers = scales, themes, facets, annotations\n\nSyntax: -g &lt;- ggplot(data = acs_data) = “I would like to make this plot” … if after running this code you ran g, you would just get a black canvas in viewer\n\nAdding aesthetics: g &lt;- ggplot(data = acs_data) + aes(x=income, y=%_bachelor)\nAdding geometries: g &lt;- ggplot(...) + aes(...) + geom_points(decorate...colors...size...)\n\n\n\n\n\n\nEDA = Exploratory Data Analysis … take the time to get to know your data\n\nLoad and inspect – dimensions, variables types, missing data\nAssess reliability – examine MOEs and calculate coeffs of variation\nvisualize distributions …\n\nFirst steps:\n\nHistogram – distribution of data\nBox plot – finding outliers\n\n\n\n\n\n\nLeft join – preserves ‘left’ tables all ids from identifying category, add data from ‘right’ table based on identifying category but will skip mismatched ids\n\nright join is silly…just use left join and switch order\n\nFull join – combined all ids from each tables’ id category\nInner Join – only retain matching ids from each table\nCan join if columns have different names as long as they have matching values, but matching fields must have same data type"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "title": "Week 3 Notes - Course Introduction",
    "section": "",
    "text": "Main concepts:\nTechnical skills covered:"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#notes",
    "href": "weekly-notes/week-03-notes.html#notes",
    "title": "Week 3 Notes",
    "section": "",
    "text": "Anscombe’s Quartet\n\nJust looking at summary can hide important details about trends in the data – all 4 graphs have the same variances, same correlation, and same regression line … wouldn’t realize how different they are unless looking at the graph\nPolicy Implications - policy decisions made based on misunderstood data\n\n73% of planners don’t warn user about data unreliability – violating ethics arguably\n\nCommon problems in data presentation:\n\nmisleading scales, cherry-picking, missing context\nhiding uncertainty (not communicating MOEs) – verrrryyyyyyy tempting to use census block groups with ACS5 because you can … but MOEs can be quite high at this level\n\n\n\n\n\n\n\nData -&gt; Aesthetics -&gt; Geometries -&gt; Visual\n\nData = dataset, df or tibble\nAesthetics = what variables map to visual properties\nGeometries = how to display the data (as points? bars? lines?) and how to decorate/stylize\nadditional layers = scales, themes, facets, annotations\n\nSyntax: -g &lt;- ggplot(data = acs_data) = “I would like to make this plot” … if after running this code you ran g, you would just get a black canvas in viewer\n\nAdding aesthetics: g &lt;- ggplot(data = acs_data) + aes(x=income, y=%_bachelor)\nAdding geometries: g &lt;- ggplot(...) + aes(...) + geom_points(decorate...colors...size...)\n\n\n\n\n\n\nEDA = Exploratory Data Analysis … take the time to get to know your data\n\nLoad and inspect – dimensions, variables types, missing data\nAssess reliability – examine MOEs and calculate coeffs of variation\nvisualize distributions …\n\nFirst steps:\n\nHistogram – distribution of data\nBox plot – finding outliers\n\n\n\n\n\n\nLeft join – preserves ‘left’ tables all ids from identifying category, add data from ‘right’ table based on identifying category but will skip mismatched ids\n\nright join is silly…just use left join and switch order\n\nFull join – combined all ids from each tables’ id category\nInner Join – only retain matching ids from each table\nCan join if columns have different names as long as they have matching values, but matching fields must have same data type"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nGood pattern for finding census variables :\n\nfilter(str_detect(field, \"str\"))\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\nin the load variables table, “concept” column corresponds to tables"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection"
  },
  {
    "objectID": "labs/lab1/scripts/assignment1_kk.html#given-that-i-chose-three-relatively-low-population-counties-in-a-state-with-a-vary-large-white-majority-the-conclusions-made-about-the-link-between-racial-homogeneity-and-reliability-of-minority-estimates-is-a-limited-conclusion.",
    "href": "labs/lab1/scripts/assignment1_kk.html#given-that-i-chose-three-relatively-low-population-counties-in-a-state-with-a-vary-large-white-majority-the-conclusions-made-about-the-link-between-racial-homogeneity-and-reliability-of-minority-estimates-is-a-limited-conclusion.",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Given that I chose three relatively low population counties in a state with a vary large White majority, the conclusions made about the link between racial homogeneity and reliability of minority estimates is a limited conclusion.",
    "text": "Given that I chose three relatively low population counties in a state with a vary large White majority, the conclusions made about the link between racial homogeneity and reliability of minority estimates is a limited conclusion."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 Notes",
    "section": "",
    "text": "WHERE are these patterns occurring matters\n\nGeographic clustering of problems\nSpatial relationships between communities\nAccess to services and resources is often based on geography\n\n\n\n\n\n\nTwo families of representing world 2-dimensionally\n\nVector\n\ndiscrete objects – things that have definite boundaries\n\nRaster\n\nPixels\nContinuous data\n\n\n\nVector Data Representation - Three basic types of geometric representations - points - lines - polygons - (kinda like illustrator)\nCommon Spatial Data File Formats - Shapefile - Developed by ESRI - Three fundamental objects (need all three to render accurately): - .shp - stores info about the geometry - .shx - shape index - .dbf - names of things - Integrates with tidyverse, follows inernational standards - GeoJSON - All one file ! Little more - KML - Google Earth - Database connections (PostGIS)\nSimple Features - Multi-shapes – like a broken bridge – think multiple shapes for one row of a table - Think Hawaii - -multiple shapes that are all “Hawaii”\nTidyCensus gives you characteristics but no shapes, tigris gives you shapes but no characteristics – must combine\nImportant syntax:\n\n==ggplot geom for mapping: geom_sf()==\n==ggplot use theme_void() to get rid of graph backdrop==\n==st_filter() is like filter() but for spatial filtering rather than simple df`\nst_union() works like arcgis “dissolve”\n\nSpatial Subsetting: - preserves original shapes, not for doing something like finding part of a county. Rather for doing things like finding a county within a list of counties, or all counties that touch a specific county\nneighbors &lt;- pa_counties %&gt;%   st_filter(allegheny, .predicate = st_touches)\n\nIn this example, the terminology “st_touches” is common across all GIS\n.predicate tells st_filter() what kind of relationship to look for, if nothing specified, then st_intersect is default\n\ncheat sheet:\n\n\n\ncheat-sheet\n\n\n\n\n\n\nbasic problem : world is round, maps are flat, projecting from 3d to 2d will necessarily cause some kind of distortion\nEarth is also not perfectly round, but a geoid ( a lil bumpy ), but geoid is mathematically inconvenient, so the Step 1 in projection is we project from he geoid to an ==ellipsoid==, which doesn’t perfectly represent the geoid, but good enough, and smooth\n\nThis means there are multiple ellipsoids that fit the earth, and might fit certain areas better than others\n\n\n\n\n\ncrs\n\n\n\nStep 2 – tie the ellipsoid to the real earth to create a Geographic (Geodetic) Coordinate System i.e. Lat Longs\n\nClark, 1866, uses ‘flattening’ to make a nice lil ellipsoid that is particularly well-fitted to North America – Meades Ranch, Kansas, is where the ellipsoid and geoid smooch :-*\n==North American Datum 1927== or ==NAD27==\nProgress made, made a better ellipsoid than Clarke (“We’re not in Kansas anymore”), based on Earth Center instead of NA.. this one called ==GRS80==\n==WGS84== – yet another used by GPS systems\n\nStep 3 – take the 3d ellipsoid points and project onto 2d surface\n\nCylindrical Projections\n\n\n\n\n\ncylindical-projections\n\n\n    -   Line of tangency -- where the ellipsoid and the projected\n        surface smooch, all other locations will be distorted\n    -   Mercator is prime example\n    -   Bad at preserving sizes of things on 2d surface, but good at\n        preserving angles\n-   Transverse Cylindrical Projection\n    -   Good at preserving up and down (good for Chile, for example)\n    \n\n\n\ntransverse\n\n\n-   Conical Projection\n    -   Good for areas concentrated in a segment of the ellipsoid,\n        like North America\n        \n\n\n\nconical\n\n\n-   Projected Coordinate System\n    -   Localized coordinate system built on a regular,\n        non-distorted grid\n    -   ==UTM== --\n\n\n\nUTM\n\n\n        -   Need to know what zone you are in -- all numbers are\n            based on grid system originating in the lower left\n            corner of the zone\n        -   No negatives\n        -   Who uses UTM ? depends on state -- if looking at a long\n            skinny area like Idaho for example\n    -   ==State Plane== (SPC)\n        -   Used in PA - Two state plane grids in PA, North and\n            South\n\n\n\nstate-plane\n\n\n        -   Some states use Feet, others use Meters -- make sure to\n            check !\n-   SADD -- can't have all 4 in a projection\n    -   Shape\n    -   Area\n    -   Distance\n    -   Direction\n\nst_crs() to check CRS for a dataset\nst_set_crs(data, ####) – set CRS – ONLY IF CRS MISSING, do NOT use to transform CRS (path of despair) – this will not change the numbers themselves but rather how the computer interprets the numbers, only use if st_crs() returns &lt;unknown&gt;\nst_transform() – this will actually recalculate the coordinates from one CRS to another\nIn arcGIS – this is the like the difference between “define projection” and “project”"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#notes",
    "href": "weekly-notes/week-04-notes.html#notes",
    "title": "Week 4 Notes",
    "section": "",
    "text": "WHERE are these patterns occurring matters\n\nGeographic clustering of problems\nSpatial relationships between communities\nAccess to services and resources is often based on geography\n\n\n\n\n\n\nTwo families of representing world 2-dimensionally\n\nVector\n\ndiscrete objects – things that have definite boundaries\n\nRaster\n\nPixels\nContinuous data\n\n\n\nVector Data Representation - Three basic types of geometric representations - points - lines - polygons - (kinda like illustrator)\nCommon Spatial Data File Formats - Shapefile - Developed by ESRI - Three fundamental objects (need all three to render accurately): - .shp - stores info about the geometry - .shx - shape index - .dbf - names of things - Integrates with tidyverse, follows inernational standards - GeoJSON - All one file ! Little more - KML - Google Earth - Database connections (PostGIS)\nSimple Features - Multi-shapes – like a broken bridge – think multiple shapes for one row of a table - Think Hawaii - -multiple shapes that are all “Hawaii”\nTidyCensus gives you characteristics but no shapes, tigris gives you shapes but no characteristics – must combine\nImportant syntax:\n\n==ggplot geom for mapping: geom_sf()==\n==ggplot use theme_void() to get rid of graph backdrop==\n==st_filter() is like filter() but for spatial filtering rather than simple df`\nst_union() works like arcgis “dissolve”\n\nSpatial Subsetting: - preserves original shapes, not for doing something like finding part of a county. Rather for doing things like finding a county within a list of counties, or all counties that touch a specific county\nneighbors &lt;- pa_counties %&gt;%   st_filter(allegheny, .predicate = st_touches)\n\nIn this example, the terminology “st_touches” is common across all GIS\n.predicate tells st_filter() what kind of relationship to look for, if nothing specified, then st_intersect is default\n\ncheat sheet:\n\n\n\ncheat-sheet\n\n\n\n\n\n\nbasic problem : world is round, maps are flat, projecting from 3d to 2d will necessarily cause some kind of distortion\nEarth is also not perfectly round, but a geoid ( a lil bumpy ), but geoid is mathematically inconvenient, so the Step 1 in projection is we project from he geoid to an ==ellipsoid==, which doesn’t perfectly represent the geoid, but good enough, and smooth\n\nThis means there are multiple ellipsoids that fit the earth, and might fit certain areas better than others\n\n\n\n\n\ncrs\n\n\n\nStep 2 – tie the ellipsoid to the real earth to create a Geographic (Geodetic) Coordinate System i.e. Lat Longs\n\nClark, 1866, uses ‘flattening’ to make a nice lil ellipsoid that is particularly well-fitted to North America – Meades Ranch, Kansas, is where the ellipsoid and geoid smooch :-*\n==North American Datum 1927== or ==NAD27==\nProgress made, made a better ellipsoid than Clarke (“We’re not in Kansas anymore”), based on Earth Center instead of NA.. this one called ==GRS80==\n==WGS84== – yet another used by GPS systems\n\nStep 3 – take the 3d ellipsoid points and project onto 2d surface\n\nCylindrical Projections\n\n\n\n\n\ncylindical-projections\n\n\n    -   Line of tangency -- where the ellipsoid and the projected\n        surface smooch, all other locations will be distorted\n    -   Mercator is prime example\n    -   Bad at preserving sizes of things on 2d surface, but good at\n        preserving angles\n-   Transverse Cylindrical Projection\n    -   Good at preserving up and down (good for Chile, for example)\n    \n\n\n\ntransverse\n\n\n-   Conical Projection\n    -   Good for areas concentrated in a segment of the ellipsoid,\n        like North America\n        \n\n\n\nconical\n\n\n-   Projected Coordinate System\n    -   Localized coordinate system built on a regular,\n        non-distorted grid\n    -   ==UTM== --\n\n\n\nUTM\n\n\n        -   Need to know what zone you are in -- all numbers are\n            based on grid system originating in the lower left\n            corner of the zone\n        -   No negatives\n        -   Who uses UTM ? depends on state -- if looking at a long\n            skinny area like Idaho for example\n    -   ==State Plane== (SPC)\n        -   Used in PA - Two state plane grids in PA, North and\n            South\n\n\n\nstate-plane\n\n\n        -   Some states use Feet, others use Meters -- make sure to\n            check !\n-   SADD -- can't have all 4 in a projection\n    -   Shape\n    -   Area\n    -   Distance\n    -   Direction\n\nst_crs() to check CRS for a dataset\nst_set_crs(data, ####) – set CRS – ONLY IF CRS MISSING, do NOT use to transform CRS (path of despair) – this will not change the numbers themselves but rather how the computer interprets the numbers, only use if st_crs() returns &lt;unknown&gt;\nst_transform() – this will actually recalculate the coordinates from one CRS to another\nIn arcGIS – this is the like the difference between “define projection” and “project”"
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html",
    "href": "labs/lab2/assignment2_kk.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives:\n\nApply spatial operations to answer policy-relevant research questions\nIntegrate census demographic data with spatial analysis\nCreate publication-quality visualizations and maps\nWork with spatial data from multiple sources\nCommunicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html#assignment-overview",
    "href": "labs/lab2/assignment2_kk.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives:\n\nApply spatial operations to answer policy-relevant research questions\nIntegrate census demographic data with spatial analysis\nCreate publication-quality visualizations and maps\nWork with spatial data from multiple sources\nCommunicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "labs/lab2/assignment2_kk.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data:\n\nPennsylvania county boundaries\nPennsylvania hospitals (from lecture data)\nPennsylvania census tracts\n\nYour Task:\n\n# Load spatial data\npa_counties &lt;- st_read(\"data/Pennsylvania_County_Boundaries.shp\")\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `C:\\Users\\knoxk\\OneDrive\\Documents\\musa-5080\\portfolio-setup-kkxix\\labs\\lab2\\data\\Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nhospitals &lt;- st_read(\"data/hospitals.geojson\")\n\nReading layer `hospitals' from data source \n  `C:\\Users\\knoxk\\OneDrive\\Documents\\musa-5080\\portfolio-setup-kkxix\\labs\\lab2\\data\\hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\ncensus_tracts &lt;- st_transform(census_tracts, st_crs(pa_counties))\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\n\n# Check that all data loaded correctly\nglimpse(pa_counties)\n\nRows: 67\nColumns: 20\n$ OBJECTID   &lt;int&gt; 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347,…\n$ MSLINK     &lt;int&gt; 46, 8, 9, 58, 59, 60, 62, 63, 42, 43, 44, 47, 48, 49, 50, 5…\n$ COUNTY_NAM &lt;chr&gt; \"MONTGOMERY\", \"BRADFORD\", \"BUCKS\", \"TIOGA\", \"UNION\", \"VENAN…\n$ COUNTY_NUM &lt;chr&gt; \"46\", \"08\", \"09\", \"58\", \"59\", \"60\", \"62\", \"63\", \"42\", \"43\",…\n$ FIPS_COUNT &lt;chr&gt; \"091\", \"015\", \"017\", \"117\", \"119\", \"121\", \"125\", \"127\", \"08…\n$ COUNTY_ARE &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ COUNTY_PER &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ NUMERIC_LA &lt;int&gt; 5, 2, 5, 2, 2, 3, 1, 2, 1, 3, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1,…\n$ COUNTY_N_1 &lt;int&gt; 46, 8, 9, 58, 59, 60, 62, 63, 42, 43, 44, 47, 48, 49, 50, 5…\n$ AREA_SQ_MI &lt;dbl&gt; 487.4271, 1161.3379, 622.0836, 1137.2480, 319.1893, 683.367…\n$ SOUND      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ SPREAD_SHE &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ IMAGE_NAME &lt;chr&gt; \"poll.bmp\", \"poll.bmp\", \"poll.bmp\", \"poll.bmp\", \"poll.bmp\",…\n$ NOTE_FILE  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ VIDEO      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ DISTRICT_N &lt;chr&gt; \"06\", \"03\", \"06\", \"03\", \"03\", \"01\", \"12\", \"04\", \"02\", \"01\",…\n$ PA_CTY_COD &lt;chr&gt; \"46\", \"08\", \"09\", \"59\", \"60\", \"61\", \"63\", \"64\", \"42\", \"43\",…\n$ MAINT_CTY_ &lt;chr&gt; \"4\", \"9\", \"1\", \"7\", \"8\", \"5\", \"4\", \"6\", \"5\", \"4\", \"7\", \"3\",…\n$ DISTRICT_O &lt;chr&gt; \"6-4\", \"3-9\", \"6-1\", \"3-7\", \"3-8\", \"1-5\", \"12-4\", \"4-6\", \"2…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((-8398884 48..., MULTIPOLYGON (…\n\nglimpse(census_tracts)\n\nRows: 3,445\nColumns: 14\n$ STATEFP    &lt;chr&gt; \"42\", \"42\", \"42\", \"42\", \"42\", \"42\", \"42\", \"42\", \"42\", \"42\",…\n$ COUNTYFP   &lt;chr&gt; \"001\", \"013\", \"013\", \"013\", \"013\", \"011\", \"011\", \"011\", \"01…\n$ TRACTCE    &lt;chr&gt; \"031101\", \"100400\", \"100500\", \"100800\", \"101900\", \"011200\",…\n$ GEOIDFQ    &lt;chr&gt; \"1400000US42001031101\", \"1400000US42013100400\", \"1400000US4…\n$ GEOID      &lt;chr&gt; \"42001031101\", \"42013100400\", \"42013100500\", \"42013100800\",…\n$ NAME       &lt;chr&gt; \"311.01\", \"1004\", \"1005\", \"1008\", \"1019\", \"112\", \"2\", \"115\"…\n$ NAMELSAD   &lt;chr&gt; \"Census Tract 311.01\", \"Census Tract 1004\", \"Census Tract 1…\n$ STUSPS     &lt;chr&gt; \"PA\", \"PA\", \"PA\", \"PA\", \"PA\", \"PA\", \"PA\", \"PA\", \"PA\", \"PA\",…\n$ NAMELSADCO &lt;chr&gt; \"Adams County\", \"Blair County\", \"Blair County\", \"Blair Coun…\n$ STATE_NAME &lt;chr&gt; \"Pennsylvania\", \"Pennsylvania\", \"Pennsylvania\", \"Pennsylvan…\n$ LSAD       &lt;chr&gt; \"CT\", \"CT\", \"CT\", \"CT\", \"CT\", \"CT\", \"CT\", \"CT\", \"CT\", \"CT\",…\n$ ALAND      &lt;dbl&gt; 3043185, 993724, 1130204, 996553, 573726, 1539365, 1949529,…\n$ AWATER     &lt;dbl&gt; 0, 0, 0, 0, 0, 9308, 159015, 12469, 0, 0, 0, 1271, 6352, 74…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((-8575060 48..., MULTIPOLYGON (…\n\nglimpse(hospitals)\n\nRows: 223\nColumns: 12\n$ CHIEF_EXEC &lt;chr&gt; \"Peter J Adamo\", \"Autumn DeShields\", \"Shawn Parekh\", \"DIANE…\n$ CHIEF_EX_1 &lt;chr&gt; \"President\", \"Chief Executive Officer\", \"Chief Executive Of…\n$ FACILITY_U &lt;chr&gt; \"https://www.phhealthcare.org\", \"https://www.malvernbh.com\"…\n$ LONGITUDE  &lt;dbl&gt; -79.91131, -75.17005, -75.20963, -80.27907, -79.02513, -75.…\n$ COUNTY     &lt;chr&gt; \"Washington\", \"Philadelphia\", \"Philadelphia\", \"Washington\",…\n$ FACILITY_N &lt;chr&gt; \"Penn Highlands Mon Valley\", \"MALVERN BEHAVIORAL HEALTH\", \"…\n$ STREET     &lt;chr&gt; \"1163 Country Club Road\", \"1930 South Broad Street Unit 4\",…\n$ CITY_OR_BO &lt;chr&gt; \"Monongahela\", \"Philadelphia\", \"Philadelphia\", \"WASHINGTON\"…\n$ LATITUDE   &lt;dbl&gt; 40.18193, 39.92619, 40.02869, 40.15655, 39.80913, 40.24273,…\n$ TELEPHONE_ &lt;chr&gt; \"724-258-1000\", \"610-480-8919\", \"215-483-9900\", \"7248840710…\n$ ZIP_CODE   &lt;chr&gt; \"15063\", \"19145\", \"19128\", \"15301\", \"15552\", \"19464\", \"1776…\n$ geometry   &lt;POINT [m]&gt; POINT (-8895686 4892415), POINT (-8367892 4855223), P…\n\n# With ggplot2\np1 &lt;- ggplot(pa_counties) +\n  geom_sf() +\n  theme_void()\n\np2 &lt;- ggplot(census_tracts) +\n  geom_sf() +\n  theme_void()\n\np3 &lt;- ggplot(hospitals) +\n  geom_sf() +\n  theme_void()\n\np1 | p2 | p3\n\n\n\n\n\n\n\n\nQuestions to answer:\n\nHow many hospitals are in your dataset?\n\n223\n\nHow many census tracts?\n\n3445\n\nWhat coordinate reference system is each dataset in?\n\nI transformed hospitals and census tracts to PA counties CRS, which is WGS84. Before that, hospitals were also in WGS84, but census tracts were in NAD83.\n\n\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables:\n\nTotal population\nMedian household income\nPopulation 65 years and over (you may need to sum multiple age categories)\n\nYour Task:\n\n# Get demographic data from ACS\npa_tracts_data &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    median_income = \"B19013_001\",\n    total_pop = \"B01003_001\",\n    over_65 = \"B01001_020\"  # Population 65 years and over\n  ),\n  state = \"PA\",\n  year = 2023,\n  output = \"wide\"\n)\n\n# Join to tract boundaries\ncensus_tracts &lt;- census_tracts %&gt;%\n  left_join(pa_tracts_data, by= \"GEOID\")\n\nsummary(census_tracts)\n\n   STATEFP            COUNTYFP           TRACTCE            GEOIDFQ         \n Length:3445        Length:3445        Length:3445        Length:3445       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    GEOID              NAME.x            NAMELSAD            STUSPS         \n Length:3445        Length:3445        Length:3445        Length:3445       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  NAMELSADCO         STATE_NAME            LSAD               ALAND          \n Length:3445        Length:3445        Length:3445        Min.   :7.224e+04  \n Class :character   Class :character   Class :character   1st Qu.:1.320e+06  \n Mode  :character   Mode  :character   Mode  :character   Median :4.183e+06  \n                                                          Mean   :3.364e+07  \n                                                          3rd Qu.:2.858e+07  \n                                                          Max.   :1.024e+09  \n                                                                             \n     AWATER            NAME.y          median_incomeE   median_incomeM  \n Min.   :       0   Length:3445        Min.   : 13307   Min.   :   472  \n 1st Qu.:       0   Class :character   1st Qu.: 57864   1st Qu.:  9324  \n Median :   13905   Mode  :character   Median : 72944   Median : 13780  \n Mean   :  433440                      Mean   : 80731   Mean   : 16219  \n 3rd Qu.:  240806                      3rd Qu.: 96691   3rd Qu.: 20141  \n Max.   :29792870                      Max.   :250001   Max.   :208341  \n                                       NA's   :65       NA's   :73      \n   total_popE      total_popM        over_65E         over_65M     \n Min.   :    0   Min.   :   3.0   Min.   :  0.00   Min.   :  2.00  \n 1st Qu.: 2616   1st Qu.: 248.0   1st Qu.: 19.00   1st Qu.: 19.00  \n Median : 3640   Median : 375.0   Median : 39.00   Median : 31.00  \n Mean   : 3770   Mean   : 402.7   Mean   : 47.59   Mean   : 39.76  \n 3rd Qu.: 4810   3rd Qu.: 527.0   3rd Qu.: 66.00   3rd Qu.: 51.00  \n Max.   :10406   Max.   :2091.0   Max.   :324.00   Max.   :342.00  \n                                                                   \n          geometry   \n MULTIPOLYGON :3445  \n epsg:3857    :   0  \n +proj=merc...:   0  \n                     \n                     \n                     \n                     \n\n\nQuestions to answer:\n\nWhat year of ACS data are you using?\n\n2019-2023 5-year ACS data\n\nHow many tracts have missing income data?\n\n65\n\nWhat is the median income across all PA census tracts?\n\n$72,944\n\n\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria:\n\nLow median household income (choose an appropriate threshold)\nSignificant elderly population (choose an appropriate threshold)\n\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\nvulnerable_tracts &lt;- census_tracts %&gt;%\n  filter(median_incomeE &lt; 42398 | (over_65E/total_popE) &gt; .017)\n\nQuestions to answer:\n\nWhat income threshold did you choose and why?\n\nI looked at the 2023 poverty guidelines, the average household size in PA in 2023, which was 2.4, and then looked into eligibility for various programs, which is anywhere from 133% FPIG to 215%. I am going with vulnerable household is 215% or less of the 2-person household FPIG, or $42,398.\n\nWhat elderly population threshold did you choose and why?\n\nIn order to control for overall population size, instead of looking at the raw number of elderly, I looked at the over 65 as percent of total population. I defined vulnerable as any tract in the 75th percentile of elderly percent of tract population (1.7% or above).\n\nHow many tracts meet your vulnerability criteria?\n\n1,145\n\nWhat percentage of PA census tracts are considered vulnerable by your definition?\n\n33.2% – about a third.\n\n\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n#convert to Albers\nvulnerable_tracts &lt;- st_transform(vulnerable_tracts, crs = 5070)\nhospitals &lt;- st_transform(hospitals, crs = 5070)\n\n# Calculate distance from each tract centroid to nearest hospital\ntract_centroids &lt;- st_centroid(vulnerable_tracts)\nnearest_hospital &lt;- st_nearest_feature(tract_centroids, hospitals)\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(nearest_hospital_geom = hospitals$geometry[nearest_hospital])\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(distance_to_nearst_hospital = set_units(st_distance(tract_centroids, vulnerable_tracts$nearest_hospital_geom, by_element = TRUE), \"mi\"))\n\nsummary(vulnerable_tracts$distance_to_nearst_hospital) \n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 0.02107  1.21850  2.75761  4.37494  6.02931 29.75156 \n\n\nRequirements:\n\nUse an appropriate projected coordinate system for Pennsylvania\nCalculate distances in miles\nExplain why you chose your projection\n\nI chose Albers Conical Equal Area, NAD 83 projection based on this PA document from 2011’s recommendation.\nQuestions to answer:\n\nWhat is the average distance to the nearest hospital for vulnerable tracts?\n\n4.4 miles\n\nWhat is the maximum distance?\n\n29.8 miles\n\nHow many vulnerable tracts are more than 15 miles from the nearest hospital?\n\n33\n\n\n\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n# Create underserved variable\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(underserved = ifelse(as.numeric(distance_to_nearst_hospital) &gt; 15,\"Underserved\",\"\"))\n\nvulnerable_tracts %&gt;%\n  group_by(underserved)%&gt;%\n  summarise(n())\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1272296 ymin: 1971050 xmax: 1776268 ymax: 2294525\nProjected CRS: NAD83 / Conus Albers\n# A tibble: 2 × 3\n  underserved   `n()`                                                   geometry\n  &lt;chr&gt;         &lt;int&gt;                                         &lt;MULTIPOLYGON [m]&gt;\n1 \"\"             1112 (((1324165 2015512, 1324049 2015276, 1323967 2015195, 132…\n2 \"Underserved\"    33 (((1391776 2000059, 1392091 2000026, 1392335 1999901, 139…\n\n\nQuestions to answer:\n\nHow many tracts are under-served?\n\n33\n\nWhat percentage of vulnerable tracts are under-served?\n\n3%\n\nDoes this surprise you? Why or why not?\n\nUnfortunately, this does not surprise me, given how rural certain areas of Pennsylvania are and knowing how rural healthcare is a persistent issue in the US in general. However, it would surprise me if some of these tracts are in urban areas.\n\n\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\n# Spatial join tracts to counties\npa_counties &lt;- pa_counties %&gt;% st_transform(st_crs(vulnerable_tracts))\n\nvulnerable_per_county &lt;- vulnerable_tracts %&gt;%\n  st_join(pa_counties) %&gt;%\n  st_drop_geometry()\n\n# Aggregate statistics by county\nvulnerable_per_county_stats &lt;- vulnerable_per_county %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarise(\n    num_vulnerable_tracts = n(),\n    num_underserved_tracts = sum(underserved == \"Underserved\"),\n    percent_underserved = sum(underserved == \"Underserved\")/n(),\n    avg_distance_to_nearst_hospital = mean(distance_to_nearst_hospital),\n    total_pop = sum(total_popE),\n    total_underserved_pop = sum(ifelse(underserved == \"Underserved\", total_popE, 0))\n  )\n\nRequired county-level statistics:\n\nNumber of vulnerable tracts\nNumber of underserved tracts\n\nPercentage of vulnerable tracts that are underserved\nAverage distance to nearest hospital for vulnerable tracts\nTotal vulnerable population\n\nQuestions to answer:\n\nWhich 5 counties have the highest percentage of underserved vulnerable tracts?\n\nCameron\nJuniata\nPotter\nSnyder\nSullivan\n\nWhich counties have the most vulnerable people living far from hospitals?\n\nClearfield\nChester\nJuniata\nSnyder\nPike\n\nAre there any patterns in where underserved counties are located?\n\nThere are a lot of quite rural underserved counties, like Sullivan, Juniata, and Clearfield, but there are also suburban counties of major PA cities like Chester County outside of Philadelphia and Cumberland County outside Harrisburg.\n\n\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n# Create and format priority counties table\nvulnerable_per_county_stats %&gt;% \n  arrange(desc(total_underserved_pop)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  select(\n    c(\"COUNTY_NAM\", \"total_underserved_pop\", \"percent_underserved\", \"avg_distance_to_nearst_hospital\")\n  ) %&gt;%\n  mutate(\n    avg_distance_to_nearst_hospital = as.numeric(avg_distance_to_nearst_hospital),\n    percent_underserved = paste0(round(percent_underserved*100, 2), \"%\")\n  )%&gt;%\n  kable(\n    col.names = c(\"County\", \"Total Underserved Population\", \"Percent of Vulnerable Tracts that are Underserved\", \"Average Distance of to Nearest Hospital\"),\n    digit = 1,\n    format.args = list(big.mark = \",\"),\n    align = \"l\",\n    caption = \"10 Counties in PA with the Highest Absolute Population more than 15 miles from Nearest Hospital\"\n  )\n\n\n10 Counties in PA with the Highest Absolute Population more than 15 miles from Nearest Hospital\n\n\n\n\n\n\n\n\nCounty\nTotal Underserved Population\nPercent of Vulnerable Tracts that are Underserved\nAverage Distance of to Nearest Hospital\n\n\n\n\nCLEARFIELD\n17,027\n28.57%\n12.4\n\n\nCHESTER\n15,467\n7.5%\n6.1\n\n\nJUNIATA\n13,955\n50%\n15.4\n\n\nSNYDER\n12,073\n50%\n15.1\n\n\nPIKE\n10,292\n35.29%\n15.4\n\n\nDAUPHIN\n8,815\n8.33%\n5.3\n\n\nSCHUYLKILL\n8,815\n7.41%\n7.0\n\n\nPERRY\n8,761\n22.22%\n12.3\n\n\nLANCASTER\n8,055\n5.71%\n5.7\n\n\nCENTRE\n6,843\n5.88%\n6.2\n\n\n\n\n\nRequirements:\n\nUse knitr::kable() or similar for formatting\nInclude descriptive column names\nFormat numbers appropriately (commas for population, percentages, etc.)\nAdd an informative caption\nSort by priority (you decide the metric)"
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html#part-2-comprehensive-visualization",
    "href": "labs/lab2/assignment2_kk.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\n# Create county-level access map\nvulnerable_per_county_stats &lt;- vulnerable_per_county_stats %&gt;%\n  left_join(pa_counties, by=\"COUNTY_NAM\")\n\nvulnerable_per_county_stats &lt;- vulnerable_per_county_stats %&gt;%\n  st_as_sf() %&gt;% \n  st_transform(st_crs(census_tracts)) %&gt;%\n  mutate(percent_underserved = percent_underserved*100\n  )\n\nhospitals &lt;- hospitals %&gt;%\n  mutate(type = \"Hospital\")\n\nggplot(vulnerable_per_county_stats) +\n  geom_sf(aes(fill = percent_underserved)) +\n  scale_fill_gradient(\n    low = \"#C3CDFE\",   \n    high = \"#485EFE\",   \n    name = \"% Underserved Tracts\"\n  ) +\n  new_scale_fill()+\n  geom_sf(\n    data = hospitals, \n    aes(fill=type),      \n    size = 3, \n    shape = 21, \n    color = \"#FF8600\",     \n    stroke = 0.8\n  ) +\n  scale_fill_manual(\n    values = c(\"Hospital\" = \"#FF8600\"),\n    name = NULL\n  )+\n  labs(\n    title = \"Underserved Tracts per Counties in Pennsylvania\",\n    subtitle = str_wrap(\"Where an underserved tract is one where the center is at least 15 miles from the nearest hospital\",50),\n    caption =\"Data Sources: US Census Data, OpenDataPhilly\"\n  ) +\n  annotation_north_arrow(             \n    location = \"tr\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    legend.direction = \"vertical\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\nRequirements: - Fill counties by percentage of vulnerable tracts that are underserved - Include hospital locations as points - Use an appropriate color scheme - Include clear title, subtitle, and caption - Use theme_void() or similar clean theme - Add a legend with formatted labels\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\n# Create detailed tract-level map\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  st_as_sf() %&gt;% \n  st_transform(st_crs(census_tracts))\n\nunderserved_tracts &lt;- vulnerable_tracts[vulnerable_tracts$underserved==\"Underserved\",]\n\npa_counties$legend &lt;- \"County Boundary\"\n\nggplot(census_tracts) +\n  geom_sf(\n    color=\"darkgrey\",\n    size=.5\n  )+\n  geom_sf(\n    data=underserved_tracts,\n    aes(fill = underserved)\n  )+\n  geom_sf(\n    data=pa_counties,\n    aes(color=legend),     \n    linewidth = .8,\n    fill=NA\n  )+\n  geom_sf(\n    data=hospitals,\n    aes(fill = type),      \n    size = 1, \n    shape = 21, \n    color = \"#FF8600\",     \n    stroke = 0.8\n  )+\n  scale_fill_manual(\n    values = c(\"Underserved\" = \"#485EFE\",\n               \"Hospital\" = \"#FF8600\"),\n    name = NULL  # or \"Legend\" if you want a title\n  ) +\n  scale_color_manual(\n    values = c(\"County Boundary\" = \"black\"),\n    name = NULL\n  ) +\n  annotation_north_arrow(             \n    location = \"tr\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void()+\n  labs(\n    title = \"Underserved Tracts in Pennsylvania\",\n    subtitle = str_wrap(\"Where an underserved tract's center is at least 15 miles from the nearest hospital\",50),\n    caption =\"Data Sources: US Census Data, OpenDataPhilly\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\nRequirements: - Show underserved vulnerable tracts in a contrasting color - Include county boundaries for context - Show hospital locations - Use appropriate visual hierarchy (what should stand out?) - Include informative title and subtitle\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create distribution visualization\n\ndep_regions&lt;- st_read(\"data/DEPRegions2024_03.shp\")\n\nReading layer `DEPRegions2024_03' from data source \n  `C:\\Users\\knoxk\\OneDrive\\Documents\\musa-5080\\portfolio-setup-kkxix\\labs\\lab2\\data\\DEPRegions2024_03.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -8963379 ymin: 4825316 xmax: -8314404 ymax: 5201420\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ndep_regions &lt;- dep_regions %&gt;% st_transform(st_crs(vulnerable_tracts))\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;% \n  mutate(\n      urban_rural = case_when(\n      total_popE &gt;= 5000 ~ \"Urban\",\n      TRUE ~ \"Rural\"\n      )\n  )\n\nvulnerable_tracts_with_regions &lt;- vulnerable_tracts %&gt;%\n  st_join(dep_regions) %&gt;%\n  st_drop_geometry()\n\nvulnerable_tracts_with_regions &lt;- vulnerable_tracts_with_regions %&gt;%\n  mutate(\n    Region = case_when(\n      SNAME == \"NCRO\"~\"North Central\",\n      SNAME ==\"NERO\"~\"North East\",\n      SNAME==\"NWRO\"~\"North West\",\n      SNAME==\"SCRO\"~\"South Central\",\n      SNAME==\"SERO\"~\"South East\",\n      SNAME==\"SWRO\"~\"South West\"\n    )\n  )\n  \n  \nggplot(vulnerable_tracts_with_regions)+\n  geom_boxplot(\n    aes(x=Region, y=distance_to_nearst_hospital)\n  )+\n  labs(\n    title=\"Distance of Vulnerable Tracts to Nearest Hospital by Region\",\n    caption=str_wrap(\"Where a vulnerable tract is in the top 25% of concentration of elderly population or 215% of the Federal Poverty Line for 2-person household. Data sources: US Census, Pennsylvania Spatial Data Access  -- Department of Environmental Protection Regions\",100),\n    y=str_wrap(\"Distance from Center of Tract to Nearest Hospital\",50),\n    x=\"Region of Tract\"\n  )+\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\nNorthern Pennsylvania vulnerable populations generally are location farther from hospitals than Southern Pennsylvania, with North Central PA having the highest median and upper quartiles of distance to the nearest hospital of all 6 regions. However another trend we can see is that the highest number of outliers fall within the South East and South West regions, home to the two biggest cities in PA, Philadelphia and Pittsburgh, respectively. That shows that these regions have more tracts that are facing very different needs that the middle 50% of the region.\nSuggested chart types: - Histogram or density plot of distances - Box plot comparing distances across regions - Bar chart of underserved tracts by county - Scatter plot of distance vs. vulnerable population size\nRequirements: - Clear axes labels with units - Appropriate title - Professional formatting - Brief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html#part-3-bring-your-own-data-analysis",
    "href": "labs/lab2/assignment2_kk.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\n\n\n\nYour Analysis\n\nDigital Justice\nOption C: Digital Equity\n\nData: Census Broadband access, device access, and income, Philadelphia free wifi spots from OpenDataPhilly\nQuestion: “Do digitally disadvantaged neighborhoods have equitable access to city internet?”\nOperations: Buffer free wifi spots with a computer (10-minute walk = 0.5 mile), calculate connectivity by tract, determine digitally vulnerable tracts from census data and underserved tracts by distance from buffers.\nPolicy relevance: Digital equity, broadband infrastructure, internet-connected device access\n\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load your additional dataset\nv21 &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n\n# Search for broadband and computer-related variables\nbb_vars &lt;- v21 %&gt;% filter(str_detect(label, \"broadband\"))\ncomp_vars &lt;-v21 %&gt;% filter(str_detect(label, \"computer\"))\n\n#B28003_002 = total with computer\n#B28002_004 = total with broadband internet subscription\n\nphilly_digital_access &lt;-  get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    has_computer = \"B28003_002\", \n    has_broadband = \"B28002_004\"\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2023,\n  output = \"wide\"\n)\n\nfree_wifi_spots &lt;- st_read(\"data/free_city_wifi_locations.shp\")%&gt;%st_transform(2272)\n\nReading layer `free_city_wifi_locations' from data source \n  `C:\\Users\\knoxk\\OneDrive\\Documents\\musa-5080\\portfolio-setup-kkxix\\labs\\lab2\\data\\free_city_wifi_locations.shp' \n  using driver `ESRI Shapefile'\nreplacing null geometries with empty geometries\nSimple feature collection with 254 features and 47 fields (with 1 geometry empty)\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8378350 ymin: 4852065 xmax: -8345555 ymax: 4883809\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nphilly_digital_access &lt;- left_join(\n  philly_digital_access,\n  census_tracts,\n  by=\"GEOID\"\n) %&gt;%\n  st_as_sf() %&gt;%\n  st_transform(2272)\n\nggplot(philly_digital_access)+\n  geom_sf()+\n  geom_sf(\n    data=free_wifi_spots\n  )+\n  theme_void()\n\n\n\n\n\n\n\n\nQuestions to answer:\n\nWhat dataset did you choose and why?\n\nI choose the ACS variables showing whether a household has broadband access and whether they have a computer, as well as OpenDataPhilly’s database of free wi-fi locations, which inlcudes a variable showing how many public use computers are available at the location.\n\nWhat is the data source and date?\n\nThe ACS data is 5-year data from 2018 - 2023, and the wi-fi locations are from 2024, so the home broadband and computer access data may be slightly outdated. Also important to note is 2018 - 2023 range includes the pandemic, a period in which there was unprecedented investment in home internet subsidies that have expired in 2024 .\n\nHow many features does it contain?\n\nThere are 254 free-wifi locations, and 408 census tracts in Philadelphia.\n\nWhat CRS is it in? Did you need to transform it?\n\nBefore transformation, both datasets were in web mercator, EPSG 3857, and since Philly falls within southern pennsylvania, I transformed it to Pennsylvania South state plane projection, EPSG 2272.\n\n\n\nPose a research question"
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "labs/lab2/assignment2_kk.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nTake a few moments to clean up your markdown document and then write a line or two or three about how you may have incorporated feedback that you recieved after your first assignment."
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html#submission-requirements",
    "href": "labs/lab2/assignment2_kk.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 5 Notes",
    "section": "",
    "text": "General Problem: We observe data and we believe there’s some relationship between these variables. Statistical Learning = a set of approaches for estimating that relationship.\nFormalizing the Relationship: For response Y and predicators X_1, X_2,...X_n: Y = f(X) + E where E is error term.\nWhat is f()? – the true relationship between predictors and outcome, fixed but unknown\n\nWe want to know f() not only to predict Y, but to learn about/infer relationship between Xs\nTwo broad approaches:\n\nParametric\n\nmake and assumption about the form, i.e. predefine the shape the the slope, easier to interpret, reduces problem to estimating a few parameters\nunintuitively perhaps, modern AI is parametric, just with an insane number of parameters\n\nNon-parametric\n\ndoesn’t assume a form, it’s about find the best bit line, requires more data and is a bit harder to interpret\n\n\n\n\n\n\n\nAssumption: Relationship between x and y is linear\nTask : What are coefficients B in Y = B_1X_1 + B_2X_2 ...\nMethod: Ordinary Least Squares\nWhy? Simple, well-understood\nIn R : lm(Y ~ X, data = data)\n\nInterpreting coefficients:\n\nB_0 is intercept, i.e. what is dependent variable when independent is 0\nB_1 is slope\nRecall, all stats is based on sample, we never have full universe of data.\nTrue relationship is unknowable, but based on different samples, we may get relationships more or less close to unknowable true relationship.\nNull hypothesis: The relationship actually 0, we get absolutely no information about Y from X.\nSo, “statistical significance” is saying there is enough evidence to disprove null hypothesis\nHow we do this: how weird would the data sample have to be for null hypothesis to be true?\nIs relationship real? p-value is the probability that the null hypothesis is true given the sample. We are looking for small p-values.\n\n\n\n\n\nFor inference, how explanatory is model relationship between X and Y – R-squared tells us how fitting the line is to sample.\nFor prediction, how well would model fit new data? R-squared alone does not tell us if model is trustworthy\noverfitting – model is too good at predicting one sample, R-squared is crazy good, “following noise” essentially, won’t generalize well though to other samples\nunderfitting – model ignores the relationship entirely, low R-squared\nperfect fit – “the gist of things”\n\n\n\n\n\nSolution: hold out some data to test predictions\n\nsay we have 67 counties in PA, train model on 70% of sample, test on 30%\n\nset.seed() will preserve random selection if you run code multiple times\n\n\n\n\n\nRoot means square errors, RMSE – i.e. “our predictions are off the real observations by RMSE units” – is this good enough?\n\n\n\n\n\nhow well does models apply to new datasets?\nK-fold cross-validation - fold the 30/70 split at different segments and run train/test on each one, average RMSE across all\nin R: library(caret) functions trainControl() sets a cross-validation method with number of folds, train() creates mode based on data and the object returned from trainControl()\n\n\n\n\n\nLinear regression makes assumptions, if violated:\n\ncoefficients may be biased\nSE may be wrong\n\nheteroscedasticity – variance changes across X\nformal test: Breusch-Pagan test, p&lt;.05 bad\n\n\n\n\n\n\nheteroscedasticity\n\n\npredictions unreliable\nNormality of residuals - Solution Q-Q Plot\nno multicollinearity - vif()\noutliers\n\n\n\n\nadd more features/variables\ncreate categorical variables\n\n\n\n\n\nunderstand the framework\nvisualize first\nfit the model\nevaluate performance (cross validation)\ncheck assumptions\nimprove model if needed\nconsider ethics (who could be harmed?)"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#notes",
    "href": "weekly-notes/week-05-notes.html#notes",
    "title": "Week 5 Notes",
    "section": "",
    "text": "General Problem: We observe data and we believe there’s some relationship between these variables. Statistical Learning = a set of approaches for estimating that relationship.\nFormalizing the Relationship: For response Y and predicators X_1, X_2,...X_n: Y = f(X) + E where E is error term.\nWhat is f()? – the true relationship between predictors and outcome, fixed but unknown\n\nWe want to know f() not only to predict Y, but to learn about/infer relationship between Xs\nTwo broad approaches:\n\nParametric\n\nmake and assumption about the form, i.e. predefine the shape the the slope, easier to interpret, reduces problem to estimating a few parameters\nunintuitively perhaps, modern AI is parametric, just with an insane number of parameters\n\nNon-parametric\n\ndoesn’t assume a form, it’s about find the best bit line, requires more data and is a bit harder to interpret\n\n\n\n\n\n\n\nAssumption: Relationship between x and y is linear\nTask : What are coefficients B in Y = B_1X_1 + B_2X_2 ...\nMethod: Ordinary Least Squares\nWhy? Simple, well-understood\nIn R : lm(Y ~ X, data = data)\n\nInterpreting coefficients:\n\nB_0 is intercept, i.e. what is dependent variable when independent is 0\nB_1 is slope\nRecall, all stats is based on sample, we never have full universe of data.\nTrue relationship is unknowable, but based on different samples, we may get relationships more or less close to unknowable true relationship.\nNull hypothesis: The relationship actually 0, we get absolutely no information about Y from X.\nSo, “statistical significance” is saying there is enough evidence to disprove null hypothesis\nHow we do this: how weird would the data sample have to be for null hypothesis to be true?\nIs relationship real? p-value is the probability that the null hypothesis is true given the sample. We are looking for small p-values.\n\n\n\n\n\nFor inference, how explanatory is model relationship between X and Y – R-squared tells us how fitting the line is to sample.\nFor prediction, how well would model fit new data? R-squared alone does not tell us if model is trustworthy\noverfitting – model is too good at predicting one sample, R-squared is crazy good, “following noise” essentially, won’t generalize well though to other samples\nunderfitting – model ignores the relationship entirely, low R-squared\nperfect fit – “the gist of things”\n\n\n\n\n\nSolution: hold out some data to test predictions\n\nsay we have 67 counties in PA, train model on 70% of sample, test on 30%\n\nset.seed() will preserve random selection if you run code multiple times\n\n\n\n\n\nRoot means square errors, RMSE – i.e. “our predictions are off the real observations by RMSE units” – is this good enough?\n\n\n\n\n\nhow well does models apply to new datasets?\nK-fold cross-validation - fold the 30/70 split at different segments and run train/test on each one, average RMSE across all\nin R: library(caret) functions trainControl() sets a cross-validation method with number of folds, train() creates mode based on data and the object returned from trainControl()\n\n\n\n\n\nLinear regression makes assumptions, if violated:\n\ncoefficients may be biased\nSE may be wrong\n\nheteroscedasticity – variance changes across X\nformal test: Breusch-Pagan test, p&lt;.05 bad\n\n\n\n\n\n\nheteroscedasticity\n\n\npredictions unreliable\nNormality of residuals - Solution Q-Q Plot\nno multicollinearity - vif()\noutliers\n\n\n\n\nadd more features/variables\ncreate categorical variables\n\n\n\n\n\nunderstand the framework\nvisualize first\nfit the model\nevaluate performance (cross validation)\ncheck assumptions\nimprove model if needed\nconsider ethics (who could be harmed?)"
  },
  {
    "objectID": "labs/lab2/assignment2_kk.html#do-digitally-vulnerable-tracts-have-adequate-access-to-free-wifi-spots-with-computers",
    "href": "labs/lab2/assignment2_kk.html#do-digitally-vulnerable-tracts-have-adequate-access-to-free-wifi-spots-with-computers",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Do digitally vulnerable tracts have adequate access to free wifi spots with computers?",
    "text": "Do digitally vulnerable tracts have adequate access to free wifi spots with computers?\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+):\n\nBuffers\nSpatial joins\nSpatial filtering with predicates\nDistance calculations\nIntersections or unions\nPoint-in-polygon aggregation\n\nYour Task:\n\n# Your spatial analysis\n\n#Filter Philly census tracts by the most digitally vulnerable: in the top quartile of percent of tract that has no home broadband or top quartile of percent of tract with no computer. \nphilly_digital_access &lt;- philly_digital_access %&gt;%\n  mutate(\n    percent_no_bb = (1-has_broadbandE/total_popE.x)*100,\n    percent_no_comp = (1-has_computerE/total_popE.x)*100\n  )\n\nsummary(philly_digital_access$percent_no_bb)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  25.95   56.43   62.88   61.71   68.65  100.00      17 \n\nsummary(philly_digital_access$percent_no_comp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  25.20   54.34   60.27   59.29   65.62  100.00      17 \n\n\n\n#based on summary stats, the most digitally vulnerable are tract with 68.65% or more of households that do not have home broadband or 65.62% or more of households that do not have a computer. \n\ndigit_vulnerable_philly &lt;- philly_digital_access %&gt;%\n  filter(percent_no_bb&gt;=68.65 | percent_no_comp&gt;=65.62 )\n\n#visually examine digitally vulnerable tracts and free wifi spots with computers\nggplot(philly_digital_access)+\n  geom_sf()+\n  geom_sf(\n    data=digit_vulnerable_philly,\n    fill=\"purple\"\n  )+\n  geom_sf(data=free_wifi_spots[free_wifi_spots$computers_==\"Y\",])+\n  theme_void()\n\n\n\n\n\n\n\n\n\n# Find nearest free wifi spots with a computer to tract centroids \ndigit_vulnerable_centroids &lt;- st_centroid(digit_vulnerable_philly)\nnearest_wifi_spot_comp &lt;- st_nearest_feature(digit_vulnerable_centroids, free_wifi_spots[free_wifi_spots$computers_==\"Y\",])\n\ndigit_vulnerable_philly &lt;- digit_vulnerable_philly %&gt;%\n  mutate(nearest_wifi_spot_geom = free_wifi_spots$geometry[nearest_wifi_spot_comp])\n\ndigit_vulnerable_philly &lt;- digit_vulnerable_philly %&gt;%\n  mutate(distance_wifi_spot_comp = set_units(st_distance(digit_vulnerable_centroids, digit_vulnerable_philly$nearest_wifi_spot_geom, by_element = TRUE), \"mi\"))\n\nsummary(digit_vulnerable_philly$distance_wifi_spot_comp) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3979  3.3372  5.8429  6.0722  7.9510 13.7746 \n\n\n\n#Find walking distance buffers around free wifi spots with computers \nfree_wifi_buffers &lt;- free_wifi_spots %&gt;%\n  filter(computers_==\"Y\" & to_display==\"ACTIVE\")%&gt;%\n  st_buffer(dist = 2640)  # 2640 ft = .5 mi\n\n\n#dissolve overlapping buffers \nfree_wifi_buffers_dissolve &lt;- free_wifi_buffers%&gt;%\n  st_union() %&gt;%\n  st_cast(\"POLYGON\")%&gt;%\n  st_as_sf()%&gt;%\n    mutate(\n    legend= \".5 mi from Free Wifi Spot w Computer\"\n  )\n\n  \n#examine buffers and tracts overlap\nggplot(philly_digital_access)+\n  geom_sf(\n    color=\"darkgray\",\n    linewidth=.5,\n    alpha=.5\n  )+\n  geom_sf(\n    data=digit_vulnerable_philly,\n    aes(fill=percent_no_bb+percent_no_comp)\n  )+\n  scale_fill_gradient(\n    low=\"#FFCF33\",\n    high=\"#F53D00\",\n    name=\"% No broadband + % No Computer\"\n  )+\n  new_scale_fill()+\n  geom_sf( \n    data=free_wifi_buffers_dissolve,\n    aes(fill=legend),\n    color=\"black\",\n    linewidth=.8,\n    alpha=0.4\n  )+\n  scale_color_manual(\n    values = c(\".5 mi from Free Wifi Spot w Computer\" =  \"#F5A6E6\") ,\n    name=NULL\n  )+\n  theme_void()\n\n\n\n\n\n\n\n\n\n#Define underserved tract as a tract that is digitally vulnerable and outside walking distance to a free wifi spot with a computer. \n\noverlap &lt;- st_intersects(digit_vulnerable_philly, free_wifi_buffers)\n\nno_overlap &lt;- lengths(overlap) == 0\n\ndigit_underserved &lt;- digit_vulnerable_philly[no_overlap, ]\n\nsummary(digit_underserved)\n\n    GEOID               NAME            total_popE.x   total_popM.x   \n Length:30          Length:30          Min.   :1027   Min.   :  82.0  \n Class :character   Class :character   1st Qu.:3868   1st Qu.: 624.0  \n Mode  :character   Mode  :character   Median :4604   Median : 808.0  \n                                       Mean   :4793   Mean   : 927.2  \n                                       3rd Qu.:6202   3rd Qu.:1139.5  \n                                       Max.   :8145   Max.   :2091.0  \n                                                                      \n has_computerE  has_computerM   has_broadbandE has_broadbandM \n Min.   :   0   Min.   : 11.0   Min.   :   0   Min.   : 11.0  \n 1st Qu.:1121   1st Qu.:155.8   1st Qu.:1103   1st Qu.:165.0  \n Median :1515   Median :226.0   Median :1328   Median :229.0  \n Mean   :1444   Mean   :223.7   Mean   :1346   Mean   :227.7  \n 3rd Qu.:1785   3rd Qu.:291.0   3rd Qu.:1677   3rd Qu.:284.0  \n Max.   :2528   Max.   :431.0   Max.   :2247   Max.   :444.0  \n                                                              \n   STATEFP            COUNTYFP           TRACTCE            GEOIDFQ         \n Length:30          Length:30          Length:30          Length:30         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    NAME.x            NAMELSAD            STUSPS           NAMELSADCO       \n Length:30          Length:30          Length:30          Length:30         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n  STATE_NAME            LSAD               ALAND             AWATER       \n Length:30          Length:30          Min.   : 184674   Min.   :      0  \n Class :character   Class :character   1st Qu.: 491366   1st Qu.:      0  \n Mode  :character   Mode  :character   Median : 613069   Median :      0  \n                                       Mean   : 863564   Mean   :  46550  \n                                       3rd Qu.: 854698   3rd Qu.:      0  \n                                       Max.   :4488319   Max.   :1362058  \n                                                                          \n    NAME.y          median_incomeE   median_incomeM   total_popE.y \n Length:30          Min.   : 13721   Min.   : 2945   Min.   :1027  \n Class :character   1st Qu.: 38976   1st Qu.:11549   1st Qu.:3868  \n Mode  :character   Median : 54871   Median :14980   Median :4604  \n                    Mean   : 54849   Mean   :17043   Mean   :4793  \n                    3rd Qu.: 62857   3rd Qu.:22100   3rd Qu.:6202  \n                    Max.   :101094   Max.   :43870   Max.   :8145  \n                    NA's   :1        NA's   :1                     \n  total_popM.y       over_65E         over_65M               geometry \n Min.   :  82.0   Min.   :  0.00   Min.   :  3.00   MULTIPOLYGON :30  \n 1st Qu.: 624.0   1st Qu.:  6.00   1st Qu.: 13.00   epsg:2272    : 0  \n Median : 808.0   Median : 23.00   Median : 28.50   +proj=lcc ...: 0  \n Mean   : 927.2   Mean   : 31.63   Mean   : 36.50                     \n 3rd Qu.:1139.5   3rd Qu.: 50.50   3rd Qu.: 50.75                     \n Max.   :2091.0   Max.   :123.00   Max.   :121.00                     \n                                                                      \n percent_no_bb    percent_no_comp    nearest_wifi_spot_geom\n Min.   : 67.08   Min.   : 63.23   POINT        :30        \n 1st Qu.: 69.48   1st Qu.: 66.97   epsg:2272    : 0        \n Median : 70.93   Median : 68.08   +proj=lcc ...: 0        \n Mean   : 72.34   Mean   : 70.55                           \n 3rd Qu.: 73.19   3rd Qu.: 71.73                           \n Max.   :100.00   Max.   :100.00                           \n                                                           \n distance_wifi_spot_comp\n Min.   : 2.465         \n 1st Qu.: 5.471         \n Median : 6.020         \n Mean   : 7.138         \n 3rd Qu.: 9.588         \n Max.   :13.775         \n                        \n\n#what are characteristics of top 10 digitally underserved tracts by absolute number of those with no broadband + those with no computer? \n\n#note, the sum of estiamated households without computer and without broadband is not an accurate estimate of reality because those without one are likely overlapping with the other. This metric is just being used to create a metric showing reflecting broadband and computer need in one. \ndigit_underserved&lt;-digit_underserved%&gt;%\n  mutate(\n    digit_need_heuristic = (total_popE.x - has_computerE)+(total_popE.x - has_broadbandE)\n  )\n\ntop_10_digit_underserved &lt;- digit_underserved%&gt;%\n  arrange(desc(digit_need_heuristic))%&gt;%\n  slice_head(n = 10)\n \ntop_10_digit_underserved %&gt;%\n  st_drop_geometry()%&gt;%\n  select(c(\n    \"percent_no_bb\",\n    \"percent_no_comp\",\n    \"median_incomeE\",\n    \"distance_wifi_spot_comp\"\n  )) %&gt;%\n  kable(\n    col.names = c(\"Percent w/o Broadband\",\n                  \"Percent w/o a Computer\", \n                  \"Median Income\", \n                  \"Distance to Free Wi-Fi spot w/ Computer\"),\n    digits = 1,\n    format.args = list(big.mark = \",\"),\n    align = \"l\",\n    caption = \"10 Census Tracts in Philadelphia that are most Digitally Vulnerable and Distance to Nearest Free Wi-Fi Spot with a Public Computer\"\n  )\n\n\n10 Census Tracts in Philadelphia that are most Digitally Vulnerable and Distance to Nearest Free Wi-Fi Spot with a Public Computer\n\n\n\n\n\n\n\n\nPercent w/o Broadband\nPercent w/o a Computer\nMedian Income\nDistance to Free Wi-Fi spot w/ Computer\n\n\n\n\n82.4\n79.6\n38,976\n2.464926 [mi]\n\n\n73.3\n70.4\n13,721\n7.369100 [mi]\n\n\n75.5\n71.6\n56,818\n10.021851 [mi]\n\n\n73.6\n71.7\n68,652\n9.960189 [mi]\n\n\n70.9\n67.3\n33,419\n5.459912 [mi]\n\n\n76.1\n74.2\n50,245\n5.314348 [mi]\n\n\n69.9\n67.5\n100,857\n13.774589 [mi]\n\n\n67.8\n67.5\n71,551\n3.367629 [mi]\n\n\n67.6\n67.1\n54,871\n8.256588 [mi]\n\n\n68.1\n66.5\n93,300\n12.009875 [mi]\n\n\n\n\n\n\n#Map the underserved tracts and wifi spots buffers \ndigit_underserved&lt;-digit_underserved%&gt;%\n  mutate(\n    legend=\"Digitally Underserved Tract\"\n  )\n\nggplot(philly_digital_access)+\n  geom_sf(\n    color=\"darkgray\",\n    linewidth=.5,\n    alpha=.5\n  )+\n  geom_sf(\n    data=digit_underserved,\n    aes(fill=\"Digitally Underserved Tract\")\n  )+\n  scale_fill_manual(\n    values = c(\"Digitally Underserved Tract\" = \"#3F88C5\"),\n    name=NULL\n  )+\n  new_scale_fill()+\n  geom_sf( \n    data=free_wifi_buffers_dissolve,\n    aes(fill=\".5 mi from Free Wifi Spot w Computer\"),\n    color=\"black\",\n    linewidth=.8,\n    alpha=0.4\n  )+\n  scale_color_manual(\n    values = c(\".5 mi from Free Wifi Spot w Computer\" =  \"#F5A6E6\") \n  )+\n  guides(fill = guide_legend(title = NULL))+\n  theme_void()\n\n\n\n\n\n\n\n\nAnalysis requirements: - Clear code comments explaining each step - Appropriate CRS transformations - Summary statistics or counts - At least one map showing your findings - Brief interpretation of results (3-5 sentences)\nYour interpretation:\n[Write your findings here]"
  },
  {
    "objectID": "labs/lab2/Knox_Katie_Assignment2.html",
    "href": "labs/lab2/Knox_Katie_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives:\n\nApply spatial operations to answer policy-relevant research questions\nIntegrate census demographic data with spatial analysis\nCreate publication-quality visualizations and maps\nWork with spatial data from multiple sources\nCommunicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/lab2/Knox_Katie_Assignment2.html#assignment-overview",
    "href": "labs/lab2/Knox_Katie_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives:\n\nApply spatial operations to answer policy-relevant research questions\nIntegrate census demographic data with spatial analysis\nCreate publication-quality visualizations and maps\nWork with spatial data from multiple sources\nCommunicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/lab2/Knox_Katie_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "labs/lab2/Knox_Katie_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data:\n\nPennsylvania county boundaries\nPennsylvania hospitals (from lecture data)\nPennsylvania census tracts\n\nYour Task:\n\n# Load spatial data\npa_counties &lt;- st_read(\"data/Pennsylvania_County_Boundaries.shp\")\nhospitals &lt;- st_read(\"data/hospitals.geojson\")\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n\n# Check that all data loaded correctly\nglimpse(pa_counties)\nglimpse(census_tracts)\nglimpse(hospitals)\n\n\ncensus_tracts &lt;- st_transform(census_tracts, st_crs(pa_counties))\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\n\n\n# With ggplot2\np1 &lt;- ggplot(pa_counties) +\n  geom_sf() +\n  theme_void()\n\np2 &lt;- ggplot(census_tracts) +\n  geom_sf() +\n  theme_void()\n\np3 &lt;- ggplot(hospitals) +\n  geom_sf() +\n  theme_void()\n\np1 | p2 | p3\n\n\n\n\n\n\n\n\nQuestions to answer:\n\nHow many hospitals are in your dataset?\n\n223\n\nHow many census tracts?\n\n3445\n\nWhat coordinate reference system is each dataset in?\n\nI transformed hospitals and census tracts to PA counties CRS, which is WGS84. Before that, hospitals were also in WGS84, but census tracts were in NAD83.\n\n\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables:\n\nTotal population\nMedian household income\nPopulation 65 years and over (you may need to sum multiple age categories)\n\nYour Task:\n\n# Get demographic data from ACS\npa_tracts_data &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    median_income = \"B19013_001\",\n    total_pop = \"B01003_001\",\n    over_65 = \"B01001_020\"  # Population 65 years and over\n  ),\n  state = \"PA\",\n  year = 2023,\n  output = \"wide\"\n)\n\n# Join to tract boundaries\ncensus_tracts &lt;- census_tracts %&gt;%\n  left_join(pa_tracts_data, by= \"GEOID\")\n\nsummary(census_tracts)\n\nQuestions to answer:\n\nWhat year of ACS data are you using?\n\n2019-2023 5-year ACS data\n\nHow many tracts have missing income data?\n\n65\n\nWhat is the median income across all PA census tracts?\n\n$72,944\n\n\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria:\n\nLow median household income (choose an appropriate threshold)\nSignificant elderly population (choose an appropriate threshold)\n\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\nvulnerable_tracts &lt;- census_tracts %&gt;%\n  filter(median_incomeE &lt; 42398 | (over_65E/total_popE) &gt; .017)\n\nQuestions to answer:\n\nWhat income threshold did you choose and why?\n\nI looked at the 2023 poverty guidelines, the average household size in PA in 2023, which was 2.4, and then looked into eligibility for various programs, which is anywhere from 133% FPIG to 215%. I am going with vulnerable household is 215% or less of the 2-person household FPIG, or $42,398.\n\nWhat elderly population threshold did you choose and why?\n\nIn order to control for overall population size, instead of looking at the raw number of elderly, I looked at the over 65 as percent of total population. I defined vulnerable as any tract in the 75th percentile of elderly percent of tract population (1.7% or above).\n\nHow many tracts meet your vulnerability criteria?\n\n1,145\n\nWhat percentage of PA census tracts are considered vulnerable by your definition?\n\n33.2% – about a third.\n\n\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n#convert to Albers\nvulnerable_tracts &lt;- st_transform(vulnerable_tracts, crs = 5070)\nhospitals &lt;- st_transform(hospitals, crs = 5070)\n\n# Calculate distance from each tract centroid to nearest hospital\ntract_centroids &lt;- st_centroid(vulnerable_tracts)\nnearest_hospital &lt;- st_nearest_feature(tract_centroids, hospitals)\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(nearest_hospital_geom = hospitals$geometry[nearest_hospital])\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(distance_to_nearst_hospital = set_units(st_distance(tract_centroids, vulnerable_tracts$nearest_hospital_geom, by_element = TRUE), \"mi\"))\n\nsummary(vulnerable_tracts$distance_to_nearst_hospital) \n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 0.02107  1.21850  2.75761  4.37494  6.02931 29.75156 \n\n\nRequirements:\n\nUse an appropriate projected coordinate system for Pennsylvania\nCalculate distances in miles\nExplain why you chose your projection\n\nI chose Albers Conical Equal Area, NAD 83 projection based on this PA document from 2011’s recommendation.\nQuestions to answer:\n\nWhat is the average distance to the nearest hospital for vulnerable tracts?\n\n4.4 miles\n\nWhat is the maximum distance?\n\n29.8 miles\n\nHow many vulnerable tracts are more than 15 miles from the nearest hospital?\n\n33\n\n\n\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n# Create underserved variable\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(underserved = ifelse(as.numeric(distance_to_nearst_hospital) &gt; 15,\"Underserved\",\"\"))\n\nvulnerable_tracts %&gt;%\n  group_by(underserved)%&gt;%\n  summarise(n())\n\nQuestions to answer:\n\nHow many tracts are under-served?\n\n33\n\nWhat percentage of vulnerable tracts are under-served?\n\n3%\n\nDoes this surprise you? Why or why not?\n\nUnfortunately, this does not surprise me, given how rural certain areas of Pennsylvania are and knowing how rural healthcare is a persistent issue in the US in general. However, it would surprise me if some of these tracts are in urban areas.\n\n\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\n# Spatial join tracts to counties\npa_counties &lt;- pa_counties %&gt;% st_transform(st_crs(vulnerable_tracts))\n\nvulnerable_per_county &lt;- vulnerable_tracts %&gt;%\n  st_join(pa_counties) %&gt;%\n  st_drop_geometry()\n\n# Aggregate statistics by county\nvulnerable_per_county_stats &lt;- vulnerable_per_county %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarise(\n    num_vulnerable_tracts = n(),\n    num_underserved_tracts = sum(underserved == \"Underserved\"),\n    percent_underserved = sum(underserved == \"Underserved\")/n(),\n    avg_distance_to_nearst_hospital = mean(distance_to_nearst_hospital),\n    total_pop = sum(total_popE),\n    total_underserved_pop = sum(ifelse(underserved == \"Underserved\", total_popE, 0))\n  )\n\nRequired county-level statistics:\n\nNumber of vulnerable tracts\nNumber of underserved tracts\n\nPercentage of vulnerable tracts that are underserved\nAverage distance to nearest hospital for vulnerable tracts\nTotal vulnerable population\n\nQuestions to answer:\n\nWhich 5 counties have the highest percentage of underserved vulnerable tracts?\n\nCameron\nJuniata\nPotter\nSnyder\nSullivan\n\nWhich counties have the most vulnerable people living far from hospitals?\n\nClearfield\nChester\nJuniata\nSnyder\nPike\n\nAre there any patterns in where underserved counties are located?\n\nThere are a lot of quite rural underserved counties, like Sullivan, Juniata, and Clearfield, but there are also suburban counties of major PA cities like Chester County outside of Philadelphia and Cumberland County outside Harrisburg.\n\n\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n# Create and format priority counties table\nvulnerable_per_county_stats %&gt;% \n  arrange(desc(total_underserved_pop)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  select(\n    c(\"COUNTY_NAM\", \"total_underserved_pop\", \"percent_underserved\", \"avg_distance_to_nearst_hospital\")\n  ) %&gt;%\n  mutate(\n    avg_distance_to_nearst_hospital = as.numeric(avg_distance_to_nearst_hospital),\n    percent_underserved = paste0(round(percent_underserved*100, 2), \"%\")\n  )%&gt;%\n  kable(\n    col.names = c(\"County\", \"Total Underserved Population\", \"Percent of Vulnerable Tracts that are Underserved\", \"Average Distance of to Nearest Hospital\"),\n    digit = 1,\n    format.args = list(big.mark = \",\"),\n    align = \"l\",\n    caption = \"10 Counties in PA with the Highest Absolute Population more than 15 miles from Nearest Hospital\"\n  )\n\n\n10 Counties in PA with the Highest Absolute Population more than 15 miles from Nearest Hospital\n\n\n\n\n\n\n\n\nCounty\nTotal Underserved Population\nPercent of Vulnerable Tracts that are Underserved\nAverage Distance of to Nearest Hospital\n\n\n\n\nCLEARFIELD\n17,027\n28.57%\n12.4\n\n\nCHESTER\n15,467\n7.5%\n6.1\n\n\nJUNIATA\n13,955\n50%\n15.4\n\n\nSNYDER\n12,073\n50%\n15.1\n\n\nPIKE\n10,292\n35.29%\n15.4\n\n\nDAUPHIN\n8,815\n8.33%\n5.3\n\n\nSCHUYLKILL\n8,815\n7.41%\n7.0\n\n\nPERRY\n8,761\n22.22%\n12.3\n\n\nLANCASTER\n8,055\n5.71%\n5.7\n\n\nCENTRE\n6,843\n5.88%\n6.2\n\n\n\n\n\nRequirements:\n\nUse knitr::kable() or similar for formatting\nInclude descriptive column names\nFormat numbers appropriately (commas for population, percentages, etc.)\nAdd an informative caption\nSort by priority (you decide the metric)"
  },
  {
    "objectID": "labs/lab2/Knox_Katie_Assignment2.html#part-2-comprehensive-visualization",
    "href": "labs/lab2/Knox_Katie_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\n# Create county-level access map\nvulnerable_per_county_stats &lt;- vulnerable_per_county_stats %&gt;%\n  left_join(pa_counties, by=\"COUNTY_NAM\")\n\nvulnerable_per_county_stats &lt;- vulnerable_per_county_stats %&gt;%\n  st_as_sf() %&gt;% \n  st_transform(st_crs(census_tracts)) %&gt;%\n  mutate(percent_underserved = percent_underserved*100\n  )\n\nhospitals &lt;- hospitals %&gt;%\n  mutate(type = \"Hospital\")\n\nggplot(vulnerable_per_county_stats) +\n  geom_sf(aes(fill = percent_underserved)) +\n  scale_fill_gradient(\n    low = \"#C3CDFE\",   \n    high = \"#485EFE\",   \n    name = \"% Underserved Tracts\"\n  ) +\n  new_scale_fill()+\n  geom_sf(\n    data = hospitals, \n    aes(fill=type),      \n    size = 3, \n    shape = 21, \n    color = \"#FF8600\",     \n    stroke = 0.8\n  ) +\n  scale_fill_manual(\n    values = c(\"Hospital\" = \"#FF8600\"),\n    name = NULL\n  )+\n  labs(\n    title = \"Underserved Tracts per Counties in Pennsylvania\",\n    subtitle = str_wrap(\"Where an underserved tract is one where the center is at least 15 miles from the nearest hospital\",50),\n    caption =\"Data Sources: US Census Data, OpenDataPhilly\"\n  ) +\n  annotation_north_arrow(             \n    location = \"tr\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    legend.direction = \"vertical\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\nRequirements:\n\nFill counties by percentage of vulnerable tracts that are underserved\nInclude hospital locations as points\nUse an appropriate color scheme\nInclude clear title, subtitle, and caption\nUse theme_void() or similar clean theme\nAdd a legend with formatted labels\n\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\n# Create detailed tract-level map\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  st_as_sf() %&gt;% \n  st_transform(st_crs(census_tracts))\n\nunderserved_tracts &lt;- vulnerable_tracts[vulnerable_tracts$underserved==\"Underserved\",]\n\npa_counties$legend &lt;- \"County Boundary\"\n\nggplot(census_tracts) +\n  geom_sf(\n    color=\"darkgrey\",\n    alpha=.5,\n    size=.5\n  )+\n  geom_sf(\n    data=underserved_tracts,\n    aes(fill = underserved)\n  )+\n  geom_sf(\n    data=pa_counties,\n    aes(color=legend),     \n    linewidth = .8,\n    fill=NA\n  )+\n  geom_sf(\n    data=hospitals,\n    aes(fill = type),      \n    size = 1, \n    shape = 21, \n    color = \"#FF8600\",     \n    stroke = 0.8\n  )+\n  scale_fill_manual(\n    values = c(\"Underserved\" = \"#485EFE\",\n               \"Hospital\" = \"#FF8600\"),\n    name = NULL  \n  ) +\n  scale_color_manual(\n    values = c(\"County Boundary\" = \"black\"),\n    name = NULL\n  ) +\n  annotation_north_arrow(             \n    location = \"tr\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void()+\n  labs(\n    title = \"Underserved Tracts in Pennsylvania\",\n    subtitle = str_wrap(\"Where an underserved tract's center is at least 15 miles from the nearest hospital\",50),\n    caption =\"Data Sources: US Census Data, Pennsylvania Spatial Data Access\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\nRequirements:\n\nShow underserved vulnerable tracts in a contrasting color\nInclude county boundaries for context\nShow hospital locations\nUse appropriate visual hierarchy (what should stand out?)\nInclude informative title and subtitle\n\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create distribution visualization\n\n#Load regions\ndep_regions&lt;- st_read(\"data/DEPRegions2024_03.shp\")\n\n\ndep_regions &lt;- dep_regions %&gt;% st_transform(st_crs(vulnerable_tracts))\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;% \n  mutate(\n      urban_rural = case_when(\n      total_popE &gt;= 5000 ~ \"Urban\",\n      TRUE ~ \"Rural\"\n      )\n  )\n\nvulnerable_tracts_with_regions &lt;- vulnerable_tracts %&gt;%\n  st_join(dep_regions) %&gt;%\n  st_drop_geometry()\n\nvulnerable_tracts_with_regions &lt;- vulnerable_tracts_with_regions %&gt;%\n  mutate(\n    Region = case_when(\n      SNAME == \"NCRO\"~\"North Central\",\n      SNAME ==\"NERO\"~\"North East\",\n      SNAME==\"NWRO\"~\"North West\",\n      SNAME==\"SCRO\"~\"South Central\",\n      SNAME==\"SERO\"~\"South East\",\n      SNAME==\"SWRO\"~\"South West\"\n    )\n  )\n  \n  \nggplot(vulnerable_tracts_with_regions)+\n  geom_boxplot(\n    aes(x=Region, y=distance_to_nearst_hospital)\n  )+\n  labs(\n    title=\"Distance of Vulnerable Tracts to Nearest Hospital by Region\",\n    caption=str_wrap(\"Where a vulnerable tract is in the top 25% of concentration of elderly population or 215% of the Federal Poverty Line for 2-person household. Data sources: US Census, Pennsylvania Spatial Data Access, Department of Environmental Protection Regions\",100),\n    y=str_wrap(\"Distance from Center of Tract to Nearest Hospital\",50),\n    x=\"Region of Tract\"\n  )+\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\nNorthern Pennsylvania vulnerable populations generally are location farther from hospitals than Southern Pennsylvania, with North Central PA having the highest median and upper quartiles of distance to the nearest hospital of all 6 regions. However another trend we can see is that the highest number of outliers fall within the South East and South West regions, home to the two biggest cities in PA, Philadelphia and Pittsburgh, respectively. That shows that these regions have more tracts that are facing very different needs that the middle 50% of the region.\nSuggested chart types:\n\nHistogram or density plot of distances\nBox plot comparing distances across regions\nBar chart of underserved tracts by county\nScatter plot of distance vs. vulnerable population size\n\nRequirements:\n\nClear axes labels with units\nAppropriate title\nProfessional formatting\nBrief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "labs/lab2/Knox_Katie_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "labs/lab2/Knox_Katie_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\n\n\n\nYour Analysis\n\nDigital Justice\n\nData: Census Broadband access, device access, and income, Philadelphia free wifi spots from OpenDataPhilly\nQuestion: “Do digitally disadvantaged neighborhoods have equitable access to city internet?”\nOperations: Buffer free wifi spots with a computer (10-minute walk = 0.5 mile), calculate connectivity by tract, determine digitally vulnerable tracts from census data and underserved tracts by distance from buffers.\nPolicy relevance: Digital equity, broadband infrastructure, internet-connected device access\n\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load your additional dataset\nv21 &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n\n# Search for broadband and computer-related variables\nbb_vars &lt;- v21 %&gt;% filter(str_detect(label, \"broadband\"))\ncomp_vars &lt;-v21 %&gt;% filter(str_detect(label, \"computer\"))\n\n#B28003_002 = total with computer\n#B28002_004 = total with broadband internet subscription\n\nphilly_digital_access &lt;-  get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    total_hh = \"B28001_001\",\n    has_computer = \"B28003_002\", \n    smartphone_only = \"B28001_006\",\n    has_broadband = \"B28002_004\"\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2023,\n  output = \"wide\"\n)\n\nfree_wifi_spots &lt;- st_read(\"data/free_city_wifi_locations.shp\")%&gt;%st_transform(2272)\n\nphilly_digital_access &lt;- left_join(\n  philly_digital_access,\n  census_tracts,\n  by=\"GEOID\"\n) %&gt;%\n  st_as_sf() %&gt;%\n  st_transform(2272)\n\nggplot(philly_digital_access)+\n  geom_sf()+\n  geom_sf(\n    data=free_wifi_spots\n  )+\n  theme_void()+\n  labs(\n    title = \"Free Wi-Fi Spots in Philadelphia\",\n    caption =\"Data Sources: US Census Data, OpenDataPhilly\"\n  ) +\n  annotation_north_arrow(             \n    location = \"br\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    legend.direction = \"vertical\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\nQuestions to answer:\n\nWhat dataset did you choose and why?\n\nI choose the ACS variables showing whether a household has broadband access and whether they have a computer, as well as OpenDataPhilly’s database of free wi-fi locations, which inlcudes a variable showing how many public use computers are available at the location.\n\nWhat is the data source and date?\n\nThe ACS data is 5-year data from 2018 - 2023, and the wi-fi locations are from 2024, so the home broadband and computer access data may be slightly outdated. Also important to note is 2018 - 2023 range includes the pandemic, a period in which there was unprecedented investment in home internet subsidies that have expired in 2024 Affordable Connectivity Program.\n\nHow many features does it contain?\n\nThere are 254 free-wifi locations, and 408 census tracts in Philadelphia.\n\nWhat CRS is it in? Did you need to transform it?\n\nBefore transformation, both datasets were in web mercator, EPSG 3857, and since Philly falls within southern pennsylvania, I transformed it to Pennsylvania South state plane projection, EPSG 2272.\n\n\n\n\nPose a research question\n\nDo digitally vulnerable tracts have adequate access to free wifi spots with computers?\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+):\n\nBuffers\nSpatial joins\nSpatial filtering with predicates\nDistance calculations\nIntersections or unions\nPoint-in-polygon aggregation\n\nYour Task:\n\n# Your spatial analysis\n\n#Filter Philly census tracts by the most digitally vulnerable: in the top quartile of percent of tract that has no home broadband or top quartile of percent of tract with no computer. \nphilly_digital_access &lt;- philly_digital_access %&gt;%\n  mutate(\n    percent_no_bb = (1-has_broadbandE/total_hhE)*100,\n    percent_smart_only = (smartphone_onlyE/total_hhE)*100\n  )\n\nsummary(philly_digital_access$percent_no_bb)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   5.499  10.956  12.334  17.514  37.186      20 \n\nsummary(philly_digital_access$percent_smart_only)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   4.570   9.456  10.801  15.765  38.915      20 \n\n\n\n#based on summary stats, the most digitally vulnerable are tract with 17.514% or more of households that do not have home broadband or 15.765% or more of households that do not have a smartphone only.\n\ndigit_vulnerable_philly &lt;- philly_digital_access %&gt;%\n  filter(percent_no_bb&gt;=17.514 | percent_smart_only&gt;=15.765 )%&gt;%\n  mutate(Vulnerable=\"Digtally Vulnerable Tract\")\n\n#visually examine digitally vulnerable tracts and free wifi spots with computers\nggplot(philly_digital_access)+\n  geom_sf()+\n  geom_sf(\n    data=digit_vulnerable_philly,\n    aes(fill=Vulnerable)\n  )+\n  scale_fill_manual(\n    values=c(\"Digtally Vulnerable Tract\" = \"purple\")\n  )+\n  geom_sf(data=free_wifi_spots[free_wifi_spots$computers_==\"Y\",])+\n  theme_void()+\n  labs(\n    title = str_wrap(\"Digitally Vulnerable Tracts and Free Wifi Spots with a Computer in Philadelphia\",70),\n    subtitle = str_wrap(\"Where a vulnerable tract is one where 17.514% or more of households do not have home broadband or 15.765% or more of households do not have a computer\",60),\n    caption =\"Data Sources: US Census Data, OpenDataPhilly\"\n  ) +\n  annotation_north_arrow(             \n    location = \"br\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    legend.direction = \"vertical\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\n\n# Find nearest free wifi spots with a computer to tract centroids \ndigit_vulnerable_centroids &lt;- st_centroid(digit_vulnerable_philly)\nnearest_wifi_spot_comp &lt;- st_nearest_feature(digit_vulnerable_centroids, free_wifi_spots[free_wifi_spots$computers_==\"Y\",])\n\ndigit_vulnerable_philly &lt;- digit_vulnerable_philly %&gt;%\n  mutate(nearest_wifi_spot_geom = free_wifi_spots$geometry[nearest_wifi_spot_comp])\n\ndigit_vulnerable_philly &lt;- digit_vulnerable_philly %&gt;%\n  mutate(distance_wifi_spot_comp = set_units(st_distance(digit_vulnerable_centroids, digit_vulnerable_philly$nearest_wifi_spot_geom, by_element = TRUE), \"mi\"))\n\nsummary(digit_vulnerable_philly$distance_wifi_spot_comp) \n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 0.05125  2.82746  5.39764  5.61460  7.63815 15.05424 \n\n\n\ndigit_vulnerable_philly%&gt;%filter(\n  total_popE.x&gt;400\n)%&gt;%summary()\n\n    GEOID               NAME            total_popE.x   total_popM.x   \n Length:139         Length:139         Min.   : 892   Min.   : 267.0  \n Class :character   Class :character   1st Qu.:3182   1st Qu.: 624.5  \n Mode  :character   Mode  :character   Median :4128   Median : 822.0  \n                                       Mean   :4352   Mean   : 875.7  \n                                       3rd Qu.:5264   3rd Qu.:1073.5  \n                                       Max.   :8425   Max.   :1943.0  \n                                                                      \n   total_hhE      total_hhM     has_computerE  has_computerM   smartphone_onlyE\n Min.   : 514   Min.   : 51.0   Min.   : 432   Min.   : 60.0   Min.   : 25.0   \n 1st Qu.:1242   1st Qu.:201.0   1st Qu.:1126   1st Qu.:203.5   1st Qu.:191.5   \n Median :1686   Median :241.0   Median :1526   Median :256.0   Median :291.0   \n Mean   :1740   Mean   :278.4   Mean   :1553   Mean   :282.3   Mean   :312.9   \n 3rd Qu.:2153   3rd Qu.:323.5   3rd Qu.:1926   3rd Qu.:337.0   3rd Qu.:399.5   \n Max.   :3713   Max.   :738.0   Max.   :3476   Max.   :785.0   Max.   :981.0   \n                                                                               \n smartphone_onlyM has_broadbandE   has_broadbandM    STATEFP         \n Min.   : 30.0    Min.   : 390.0   Min.   : 61.0   Length:139        \n 1st Qu.:117.0    1st Qu.: 979.5   1st Qu.:205.0   Class :character  \n Median :166.0    Median :1289.0   Median :253.0   Mode  :character  \n Mean   :178.8    Mean   :1388.0   Mean   :273.7                     \n 3rd Qu.:217.5    3rd Qu.:1778.5   3rd Qu.:320.5                     \n Max.   :614.0    Max.   :3234.0   Max.   :793.0                     \n                                                                     \n   COUNTYFP           TRACTCE            GEOIDFQ             NAME.x         \n Length:139         Length:139         Length:139         Length:139        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   NAMELSAD            STUSPS           NAMELSADCO         STATE_NAME       \n Length:139         Length:139         Length:139         Length:139        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n     LSAD               ALAND             AWATER          NAME.y         \n Length:139         Min.   : 190517   Min.   :     0   Length:139        \n Class :character   1st Qu.: 403434   1st Qu.:     0   Class :character  \n Mode  :character   Median : 555597   Median :     0   Mode  :character  \n                    Mean   : 637299   Mean   :  2039                     \n                    3rd Qu.: 763572   3rd Qu.:     0                     \n                    Max.   :2244712   Max.   :133239                     \n                                                                         \n median_incomeE   median_incomeM   total_popE.y   total_popM.y   \n Min.   : 13721   Min.   : 2116   Min.   : 892   Min.   : 267.0  \n 1st Qu.: 32050   1st Qu.: 9380   1st Qu.:3182   1st Qu.: 624.5  \n Median : 41313   Median :13530   Median :4128   Median : 822.0  \n Mean   : 42388   Mean   :15511   Mean   :4352   Mean   : 875.7  \n 3rd Qu.: 50068   3rd Qu.:20019   3rd Qu.:5264   3rd Qu.:1073.5  \n Max.   :106420   Max.   :41416   Max.   :8425   Max.   :1943.0  \n NA's   :5        NA's   :5                                      \n    over_65E         over_65M               geometry   percent_no_bb  \n Min.   :  0.00   Min.   :  2.00   MULTIPOLYGON :139   Min.   : 1.00  \n 1st Qu.:  8.00   1st Qu.: 16.00   epsg:2272    :  0   1st Qu.:14.91  \n Median : 22.00   Median : 29.00   +proj=lcc ...:  0   Median :20.23  \n Mean   : 35.91   Mean   : 43.04                       Mean   :20.17  \n 3rd Qu.: 58.50   3rd Qu.: 58.00                       3rd Qu.:26.27  \n Max.   :197.00   Max.   :210.00                       Max.   :37.19  \n                                                                      \n percent_smart_only  Vulnerable          nearest_wifi_spot_geom\n Min.   : 1.508     Length:139         POINT        :139       \n 1st Qu.:13.633     Class :character   epsg:2272    :  0       \n Median :17.844     Mode  :character   +proj=lcc ...:  0       \n Mean   :18.398                                                \n 3rd Qu.:22.327                                                \n Max.   :38.915                                                \n                                                               \n distance_wifi_spot_comp\n Min.   : 0.05125       \n 1st Qu.: 2.82746       \n Median : 5.39764       \n Mean   : 5.61460       \n 3rd Qu.: 7.63815       \n Max.   :15.05424       \n                        \n\n\n\n#Find walking distance buffers around free wifi spots with computers \nfree_wifi_buffers &lt;- free_wifi_spots %&gt;%\n  filter(computers_==\"Y\" & to_display==\"ACTIVE\")%&gt;%\n  st_buffer(dist = 2640)  # 2640 ft = .5 mi\n\n\n#dissolve overlapping buffers \nfree_wifi_buffers_dissolve &lt;- free_wifi_buffers%&gt;%\n  st_union() %&gt;%\n  st_cast(\"POLYGON\")%&gt;%\n  st_as_sf()%&gt;%\n    mutate(\n    legend= \".5 mi from Free Wifi Spot w Computer\"\n  )\n\n  \n#examine buffers and tracts overlap\nggplot(philly_digital_access)+\n  geom_sf(\n    color=\"darkgray\",\n    linewidth=.5,\n    alpha=.5\n  )+\n  geom_sf(\n    data=digit_vulnerable_philly,\n    aes(fill=percent_no_bb+percent_smart_only)\n  )+\n  scale_fill_gradient(\n    low=\"#FFCF33\",\n    high=\"#F53D00\",\n    name=str_wrap(\"Digital Vulnerability (higher number is more vulnerable)\",30)\n  )+\n  new_scale_fill()+\n  geom_sf( \n    data=free_wifi_buffers_dissolve,\n    aes(fill=legend),\n    color=\"black\",\n    linewidth=.8,\n    alpha=0.4\n  )+\n  scale_color_manual(\n    values = c(\".5 mi from Free Wifi Spot w Computer\" =  \"#F5A6E6\") ,\n    name=NULL\n  )+\n  theme_void()+\n  labs(\n    title = str_wrap(\"Degree of Digital Vulnerability and Walking Distance to Public WiFi & Computer\",50),\n    caption =\"Data Sources: US Census Data, OpenDataPhilly\"\n  ) +\n  annotation_north_arrow(             \n    location = \"br\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void() +\n  guides(fill = guide_legend(title = NULL))+\n  theme(\n    legend.position = \"right\",\n    legend.direction = \"vertical\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\n\n#Define underserved tract as a tract that is digitally vulnerable and outside walking distance to a free wifi spot with a computer. \n\noverlap &lt;- st_intersects(digit_vulnerable_philly, free_wifi_buffers)\n\nno_overlap &lt;- lengths(overlap) == 0\n\ndigit_underserved &lt;- digit_vulnerable_philly[no_overlap, ]\n\nsummary(digit_underserved)\n\n\n#what are characteristics of top 10 digitally underserved tracts by absolute number of those with no broadband + those with no computer? \n\n#note, the sum of estiamated households without computer and without broadband is not an accurate estimate of reality because those without one are likely overlapping with the other. This metric is just being used to create a metric showing reflecting broadband and computer need in one. \ndigit_underserved&lt;-digit_underserved%&gt;%\n  mutate(\n    digit_need_heuristic = (total_hhE - smartphone_onlyE)+(total_hhE - has_broadbandE)\n  )\n\ntop_10_digit_underserved &lt;- digit_underserved%&gt;%\n  arrange(desc(digit_need_heuristic))%&gt;%\n  slice_head(n = 10)\n \ntop_10_digit_underserved %&gt;%\n  st_drop_geometry()%&gt;%\n  select(c(\n    \"NAMELSAD\",\n    \"percent_no_bb\",\n    \"percent_smart_only\",\n    \"median_incomeE\",\n    \"distance_wifi_spot_comp\"\n  )) %&gt;%\n  kable(\n    col.names = c(\"Census Tract\",\n                  \"Percent w/o Broadband\",\n                  \"Percent w/o Smartphone Only\", \n                  \"Median Income\", \n                  \"Distance to Free Wi-Fi spot w/ Computer\"),\n    digits = 1,\n    format.args = list(big.mark = \",\"),\n    align = \"l\",\n    caption = \"10 Census Tracts in Philadelphia that have Highest Digitally Vulnerable Population\"\n  )\n\n\n10 Census Tracts in Philadelphia that have Highest Digitally Vulnerable Population\n\n\n\n\n\n\n\n\n\nCensus Tract\nPercent w/o Broadband\nPercent w/o Smartphone Only\nMedian Income\nDistance to Free Wi-Fi spot w/ Computer\n\n\n\n\nCensus Tract 306\n21.3\n7.5\n44,108\n5.934038 [mi]\n\n\nCensus Tract 300\n18.2\n14.5\n33,419\n5.459912 [mi]\n\n\nCensus Tract 192\n19.0\n14.6\n13,721\n7.369100 [mi]\n\n\nCensus Tract 178\n12.7\n15.8\n42,104\n2.672626 [mi]\n\n\nCensus Tract 282\n24.6\n8.6\n34,420\n7.595321 [mi]\n\n\nCensus Tract 273\n24.6\n17.9\n42,423\n3.069729 [mi]\n\n\nCensus Tract 336\n5.4\n16.6\n46,191\n5.278315 [mi]\n\n\nCensus Tract 268\n20.9\n22.9\n41,330\n5.397637 [mi]\n\n\nCensus Tract 122.01\n18.8\n19.5\n42,782\n4.911781 [mi]\n\n\nCensus Tract 278\n27.3\n37.4\n33,655\n8.069892 [mi]\n\n\n\n\n\n\n#Map the underserved tracts and wifi spots buffers \ndigit_underserved&lt;-digit_underserved%&gt;%\n  mutate(\n    legend=\"Digitally Underserved Tract\"\n  )\n\nggplot(philly_digital_access)+\n  geom_sf(\n    color=\"darkgray\",\n    linewidth=.5,\n    alpha=.5\n  )+\n  geom_sf(\n    data=digit_underserved,\n    aes(fill=\"Digitally Underserved Tract\")\n  )+\n  scale_fill_manual(\n    values = c(\"Digitally Underserved Tract\" = \"#3F88C5\"),\n    name=NULL\n  )+\n  new_scale_fill()+\n  geom_sf( \n    data=free_wifi_buffers_dissolve,\n    aes(fill=\".5 mi from Free Wifi Spot w Computer\"),\n    color=\"black\",\n    linewidth=.8,\n    alpha=0.4\n  )+\n  scale_color_manual(\n    values = c(\".5 mi from Free Wifi Spot w Computer\" =  \"#F5A6E6\") \n  )+\n  guides(fill = guide_legend(title = NULL))+\n  theme_void()+\n    labs(\n    title = str_wrap(\"Digitally Underserved Tracts in Philadelphia \",40),\n    caption =\"Data Sources: US Census Data, OpenDataPhilly\"\n  ) +\n  annotation_north_arrow(             \n    location = \"br\",                  \n    which_north = \"true\",\n    style = north_arrow_minimal\n  ) +\n  theme_void() +\n  guides(fill = guide_legend(title = NULL))+\n  theme(\n    legend.position = \"right\",\n    legend.direction = \"vertical\",\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5, size = 12),\n    plot.caption = element_text(hjust = 0.5, size = 9, margin = margin(t = 10))\n  )\n\n\n\n\n\n\n\n\nInterpretation:\nWhile the Free Wi-Fi Spots in Philadelphia are a very promising program to help bridge the gap for those without internet and a computer at home, a lot of the centers with computers are outside walking distance for the most digitally vulnerable in Philadelphia. There are 30 total tracts in Philly that completely fall outside walking distance to a Wi-Fi computer center, mostly concentrated in North Philadelphia. The estimated number of households in these 30 tracts without broadband is 103,418 and without a computer is 100,478. The average distance from these underserved tracts to the nearest free Wi-Fi and computer center is 7 miles.\nThe internet has become indispensable to most educational, workforce, social, and civic needs, and thus policy should primarily aim to enable every person to have home broadband and a computer. However, while that may be a long-term goal, free Wi-Fi and computer centers help bridge the gap. Thus they should prioritize locations as close as possible to the most digitally vulnerable areas of Philadelphia, seeing as a longer commute to these centers will compound the barriers already faced by not having broadband or a computer at home."
  },
  {
    "objectID": "labs/lab2/Knox_Katie_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "labs/lab2/Knox_Katie_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nTaking feedback into account, I have hidden sensitive code blocks and hidden lengthy console output that is not essential to interpreting my work!"
  },
  {
    "objectID": "labs/lab2/Knox_Katie_Assignment2.html#submission-requirements",
    "href": "labs/lab2/Knox_Katie_Assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "I have chosen to look at “All street lights out” violations, because it is more cozy to commit crimes when the lights are out.\n\nchi_boundary&lt;-chi_boundary%&gt;%st_transform(st_crs(all_lights_out))\n\nchi_boundary&lt;-chi_boundary%&gt;%st_transform(26971)\nall_lights_out&lt;-all_lights_out%&gt;%st_transform(26971)\n\nall_lights_out%&gt;%\n  ggplot()+\n  geom_sf(alpha = 0.1, color = \"lightblue\")+\n  geom_sf(data = chi_boundary, fill = NA, color = \"darkgreen\")+\n  theme_void()\n\n\n\n\n\n\n\n\nThe 311 complaints of all street lights out are seemingly very ubiquitous across Chicago! Every part of the city is very visual dense with instances. The only empty areas are where the airport is located, and where there are bodies of water and parks."
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-1-collect-311-violation-data",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-1-collect-311-violation-data",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "I have chosen to look at “All street lights out” violations, because it is more cozy to commit crimes when the lights are out.\n\nchi_boundary&lt;-chi_boundary%&gt;%st_transform(st_crs(all_lights_out))\n\nchi_boundary&lt;-chi_boundary%&gt;%st_transform(26971)\nall_lights_out&lt;-all_lights_out%&gt;%st_transform(26971)\n\nall_lights_out%&gt;%\n  ggplot()+\n  geom_sf(alpha = 0.1, color = \"lightblue\")+\n  geom_sf(data = chi_boundary, fill = NA, color = \"darkgreen\")+\n  theme_void()\n\n\n\n\n\n\n\n\nThe 311 complaints of all street lights out are seemingly very ubiquitous across Chicago! Every part of the city is very visual dense with instances. The only empty areas are where the airport is located, and where there are bodies of water and parks."
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-2-fishnet-grid",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-2-fishnet-grid",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 2: Fishnet Grid",
    "text": "Part 2: Fishnet Grid\n\n# Step 1: Define cell size (in map units - meters for this projection)\ncell_size &lt;- 500  # 500m x 500m cells\n\n# Step 2: Create grid over study area\nfishnet &lt;- st_make_grid(\n  chi_boundary,\n  cellsize = cell_size,\n  square = TRUE,\n  what = \"polygons\"\n) %&gt;%\n  st_sf() %&gt;%\n  mutate(uniqueID = row_number())\n\n# Step 3: Clip to study area (remove cells outside boundary)\nfishnet &lt;- fishnet[chi_boundary, ]\n\n# Check results\nnrow(fishnet)  # Number of cells\n\n[1] 2640\n\nst_area(fishnet[1, ])  # Area of one cell (should be 250,000 m²)\n\n250000 [m^2]\n\nfishnet %&gt;%\n  ggplot()+\n  geom_sf()+\n  geom_sf(data = chi_boundary, fill=NA, color=\"darkgreen\")+\n  theme_void()\n\n\n\n\n\n\n\n\n\n# Count lights out per cell\nlights_out_count &lt;- st_join(all_lights_out, fishnet) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(count = n())\n\n# Join back to fishnet\nfishnet &lt;- fishnet %&gt;%\n  left_join(lights_out_count, by = \"uniqueID\") %&gt;%\n  mutate(count_lights_out = replace_na(count, 0))\n\n# Summary\nsummary(fishnet$count_lights_out)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0    24.0    95.5   112.3   167.0   673.0"
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-3-spatial-features",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-3-spatial-features",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 3: Spatial Features",
    "text": "Part 3: Spatial Features\nFind “Nearest Neighbor”\n\n# Calculate distance to nearest complaint of all street lights out\nnn_dist &lt;- get.knnx(\n  data = st_coordinates(all_lights_out),      # \"To\" locations\n  query = st_coordinates(st_centroid(fishnet)), # \"From\" locations\n  k = 1                                          # Nearest 1\n)\n\n# Extract distances\nfishnet$lights_out_dists &lt;- nn_dist$nn.dist[, 1]\nsummary(fishnet$lights_out_dists)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n   0.9545   33.3966   54.0120  194.0979  124.7172 4603.3141 \n\n\nThe median distance from a random 500x500 area in Chicago to a reported incident of all street lights out is 54 meters, and the 3rd quantile is 124 meters. This means the majority of ares in Chicago are very near one such complaint (and likely more than one given the distribution of incident counts per grid cell from the previous step). The mean is higher that the 3rd quantile, showing that max distance is an extreme outlier, likely in one of the park or airport areas where there probably aren’t streets in a park let alone street lights.\nFind Local Moran’s I\n\n# Step 1: Create spatial object\nfishnet_sp &lt;- as_Spatial(fishnet)\n\n# Step 2: Define neighbors (Queen contiguity)\nneighbors &lt;- poly2nb(fishnet_sp, queen = TRUE)\n\n# Step 3: Create spatial weights (row-standardized)\nweights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n\n# Step 4: Calculate Local Moran's I\nlocal_moran &lt;- localmoran(\n  fishnet$count_lights_out,  # Variable of interest\n  weights,                  # Spatial weights\n  zero.policy = TRUE       # Handle cells with no neighbors\n)\n\n# Step 5: Extract components\nfishnet$local_I &lt;- local_moran[, \"Ii\"]      # Local I statistic\nfishnet$p_value &lt;- local_moran[, \"Pr(z != E(Ii))\"]  # P-value\nfishnet$z_score &lt;- local_moran[, \"Z.Ii\"]    # Z-score\n\n\n# Standardize the variable for quadrant classification\nfishnet$standardized_value &lt;- scale(fishnet$count_lights_out)\n\n# Calculate spatial lag (weighted mean of neighbors)\nfishnet$spatial_lag &lt;- lag.listw(weights, fishnet$count_lights_out)\nfishnet$standardized_lag &lt;- scale(fishnet$spatial_lag)\n\n# Identify High-High clusters\nfishnet$hotspot &lt;- 0  # Default: not a hotspot\n\n# Criteria: \n# 1. Value above mean (standardized &gt; 0)\n# 2. Neighbors above mean (spatial lag &gt; 0)\n# 3. Statistically significant (p &lt; 0.05)\n\nfishnet$hotspot[\n  fishnet$standardized_value &gt; 0 & \n  fishnet$standardized_lag &gt; 0 & \n  fishnet$p_value &lt; 0.05\n] &lt;- 1\n\n# Count hotspots\nsum(fishnet$hotspot)\n\n[1] 418\n\n\nIn this step, first each cell’s count of incidents is standardized to measure how much it deviates from the mean, and a spatial lag is calculated to summarize the average counts in neighboring cells based on the intution that areas with high incident counts tend to be near other areas with high counts, so clustering in space can reveal meaningful hotspots of activity. Cells are classified as High-High clusters (hotspots) if they have above-average counts, are surrounded by neighbors with above-average counts, and are statistically significant based on Local Moran’s I (p &lt; 0.05). The fact that the sum of hotspots is 418 indicates that 418 grid cells are part of statistically significant clusters of high incident density.\n\nfishnet%&gt;%filter(hotspot==1)%&gt;%\n  ggplot()+\n  geom_sf()+\n  theme_void()+\n  geom_sf(data = chi_boundary, fill = NA, color = \"darkgreen\")\n\n\n\n\n\n\n\n\nLooking at a map of these hotspots reveals they are concentrated in areas west and south of the main center city core.\nNext, I find distance of each grid cell to hotspot.\n\n# Step 1: Identify hotspots \nhotspot_cells &lt;- filter(fishnet, hotspot == 1)\n\n# Step 2: Calculate distances\nhotspot_dist &lt;- get.knnx(\n  data = st_coordinates(st_centroid(hotspot_cells)),\n  query = st_coordinates(st_centroid(fishnet)),\n  k = 1\n)\n\nfishnet$hotspot_nn &lt;- hotspot_dist$nn.dist[, 1]\n\n\n# Create comparison maps\np1 &lt;- ggplot(fishnet) +\n  geom_sf(aes(fill = lights_out_dists), color = NA) +\n  scale_fill_viridis_c(name = \"Distance (m)\", option = \"plasma\",  direction = -1) +\n  labs(title = \"Distance to Nearest Complaint\\nof All Street Lights Out\") +\n  theme_void()\n\n\np2 &lt;- ggplot(fishnet) +\n  geom_sf(aes(fill = hotspot_nn), color = NA) +\n  scale_fill_viridis_c(name = \"Distance (m)\", option = \"plasma\",  direction = -1) +\n  labs(title = \"Distance to Nearest Hotspot\") +\n  theme_void()\n\np1 + p2 + plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\nThe distribution of distances shows that individual complaints are much more spatially heterogeneous than the hotspots. While the nearest-complaint distances vary widely across the city, the nearest-hotspot distances are more uniform. This indicates that 311 service requests are concentrated in clusters, and once these clusters are aggregated into hotspots, the spatial variation smooths out. In other words, individual incidents occur in scattered patterns, but the resulting hotspots capture the broader areas of high activity, providing a clearer picture of concentrated problem areas."
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-4",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-4",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 4",
    "text": "Part 4"
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-5",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-5",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 5",
    "text": "Part 5"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html",
    "href": "weekly-notes/week-09-notes.html",
    "title": "Week 9 Notes",
    "section": "",
    "text": "PREDPOL, Palantir, Hunch\nDirty Data\nMore likely to predict what police are already doing rather than preventing novel crimes\nA branch of “technological solutionism”\n\n\n\n\nRevisiting Spatial Lag - e.g. adding a term in housing prediction model that takes into account average neighboring house prices - Problem when using for prediction – you don’t have future neighbor prices… so problematic to include in model - if you use current neighbor sales… risk over emphasizing past prices - if you use predicted future neighbor sale values… problem of circularity\n\n\n\n\nSetup & Data\n\n(dirty) burglary data from police\n(dirty) 311 calls about abandoned cars\nCreate fishnet (500m x 500m grid)\n\na common unit of analysis\n\naggregate burglaries to cells\n\nBaseline Comparison\n\nKernel Density Estimation (KDE)\n\nDoes our model perform better than the KDE?\n\nSimple spatial smoothing\nWhat we need to beat\n\nFeature engineering\n\n\nk-Nearest Neighbors\n\nLISA (Local Moran’s I)\n\nCount Regression Models\nSpatial Cross-Validation\n\nSpecific CV to spatial applications\n\nModel Comparison\n\nCore Logic: “Broken Windows Theory” - i.e. visual signs of ‘disorder’ lead to criminal activity\nSpatial Weights Matrix - Global Moran’s I gives us a value from [-1, 1] - Positive is similar values cluster - Negative is dissimilar values cluster - 0 is no clustering - Local Moran’s I - Test for statistical significance - so every estimate gets a p-value - Moran Scatterplot - High-high = hotspot - High-low = outliers - Low-low = coldspot - Low-high = outliers\nPoisson Distribution - Appropriate for count data - Poisson assumption: Variance = mean\nPoisson Regression - Log terms - In R, use glm() instead of lm() - Test for overdispersion - Use glm.nb() if there is overdispersion\nSpatial Cross-Validation - Standard CV might fail for spatial data because nearby observations are correlated; Spatial leakage, or model learns from neighbors of test set - Instead group together and remove neighbors together in a fold\nKernel Density Estimation - create a kernel out of different densities of observations"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#notes",
    "href": "weekly-notes/week-09-notes.html#notes",
    "title": "Week 9 Notes",
    "section": "",
    "text": "PREDPOL, Palantir, Hunch\nDirty Data\nMore likely to predict what police are already doing rather than preventing novel crimes\nA branch of “technological solutionism”\n\n\n\n\nRevisiting Spatial Lag - e.g. adding a term in housing prediction model that takes into account average neighboring house prices - Problem when using for prediction – you don’t have future neighbor prices… so problematic to include in model - if you use current neighbor sales… risk over emphasizing past prices - if you use predicted future neighbor sale values… problem of circularity\n\n\n\n\nSetup & Data\n\n(dirty) burglary data from police\n(dirty) 311 calls about abandoned cars\nCreate fishnet (500m x 500m grid)\n\na common unit of analysis\n\naggregate burglaries to cells\n\nBaseline Comparison\n\nKernel Density Estimation (KDE)\n\nDoes our model perform better than the KDE?\n\nSimple spatial smoothing\nWhat we need to beat\n\nFeature engineering\n\n\nk-Nearest Neighbors\n\nLISA (Local Moran’s I)\n\nCount Regression Models\nSpatial Cross-Validation\n\nSpecific CV to spatial applications\n\nModel Comparison\n\nCore Logic: “Broken Windows Theory” - i.e. visual signs of ‘disorder’ lead to criminal activity\nSpatial Weights Matrix - Global Moran’s I gives us a value from [-1, 1] - Positive is similar values cluster - Negative is dissimilar values cluster - 0 is no clustering - Local Moran’s I - Test for statistical significance - so every estimate gets a p-value - Moran Scatterplot - High-high = hotspot - High-low = outliers - Low-low = coldspot - Low-high = outliers\nPoisson Distribution - Appropriate for count data - Poisson assumption: Variance = mean\nPoisson Regression - Log terms - In R, use glm() instead of lm() - Test for overdispersion - Use glm.nb() if there is overdispersion\nSpatial Cross-Validation - Standard CV might fail for spatial data because nearby observations are correlated; Spatial leakage, or model learns from neighbors of test set - Instead group together and remove neighbors together in a fold\nKernel Density Estimation - create a kernel out of different densities of observations"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html",
    "href": "weekly-notes/week-06-notes.html",
    "title": "Week 6 Notes",
    "section": "",
    "text": "transforming csv or other such data into SF st_as_af() - option: if your data has coordinates, then coords = c(\"Name of Long column\", \"name of lat column\") – and set the CRS appropriately (e.g. crs = 4326) based on the coordinate system, but then transform into something more appropriate like state plane.\nChange categorical variable (such as neighborhood name) as a numerical factor that can be used in model - mutate(name = as.factor(name)) - first category alphabetically is chosen as reference variable, and the coefficients for the rest are in comparison to that\n\n\n\nwhen the effect of one variable on the model depends on another variable (i.e. square footage matters more to price in a fancy neighborhood than nonfancy neighborhood)\ninstead of ind_var_1 + ind_var_2 , for two variables you think interact, use ind_var_1 * ind_var_2\n\n\n\n\n\ncaution interpreting coefficients with quadratic terms\n\n\n\n\n3 tricks: 1. Buffer Aggregation - draw a buffer around the house, and calculate features that fall within the buffer (e.g. number of crimes in .5 mi radius) 2. k-Nearest Neighbors (kNN) - What is the average distance to the k-nearest features (e.g. what is average distance from house to 3 nearest crimes, low = close, high = far) 3. Distance to Specific Points - Straight-line distance to important features (e.g. distance from house to city hall)\n\n\n\n\nsome variable which summarizes other potential factors (e.g. neighborhood might stand in for a number of more specific variables that are harder to find data for perhaps)\n\n\n\n\n\nwant to make sure for instance that a category exists in as many instances as you have folds (e.g. if there are only 3 examples in Rittenhouse, 10-fold might have issues)\nsolution:\n\ndrop sparse categories"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#notes",
    "href": "weekly-notes/week-06-notes.html#notes",
    "title": "Week 6 Notes",
    "section": "",
    "text": "transforming csv or other such data into SF st_as_af() - option: if your data has coordinates, then coords = c(\"Name of Long column\", \"name of lat column\") – and set the CRS appropriately (e.g. crs = 4326) based on the coordinate system, but then transform into something more appropriate like state plane.\nChange categorical variable (such as neighborhood name) as a numerical factor that can be used in model - mutate(name = as.factor(name)) - first category alphabetically is chosen as reference variable, and the coefficients for the rest are in comparison to that\n\n\n\nwhen the effect of one variable on the model depends on another variable (i.e. square footage matters more to price in a fancy neighborhood than nonfancy neighborhood)\ninstead of ind_var_1 + ind_var_2 , for two variables you think interact, use ind_var_1 * ind_var_2\n\n\n\n\n\ncaution interpreting coefficients with quadratic terms\n\n\n\n\n3 tricks: 1. Buffer Aggregation - draw a buffer around the house, and calculate features that fall within the buffer (e.g. number of crimes in .5 mi radius) 2. k-Nearest Neighbors (kNN) - What is the average distance to the k-nearest features (e.g. what is average distance from house to 3 nearest crimes, low = close, high = far) 3. Distance to Specific Points - Straight-line distance to important features (e.g. distance from house to city hall)\n\n\n\n\nsome variable which summarizes other potential factors (e.g. neighborhood might stand in for a number of more specific variables that are harder to find data for perhaps)\n\n\n\n\n\nwant to make sure for instance that a category exists in as many instances as you have folds (e.g. if there are only 3 examples in Rittenhouse, 10-fold might have issues)\nsolution:\n\ndrop sparse categories"
  },
  {
    "objectID": "labs/lab_4/Assignment4_Instructions.html",
    "href": "labs/lab_4/Assignment4_Instructions.html",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "In this lab, you will apply the spatial predictive modeling techniques demonstrated in the class exercise to a 311 service request type of your choice. You will build a complete spatial predictive model, document your process, and interpret your results.\n\n\nDue Date: November 17, 2025, 10:00AM\nDeliverable: One rendered document, posted to your portfolio website.\n\n\n\nBy completing this assignment, you will demonstrate your ability to:\n\nAdapt example code to analyze a new dataset\nBuild spatial features for predictive modeling\nApply count regression techniques to spatial data\nImplement spatial cross-validation\nInterpret and communicate model results\nCritically evaluate model performance\n\n\n\n\n\n\n\n\nVisit the Chicago 311 Service Requests dataset:\nhttps://data.cityofchicago.org/stories/s/311-Dataset-Changes-12-11-2018/d7nq-5g7t\nBrowse the available service request types (e.g., Graffiti Removal, Pothole Repair, Street Light Out, Sanitation Code Violations, etc.) and choose one violation type that interests you.\n\n\n\n\nChoose a different violation type than the abandoned cars we did in class.\n\n\n\n\n\n\nUsing the class exercise as your template, adapt the code to analyze your chosen 311 violation type.\n\n\nWork through all major sections of the analysis:\n\n\n\nLoad your 311 data and Chicago spatial boundaries\nCreate visualizations showing the spatial distribution of your violation type\nDescribe patterns you observe\n\n\n\n\n\nCreate a 500m x 500m fishnet grid\nAggregate your violations to grid cells\nVisualize the count distribution\n\n\n\n\n\nCalculate k-nearest neighbor features\nPerform Local Moran’s I analysis\nIdentify hot spots and cold spots\nCreate distance-to-hotspot measures\nJoin any additional contextual data if you are looking for more to do and really get into this (e.g., demographics, land use)\n\n\n\n\n\nFit Poisson regression\nFit Negative Binomial regression\nCompare model fit (AIC)\n\n\n\n\n\nImplement Leave-One-Group-Out cross-validation on 2017 data\nCalculate and report error metrics (MAE, RMSE)\n\n\n\n\n\nGenerate final predictions for both years\nCompare to KDE baseline\nMap prediction errors for 2018\nAssess model performance across time\n\n\n\n\n\nDownload 2018 crimes https://data.cityofchicago.org/Public-Safety/Crimes-2018/3i3m-jwuy/about_data\nBe sure to filter for BURGLARIES (FORCED ENTRY only to match the 2017 data)\nAggregate 2018 violations to the same fishnet grid\nUse your 2017 model to predict 2018 counts\nCalculate temporal validation metrics\nCompare spatial vs. temporal validation performance\n\n\n\n\n\n\n\n\n\nFor each major section, you must explain in your own words:\n\nWhat you are doing in that step\nWhy this step is important for the analysis\nWhat you found or learned from the results\n\nDo not simply copy text from the example or from your AI friend. Think about the purpose of each technique and articulate it in your own words.\n\n\n\n\n\n\n\nYour knitted HTML document should be professional and easy to read:\n\n\n\nRemove unnecessary code, comments, or debugging lines\nKeep only essential code chunks\nUse code-fold: show in your YAML header\n\n\n\n\n\nUse headers to organize sections\nInclude a table of contents\nAdd your name and date\n\n\n\n\n\nEnsure all text renders properly (no broken markdown)\nCheck that headers appear as headers (not plain text)\nUse proper markdown formatting\n\n\n\n\n\nAll plots should have titles and labels\nMaps should be readable\nUse consistent color schemes\n\n\n\n\n\nProofread for typos\nRemove any “Your answer here” placeholders\nMake sure all code runs without errors\nMake IT NEAT. WE GOTTA LOOK GOOD HERE.\n\n\n\n\n\n\n\nBefore you submit, verify that your document includes:\n\n\n\nIntroduction: Brief description of your chosen 311 violation type and why you selected it\nData Exploration: Maps and summary statistics of your violation data\nFishnet Grid: Successfully created and visualized grid with aggregated counts\nSpatial Features: At least 2-3 spatial features calculated (k-NN, distance measures, etc.)\nLocal Moran’s I: Spatial autocorrelation analysis with interpretation\nCount Regression: Both Poisson and Negative Binomial models with comparison\nSpatial Cross-Validation: LOGO CV implemented on 2017 data with reported metrics\nTemporal Validation: 2018 data aggregated and tested with 2017 model(OPTIONAL.)\nValidation Comparison: Comparison of spatial vs. temporal validation performance\nModel Comparison: Your model vs. KDE baseline for 2018 predictions\nError Analysis: Maps and discussion of where model performs well/poorly\nWritten Explanations: Your own words explaining each step\n\n\n\n\n\nDocument knits to HTML without errors\nCode is clean and well-organized\nAll visualizations display properly\nHeaders and formatting render correctly\nFile is named successfully posted to your website"
  },
  {
    "objectID": "labs/lab_4/Assignment4_Instructions.html#assignment-overview",
    "href": "labs/lab_4/Assignment4_Instructions.html#assignment-overview",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "In this lab, you will apply the spatial predictive modeling techniques demonstrated in the class exercise to a 311 service request type of your choice. You will build a complete spatial predictive model, document your process, and interpret your results.\n\n\nDue Date: November 17, 2025, 10:00AM\nDeliverable: One rendered document, posted to your portfolio website.\n\n\n\nBy completing this assignment, you will demonstrate your ability to:\n\nAdapt example code to analyze a new dataset\nBuild spatial features for predictive modeling\nApply count regression techniques to spatial data\nImplement spatial cross-validation\nInterpret and communicate model results\nCritically evaluate model performance"
  },
  {
    "objectID": "labs/lab_4/Assignment4_Instructions.html#step-1-choose-your-311-violation-type",
    "href": "labs/lab_4/Assignment4_Instructions.html#step-1-choose-your-311-violation-type",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "Visit the Chicago 311 Service Requests dataset:\nhttps://data.cityofchicago.org/stories/s/311-Dataset-Changes-12-11-2018/d7nq-5g7t\nBrowse the available service request types (e.g., Graffiti Removal, Pothole Repair, Street Light Out, Sanitation Code Violations, etc.) and choose one violation type that interests you.\n\n\n\n\nChoose a different violation type than the abandoned cars we did in class."
  },
  {
    "objectID": "labs/lab_4/Assignment4_Instructions.html#step-2-complete-the-analysis",
    "href": "labs/lab_4/Assignment4_Instructions.html#step-2-complete-the-analysis",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "Using the class exercise as your template, adapt the code to analyze your chosen 311 violation type.\n\n\nWork through all major sections of the analysis:\n\n\n\nLoad your 311 data and Chicago spatial boundaries\nCreate visualizations showing the spatial distribution of your violation type\nDescribe patterns you observe\n\n\n\n\n\nCreate a 500m x 500m fishnet grid\nAggregate your violations to grid cells\nVisualize the count distribution\n\n\n\n\n\nCalculate k-nearest neighbor features\nPerform Local Moran’s I analysis\nIdentify hot spots and cold spots\nCreate distance-to-hotspot measures\nJoin any additional contextual data if you are looking for more to do and really get into this (e.g., demographics, land use)\n\n\n\n\n\nFit Poisson regression\nFit Negative Binomial regression\nCompare model fit (AIC)\n\n\n\n\n\nImplement Leave-One-Group-Out cross-validation on 2017 data\nCalculate and report error metrics (MAE, RMSE)\n\n\n\n\n\nGenerate final predictions for both years\nCompare to KDE baseline\nMap prediction errors for 2018\nAssess model performance across time\n\n\n\n\n\nDownload 2018 crimes https://data.cityofchicago.org/Public-Safety/Crimes-2018/3i3m-jwuy/about_data\nBe sure to filter for BURGLARIES (FORCED ENTRY only to match the 2017 data)\nAggregate 2018 violations to the same fishnet grid\nUse your 2017 model to predict 2018 counts\nCalculate temporal validation metrics\nCompare spatial vs. temporal validation performance"
  },
  {
    "objectID": "labs/lab_4/Assignment4_Instructions.html#step-3-write-your-analysis",
    "href": "labs/lab_4/Assignment4_Instructions.html#step-3-write-your-analysis",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "For each major section, you must explain in your own words:\n\nWhat you are doing in that step\nWhy this step is important for the analysis\nWhat you found or learned from the results\n\nDo not simply copy text from the example or from your AI friend. Think about the purpose of each technique and articulate it in your own words."
  },
  {
    "objectID": "labs/lab_4/Assignment4_Instructions.html#step-4-format-your-document",
    "href": "labs/lab_4/Assignment4_Instructions.html#step-4-format-your-document",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "Your knitted HTML document should be professional and easy to read:\n\n\n\nRemove unnecessary code, comments, or debugging lines\nKeep only essential code chunks\nUse code-fold: show in your YAML header\n\n\n\n\n\nUse headers to organize sections\nInclude a table of contents\nAdd your name and date\n\n\n\n\n\nEnsure all text renders properly (no broken markdown)\nCheck that headers appear as headers (not plain text)\nUse proper markdown formatting\n\n\n\n\n\nAll plots should have titles and labels\nMaps should be readable\nUse consistent color schemes\n\n\n\n\n\nProofread for typos\nRemove any “Your answer here” placeholders\nMake sure all code runs without errors\nMake IT NEAT. WE GOTTA LOOK GOOD HERE."
  },
  {
    "objectID": "labs/lab_4/Assignment4_Instructions.html#submission-checklist",
    "href": "labs/lab_4/Assignment4_Instructions.html#submission-checklist",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "Before you submit, verify that your document includes:\n\n\n\nIntroduction: Brief description of your chosen 311 violation type and why you selected it\nData Exploration: Maps and summary statistics of your violation data\nFishnet Grid: Successfully created and visualized grid with aggregated counts\nSpatial Features: At least 2-3 spatial features calculated (k-NN, distance measures, etc.)\nLocal Moran’s I: Spatial autocorrelation analysis with interpretation\nCount Regression: Both Poisson and Negative Binomial models with comparison\nSpatial Cross-Validation: LOGO CV implemented on 2017 data with reported metrics\nTemporal Validation: 2018 data aggregated and tested with 2017 model(OPTIONAL.)\nValidation Comparison: Comparison of spatial vs. temporal validation performance\nModel Comparison: Your model vs. KDE baseline for 2018 predictions\nError Analysis: Maps and discussion of where model performs well/poorly\nWritten Explanations: Your own words explaining each step\n\n\n\n\n\nDocument knits to HTML without errors\nCode is clean and well-organized\nAll visualizations display properly\nHeaders and formatting render correctly\nFile is named successfully posted to your website"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html",
    "href": "weekly-notes/week-07-notes.html",
    "title": "Week 7 Notes",
    "section": "",
    "text": "random errors are good, clustered errors are bad\none tool to investigate - spatial autocorrelation of errors\n\nTobler’s First Law - Everything is related to everything, but near things are more related than distanct things - Is there a spatial aspect/clustering of higher vs lower errors? –&gt; Start by mapping residuals\nMoran’s I Measure of spatial autocorrelation – range -1 to +1 where -1 is perfect negative correlation (dispersion) and +1 is perfect positive correlation (clustering)\nDefining neighbors - Contiguity – polygons that share a border (queen in chess) - Distance – all within X meters, fixed threshol - k-Nearest - closest k points, adaptive distance – use this for point data like houses – r function is knn2nk() then use the results of that in nb2listw() which creates a binary matrix of whether two points are neighbors\nComputing Moran’s I - pass the weights into moran.mc() then moran_test$statistic is the Moran’s I value - moran_test$p.value shows significance - if p.value &lt; 0.05, very significant clustering! bad!\nWhat Moran’s I tells you –&gt; 1. Add more spatial features 2. Try spatial fixed effects 3. Consider spatial regression models"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#notes",
    "href": "weekly-notes/week-07-notes.html#notes",
    "title": "Week 7 Notes",
    "section": "",
    "text": "random errors are good, clustered errors are bad\none tool to investigate - spatial autocorrelation of errors\n\nTobler’s First Law - Everything is related to everything, but near things are more related than distanct things - Is there a spatial aspect/clustering of higher vs lower errors? –&gt; Start by mapping residuals\nMoran’s I Measure of spatial autocorrelation – range -1 to +1 where -1 is perfect negative correlation (dispersion) and +1 is perfect positive correlation (clustering)\nDefining neighbors - Contiguity – polygons that share a border (queen in chess) - Distance – all within X meters, fixed threshol - k-Nearest - closest k points, adaptive distance – use this for point data like houses – r function is knn2nk() then use the results of that in nb2listw() which creates a binary matrix of whether two points are neighbors\nComputing Moran’s I - pass the weights into moran.mc() then moran_test$statistic is the Moran’s I value - moran_test$p.value shows significance - if p.value &lt; 0.05, very significant clustering! bad!\nWhat Moran’s I tells you –&gt; 1. Add more spatial features 2. Try spatial fixed effects 3. Consider spatial regression models"
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-4-count-regression-models",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-4-count-regression-models",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 4: Count Regression Models",
    "text": "Part 4: Count Regression Models\nCollect crimes data and join crime counts to fishnet\n\ncrimes &lt;- read.csv(\"C:/Users/knoxk/Downloads/Crimes_-_2018_20251117.csv\")\ncrimes &lt;- crimes%&gt;%filter(!is.na(Latitude) & !is.na(Longitude))\ncrimes_sf &lt;- crimes %&gt;% st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) \ncrimes_sf &lt;- crimes_sf %&gt;% st_transform(26971)\n\ncrimes_sf%&gt;%\n  ggplot()+\n  geom_sf(alpha = 0.1, color = \"violetred\")+\n  geom_sf(data = chi_boundary, fill = NA, color = \"darkgreen\")+\n  theme_void()\n\n\n\n\n\n\n\n\n\n# Count lights out per cell\ncrime_count &lt;- st_join(crimes_sf, fishnet) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(crime_count = n())\n\n# Join back to fishnet\nfishnet &lt;- fishnet %&gt;%\n  left_join(crime_count, by = \"uniqueID\") %&gt;%\n  mutate(crime_count = replace_na(crime_count, 0))\n\n# Summary\nsummary(fishnet$crime_count)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   11.00   60.00   99.82  136.00 2188.00 \n\n\nThis shows the distribution of crimes per fishnet grid.\n\n# Fit Poisson model\nmodel_poisson &lt;- glm(\n  crime_count ~ count_lights_out + lights_out_dists,\n  data = fishnet,\n  family = poisson(link = \"log\")\n)\n\n# View results\nsummary(model_poisson)\n\n\nCall:\nglm(formula = crime_count ~ count_lights_out + lights_out_dists, \n    family = poisson(link = \"log\"), data = fishnet)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       4.436e+00  4.602e-03   963.9   &lt;2e-16 ***\ncount_lights_out  3.404e-03  1.706e-05   199.5   &lt;2e-16 ***\nlights_out_dists -3.982e-03  3.557e-05  -111.9   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 351602  on 2639  degrees of freedom\nResidual deviance: 231205  on 2637  degrees of freedom\nAIC: 244686\n\nNumber of Fisher Scoring iterations: 9\n\n# Exponentiate coefficients for interpretation\nexp(coef(model_poisson))\n\n     (Intercept) count_lights_out lights_out_dists \n      84.4003005        1.0034096        0.9960263 \n\n\nUsing a Poisson Regression where the dependent variable is the number of crimes in a grid cell and the dependent variables are the count of complaints of lights out and distance from grid center to nearest complaint of lights out. The model shows that areas with more “lights out” complaints tend to have slightly higher crime counts. Specifically, each additional street light reported as out in a grid cell is associated with a small increase in the expected number of crimes. Conversely, the farther a location is from the nearest street light outage, the slightly lower the expected crime count. In other words, crimes tend to be more common in areas where street lights are out or clustered, suggesting a very very small link between lighting conditions and crime risk.\n\n# Calculate dispersion parameter\nmodel_pois &lt;- glm(\n  crime_count ~ count_lights_out + lights_out_dists,\n  data = fishnet,\n  family = poisson\n)\n\ndispersion &lt;- sum(residuals(model_pois, type = \"pearson\")^2) / model_pois$df.residual\n\ncat(\"Dispersion parameter:\", round(dispersion, 3), \"\\n\")\n\nDispersion parameter: 19091.61 \n\n\nThis dispersion is quite enormous, and shows that the variance is significantly higher than the mean, which violates an assumption of the Poisson Model.\n\n# Fit Negative Binomial model\nmodel_nb &lt;- glm.nb(\n  crime_count ~ count_lights_out + lights_out_dists,\n  data = fishnet,\n  init.theta = 1,      # starting value for dispersion\n  control = glm.control(epsilon = 1e-8, maxit = 50)\n)\n\n# View results\nsummary(model_nb)\n\n\nCall:\nglm.nb(formula = crime_count ~ count_lights_out + lights_out_dists, \n    data = fishnet, control = glm.control(epsilon = 1e-08, maxit = 50), \n    init.theta = 0.6218860388, link = log)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       3.563e+00  4.293e-02   82.99   &lt;2e-16 ***\ncount_lights_out  7.790e-03  2.596e-04   30.01   &lt;2e-16 ***\nlights_out_dists -7.746e-04  6.132e-05  -12.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.6219) family taken to be 1)\n\n    Null deviance: 4221.3  on 2639  degrees of freedom\nResidual deviance: 3197.0  on 2637  degrees of freedom\nAIC: 27647\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.6219 \n          Std. Err.:  0.0168 \n\n 2 x log-likelihood:  -27638.6580 \n\n# Compare to Poisson\nAIC(model_poisson)\n\n[1] 244685.8\n\nAIC(model_nb) \n\n[1] 27646.66\n\n\nA Negative Binomial model is a type of count regression that estimates the relationship between predictor variables and a count outcome while accounting for overdispersion, making it appropriate when the variance of the counts exceeds the mean, as is the case here.\nEach additional street light reported as out is associated with an approximately 0.78% increase in expected crime counts, while each additional meter further from the nearest lights-out complaint is associated with an approximately 0.08% decrease. These effects are statistically significant, though quite small in magnitude, and the negative binomial model accounts for overdispersion in crime counts, providing more reliable estimates than the Poisson Model. This is supported by the NB model’s much lower AIC."
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-5-spatial-cross-validation",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-5-spatial-cross-validation",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 5: Spatial Cross-Validation",
    "text": "Part 5: Spatial Cross-Validation\nJoin districts to fishnet:\n\ncrime_count_dist &lt;- crimes_sf %&gt;%\n  st_join(fishnet) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID, District) %&gt;% \n  summarize(crime_count = n(), .groups = \"drop\")  \n\ncrime_count_dist_unique &lt;- crime_count_dist %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarise(\n    crime_count = sum(crime_count, na.rm = TRUE),\n    District = first(District)  # pick first district if multiple\n  )\n\n# Join back to fishnet\nfishnet_2018 &lt;- fishnet %&gt;%\n  left_join(crime_count_dist_unique, by = \"uniqueID\") %&gt;%\n  mutate(\n    crime_count = replace_na(fishnet$crime_count, 0),\n    District = District  \n  )%&gt;%filter(!is.na(District))\n\nfishnet_2018 &lt;- fishnet_2018 %&gt;% \n  filter(District != 1 & District != 31)\n\n\n# Get unique districts\ndistricts &lt;- unique(fishnet_2018$District)\n\n# Initialize results\ncv_results &lt;- list()\n\n# Loop through districts\nfor (dist in districts) {\n  # Split data\n  train_data &lt;- fishnet_2018 %&gt;% filter(District != dist)\n  test_data &lt;- fishnet_2018 %&gt;% filter(District == dist)\n  \n  # Fit model on training data\n  model_cv &lt;- glm.nb(\n  crime_count ~ count_lights_out + lights_out_dists,\n  data = train_data,\n  init.theta = .1,  # starting value for dispersion\n  control = glm.control(maxit = 50, epsilon = 1e-8)\n)\n  \n  # Predict on test data\n  test_data$prediction &lt;- predict(model_cv, test_data, type = \"response\")\n  \n  # Store results\n  cv_results[[dist]] &lt;- test_data\n}\n\n# Combine all predictions\nall_predictions &lt;- bind_rows(cv_results)\n\n# Calculate metrics by district\ncv_metrics &lt;- all_predictions %&gt;%\n  group_by(District) %&gt;%\n  summarize(\n    MAE = mean(abs(crime_count - prediction)),\n    RMSE = sqrt(mean((crime_count - prediction)^2)),\n    ME = mean(crime_count - prediction)\n  )\n\n\n# Map prediction errors\nall_predictions &lt;- all_predictions %&gt;%\n  mutate(\n    error = crime_count - prediction,\n    abs_error = abs(error),\n    pct_error = (prediction - crime_count) / (crime_count + 1) * 100\n  )\n\n# Visualize\nggplot(all_predictions) +\n  geom_sf(aes(fill = error), color = NA) +\n  scale_fill_gradient2(\n    low = \"blue\", mid = \"white\", high = \"red\",\n    midpoint = 0,\n    name = \"Error\"\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Red = Over-prediction, Blue = Under-prediction\") +\n  theme_void()\n\n\n\n\n\n\n\n\nThis analysis shows that under-predictions of crimes produced by our model are concentrated in south Chicago, whereas over-predilections are concentrated in more central areas. This indicates that street lights outtages are not able to account for the distribution of crime across the city."
  },
  {
    "objectID": "labs/lab_4/Knox_Katie_Assignment4.html#part-6-model-evaluation",
    "href": "labs/lab_4/Knox_Katie_Assignment4.html#part-6-model-evaluation",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "Part 6: Model Evaluation",
    "text": "Part 6: Model Evaluation\n\npreds &lt;- all_predictions %&gt;%\n  sf::st_drop_geometry() %&gt;%\n  dplyr::select(uniqueID, prediction)\n\nfishnet_2018 &lt;- fishnet_2018 %&gt;%\n  dplyr::left_join(preds, by = \"uniqueID\")\n\n# Step 1: Convert to point pattern (ppp) object\nlights_out_ppp &lt;- as.ppp(\n  X = st_coordinates(all_lights_out),\n  W = as.owin(st_bbox(chi_boundary))\n)\n\n# Step 2: Calculate KDE\nkde_surface &lt;- density.ppp(\n  lights_out_ppp,\n  sigma = 1000,  # Bandwidth in meters\n  edge = TRUE    # Edge correction\n)\n\n# Step 3: Extract values to fishnet cells\nfishnet_2018$kde_risk &lt;- raster::extract(\n  raster(kde_surface),\n  st_centroid(fishnet_2018)\n)\n\n# Standardize to 0-1 scale for comparison\nfishnet_2018$kde_risk &lt;- (fishnet_2018$kde_risk - min(fishnet_2018$kde_risk, na.rm=T)) / \n                     (max(fishnet_2018$kde_risk, na.rm=T) - min(fishnet_2018$kde_risk, na.rm=T))\n\n# Create quintiles (5 equal groups)\nfishnet_2018$model_risk_category &lt;- cut(\n  fishnet_2018$prediction,\n  breaks = quantile(fishnet_2018$prediction, probs = seq(0, 1, 0.2)),\n  labels = c(\"1st (Lowest)\", \"2nd\", \"3rd\", \"4th\", \"5th (Highest)\"),\n  include.lowest = TRUE\n)\n\nfishnet_2018$kde_risk_category &lt;- cut(\n  fishnet_2018$kde_risk,\n  breaks = quantile(fishnet_2018$kde_risk, probs = seq(0, 1, 0.2)),\n  labels = c(\"1st (Lowest)\", \"2nd\", \"3rd\", \"4th\", \"5th (Highest)\"),\n  include.lowest = TRUE\n)\n\n\np3 &lt;- ggplot(fishnet_2018) +\n  geom_sf(aes(fill = model_risk_category), color = NA) +\n  labs(title = \"Model Risk Quintiles\") +\n  theme_void()\n\np4 &lt;- ggplot(fishnet_2018) +\n  geom_sf(aes(fill = kde_risk_category), color = NA) +\n  labs(title = \"KDE Risk Quintiles\") +\n  theme_void()\n\np3 + p4\n\n\n\n\n\n\n\n\n\n# Spatial join: 2018 crimes to fishnet with risk categories\nresults_2018 &lt;- st_join(fishnet_2018, crimes_sf) %&gt;%\n  group_by(model_risk_category) %&gt;%\n  summarize(crimes_sf = n()) %&gt;%\n  mutate(pct_of_total = 100 * crimes_sf / sum(crimes_sf))%&gt;%\n  dplyr::select(\"model_risk_category\", \"pct_of_total\")\n\nkde_2018 &lt;- st_join(fishnet_2018, crimes_sf) %&gt;%\n  group_by(kde_risk_category) %&gt;%\n  summarize(crimes_sf = n()) %&gt;%\n  mutate(pct_of_total = 100 * crimes_sf / sum(crimes_sf))%&gt;%\n  dplyr::select(\"kde_risk_category\", \"pct_of_total\")\n\nkde_2018&lt;- kde_2018%&gt;%st_drop_geometry\nresults_2018&lt;- results_2018%&gt;%st_drop_geometry\n\nresults_2018_clean &lt;- results_2018 %&gt;%\n  rename(\n    `Model Risk Category` = model_risk_category,\n    `Percent of Crimes` = pct_of_total\n  ) %&gt;%\n  mutate(\n    `Percent of Crimes` = percent(`Percent of Crimes`/100, accuracy = 0.1)\n  )\n\nkde_2018_clean &lt;- kde_2018 %&gt;%\n  rename(\n    `KDE Risk Category` = kde_risk_category,\n    `Percent of Crimes` = pct_of_total\n  ) %&gt;%\n  mutate(\n    `Percent of Crimes` = percent(`Percent of Crimes`/100, accuracy = 0.1)\n  )\n\nkbl(\n  list(results_2018_clean, kde_2018_clean),\n  caption = \"Model Risk Areas vs KDE Risk Areas and Percentage of Crimes in Risk Areas\"\n) %&gt;% \n  kable_classic(full_width = FALSE)\n\n\nModel Risk Areas vs KDE Risk Areas and Percentage of Crimes in Risk Areas\n\n\n\n\n\n\n\n\n\nModel Risk Category\nPercent of Crimes\n\n\n\n\n1st (Lowest)\n5.2%\n\n\n2nd\n15.3%\n\n\n3rd\n20.3%\n\n\n4th\n24.9%\n\n\n5th (Highest)\n34.3%\n\n\n\n\n\n\nKDE Risk Category\nPercent of Crimes\n\n\n\n\n1st (Lowest)\n4.8%\n\n\n2nd\n16.2%\n\n\n3rd\n18.9%\n\n\n4th\n22.5%\n\n\n5th (Highest)\n37.7%\n\n\n\n\n\n\n\n\n\n\nAs this table shows, the highest risk area identified by the model account for a slightly smaller percentage of total crimes than the KDE highest risk area. This shows that the model based on street light outtage reports slightly underperforms simply examining peak crime location by historical data in order to account for higher proportions of crime."
  },
  {
    "objectID": "labs/lab5/scripts/Knox_Katie_Assignment5.html",
    "href": "labs/lab5/scripts/Knox_Katie_Assignment5.html",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand",
    "section": "",
    "text": "I have chosen to examine Q4 data from 2024 (last year) as we are currently in Q4 of 2025 as of writing, and I believe last year’s patterns for this time of year would be best to inform predictions for Q4 2025.\n\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 &lt;- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n\n\n# Read Q4 2024 data\nindego &lt;- read_csv(\"../data/indego-trips-2024-q4.csv\")\n\n# Quick look at the data\nglimpse(indego)\n\nRows: 299,121\nColumns: 15\n$ trip_id             &lt;dbl&gt; 1050296434, 1050293063, 1050296229, 1050276817, 10…\n$ duration            &lt;dbl&gt; 22, 8, 12, 1, 5, 9, 5, 6, 16, 16, 16, 39, 6, 40, 1…\n$ start_time          &lt;chr&gt; \"10/1/2024 0:00\", \"10/1/2024 0:06\", \"10/1/2024 0:0…\n$ end_time            &lt;chr&gt; \"10/1/2024 0:22\", \"10/1/2024 0:14\", \"10/1/2024 0:1…\n$ start_station       &lt;dbl&gt; 3322, 3166, 3007, 3166, 3075, 3166, 3030, 3010, 33…\n$ start_lat           &lt;dbl&gt; 39.93638, 39.97195, 39.94517, 39.97195, 39.96718, …\n$ start_lon           &lt;dbl&gt; -75.15526, -75.13445, -75.15993, -75.13445, -75.16…\n$ end_station         &lt;dbl&gt; 3375, 3017, 3244, 3166, 3102, 3008, 3203, 3163, 33…\n$ end_lat             &lt;dbl&gt; 39.96036, 39.98003, 39.93865, 39.97195, 39.96759, …\n$ end_lon             &lt;dbl&gt; -75.14020, -75.14371, -75.16674, -75.13445, -75.17…\n$ bike_id             &lt;chr&gt; \"03580\", \"05386\", \"18082\", \"02729\", \"18519\", \"0272…\n$ plan_duration       &lt;dbl&gt; 365, 365, 365, 1, 30, 1, 365, 365, 30, 30, 30, 30,…\n$ trip_route_category &lt;chr&gt; \"One Way\", \"One Way\", \"One Way\", \"Round Trip\", \"On…\n$ passholder_type     &lt;chr&gt; \"Indego365\", \"Indego365\", \"Indego365\", \"Walk-up\", …\n$ bike_type           &lt;chr&gt; \"standard\", \"standard\", \"electric\", \"standard\", \"e…\n\n\n\n\n\n# How many trips?\ncat(\"Total trips in Q4 2024:\", nrow(indego), \"\\n\")\n\nTotal trips in Q4 2024: 299121 \n\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n\nUnique start stations: 256 \n\n# Trip types\ntable(indego$trip_route_category)\n\n\n   One Way Round Trip \n    282675      16446 \n\n# Passholder types\ntable(indego$passholder_type)\n\n\n  Day Pass   Indego30  Indego365 IndegoFlex       NULL    Walk-up \n     10711     159895     112690          1       1165      14659 \n\n# Bike types\ntable(indego$bike_type)\n\n\nelectric standard \n  175503   123618 \n\n\n\n\n\n\nindego &lt;- indego %&gt;%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego %&gt;% select(start_datetime, interval60, week, dotw, hour, weekend))\n\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;ord&gt; &lt;int&gt;   &lt;dbl&gt;\n1 2024-10-01 00:00:00 2024-10-01 00:00:00    40 Tue       0       0\n2 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n3 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n4 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n5 2024-10-01 00:07:00 2024-10-01 00:00:00    40 Tue       0       0\n6 2024-10-01 00:08:00 2024-10-01 00:00:00    40 Tue       0       0\n\n\n\n\n\n\n# Daily trip counts\ndaily_trips &lt;- indego %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q4 2024\",\n    subtitle = \"Fall demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\nRidership falls during autumn to winter transition. There is also a particularly low ridership towards the end of November, perhaps November 28th, Thanksgiving 2024, as well as the end of December, likely December 25th, Christmas. We can confirm this intuition by examining those specific dates.\n\nturkey_day &lt;- daily_trips %&gt;% filter(date == \"2024-11-28\")\nxmas &lt;- daily_trips %&gt;% filter(date == \"2024-12-25\")\n\ntypical_boring_thurs &lt;- indego %&gt;%\n  filter(dotw == \"Thu\", date != \"2024-11-28\") %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n()) %&gt;%\n  summarize(avg_thurs_trips = mean(trips))\n\ntypical_boring_wed &lt;- indego %&gt;%\n  filter(dotw == \"Wed\", date != \"2024-12-25\") %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n()) %&gt;%\n  summarize(avg_wed_trips = mean(trips))\n\n\nprint(turkey_day)\n\n# A tibble: 1 × 2\n  date       trips\n  &lt;date&gt;     &lt;int&gt;\n1 2024-11-28   604\n\nprint(typical_boring_thurs)\n\n# A tibble: 1 × 1\n  avg_thurs_trips\n            &lt;dbl&gt;\n1           3634.\n\nprint(xmas)\n\n# A tibble: 1 × 2\n  date       trips\n  &lt;date&gt;     &lt;int&gt;\n1 2024-12-25   495\n\nprint(typical_boring_wed)\n\n# A tibble: 1 × 1\n  avg_wed_trips\n          &lt;dbl&gt;\n1         3845.\n\n\n\n\n\n\n# Average trips by hour and day type\nhourly_patterns &lt;- indego %&gt;%\n  group_by(hour, weekend) %&gt;%\n  summarize(avg_trips = n() / n_distinct(date)) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\nWeekdays see two peaks during prime work commute hours, whereas weekends see a smooth curve of average hourly trips throughout daylight hours, peaking around mid afternoon.\n\n\n\n\n# Most popular origin stations\ntop_stations &lt;- indego %&gt;%\n  count(start_station, start_lat, start_lon, name = \"trips\") %&gt;%\n  arrange(desc(trips))\n\ntop_stations%&gt;%head(20)%&gt;%\nkable(\n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nTop 20 Indego Stations by Trip Origins\n\n\nstart_station\nstart_lat\nstart_lon\ntrips\n\n\n\n\n3,010\n39.94711\n-75.16618\n5,943\n\n\n3,032\n39.94527\n-75.17971\n4,471\n\n\n3,359\n39.94888\n-75.16978\n3,923\n\n\n3,244\n39.93865\n-75.16674\n3,492\n\n\n3,295\n39.95028\n-75.16027\n3,411\n\n\n3,020\n39.94855\n-75.19007\n3,369\n\n\n3,208\n39.95048\n-75.19324\n3,343\n\n\n3,066\n39.94561\n-75.17348\n3,342\n\n\n3,054\n39.96250\n-75.17420\n3,297\n\n\n3,101\n39.94295\n-75.15955\n3,214\n\n\n3,022\n39.95472\n-75.18323\n3,152\n\n\n3,028\n39.94061\n-75.14958\n3,101\n\n\n3,362\n39.94816\n-75.16226\n3,072\n\n\n3,063\n39.94633\n-75.16980\n2,972\n\n\n3,185\n39.95169\n-75.15888\n2,952\n\n\n3,059\n39.96244\n-75.16121\n2,926\n\n\n3,012\n39.94218\n-75.17747\n2,907\n\n\n3,061\n39.95425\n-75.17761\n2,847\n\n\n3,161\n39.95486\n-75.18091\n2,837\n\n\n3,256\n39.95269\n-75.17779\n2,816\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = top_stations,\n    aes(x = start_lon, y = start_lat, color = trips),\n    size = 0.75, alpha = 0.6\n  ) +\n  scale_color_gradientn(\n    colours = c(\"#FAFF89\", \"#D3321D\"),\n    name = \"Trips\\nOriginating at Station\"\n  )  +\n  guides(\n    fill = guide_colorbar(),\n    color = guide_colorbar()\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    legend.box = \"horizontal\"     # &lt;-- puts them side by side\n  ) +\n  mapTheme\n\n\n\n\n\n\n\n\n\n\n\n\n# Create sf object for stations\nstations_sf &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census &lt;- st_join(stations_sf, philly_census, left = TRUE) %&gt;%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  left_join(\n    stations_census %&gt;% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(has_census = !is.na(Med_Inc))\n\nsummary(stations_for_map$has_census)\n\n   Mode   FALSE    TRUE \nlogical      19     237 \n\n\n\n# Add back to trip data\nindego_census &lt;- indego %&gt;%\n  left_join(\n    stations_census %&gt;% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  left_join(\n    stations_census %&gt;% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %&gt;% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %&gt;% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n\n\n\n\n\n\n\n\n\n# Identify which stations to keep\nvalid_stations &lt;- stations_census %&gt;%\n  filter(!is.na(Med_Inc)) %&gt;%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census &lt;- indego %&gt;%\n  filter(start_station %in% valid_stations) %&gt;%\n  left_join(\n    stations_census %&gt;% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n\n\n\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q4 2024: October 1 - December 31\nweather_data &lt;- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2024-10-01\",\n  date_end = \"2024-12-31\"\n)\n\n# Process weather data\nweather_processed &lt;- weather_data %&gt;%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %&gt;%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %&gt;%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete &lt;- weather_processed %&gt;%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %&gt;%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %&gt;% select(Temperature, Precipitation, Wind_Speed))\n\n  Temperature    Precipitation        Wind_Speed    \n Min.   :12.00   Min.   :0.000000   Min.   : 0.000  \n 1st Qu.:41.00   1st Qu.:0.000000   1st Qu.: 4.000  \n Median :51.00   Median :0.000000   Median : 7.000  \n Mean   :50.80   Mean   :0.004177   Mean   : 7.663  \n 3rd Qu.:60.95   3rd Qu.:0.000000   3rd Qu.:11.000  \n Max.   :83.00   Max.   :0.520000   Max.   :30.000  \n\n\n\n\n\n\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q1 2025\",\n    subtitle = \"Winter to early spring transition\",\n    x = \"Date\",\n    y = \"Temperature (°F)\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\n\n\n\n# Count trips by station-hour\ntrips_panel &lt;- indego_census %&gt;%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %&gt;%\n  summarize(Trip_Count = n()) %&gt;%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n\n[1] 150972\n\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n\n[1] 237\n\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n\n[1] 2208\n\n\n\n\n\n\n# Calculate expected panel size\nn_stations &lt;- length(unique(trips_panel$start_station))\nn_hours &lt;- length(unique(trips_panel$interval60))\nexpected_rows &lt;- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n\nExpected panel rows: 523,296 \n\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\nCurrent rows: 150,972 \n\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\nMissing rows: 372,324 \n\n\n\n# Create complete panel\nstudy_panel &lt;- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %&gt;%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %&gt;%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes &lt;- trips_panel %&gt;%\n  group_by(start_station) %&gt;%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n\nComplete panel rows: 523,296 \n\n\n\n\n\n\nstudy_panel &lt;- study_panel %&gt;%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n\n\n\n\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %&gt;% select(Trip_Count, Temperature, Precipitation))\n\n   Trip_Count       Temperature   Precipitation   \n Min.   : 0.0000   Min.   :12.0   Min.   :0.0000  \n 1st Qu.: 0.0000   1st Qu.:41.0   1st Qu.:0.0000  \n Median : 0.0000   Median :51.0   Median :0.0000  \n Mean   : 0.5178   Mean   :50.8   Mean   :0.0042  \n 3rd Qu.: 1.0000   3rd Qu.:61.0   3rd Qu.:0.0000  \n Max.   :28.0000   Max.   :83.0   Max.   :0.5200  \n                   NA's   :5688   NA's   :5688    \n\n\n\n\n\n\n# Sort by station and time\nstudy_panel &lt;- study_panel %&gt;%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel &lt;- study_panel %&gt;%\n  group_by(start_station) %&gt;%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %&gt;%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete &lt;- study_panel %&gt;%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n\nRows after removing NA lags: 605,298 \n\n\n\n\n\n\n# Sample one station to visualize\nexample_station &lt;- study_panel_complete %&gt;%\n  filter(start_station == 3212) %&gt;%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#F09B4E\",\n    \"24 Hours Ago\" = \"#D3321D\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\n\n\nApproach: Train on weeks 1-9, test on weeks 10-13 (predicting future from past)\n\n# Split by week\n# Q4 has weeks 40-53 (Oct-Dec)\n# Train on weeks 40-49 (Oct 1 - early December)\n# Test on weeks 50-53 (rest of December)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 50) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\nlate_stations &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 50) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations &lt;- intersect(early_stations, late_stations)\n\n# Filter panel to only common stations\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 50)\n\ntest &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 50)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n\nTraining observations: 410,628 \n\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n\nTesting observations: 171,684 \n\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n\nTraining date range: 19997 to 20065 \n\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n\nTesting date range: 20066 to 20088 \n\n\n\n\n\n\n# Create day of week factor with treatment (dummy) coding\ntrain &lt;- train %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) &lt;- contr.treatment(7)\n\n# Now run the model\nmodel1 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\nsummary(model1)\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7089 -0.6783 -0.2159  0.1882 27.2727 \n\nCoefficients:\n                   Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       -0.489156   0.013731 -35.624 &lt; 0.0000000000000002 ***\nas.factor(hour)1  -0.055267   0.012906  -4.282   0.0000184994686084 ***\nas.factor(hour)2  -0.078435   0.012690  -6.181   0.0000000006378151 ***\nas.factor(hour)3  -0.110234   0.012655  -8.711 &lt; 0.0000000000000002 ***\nas.factor(hour)4  -0.074204   0.012566  -5.905   0.0000000035263476 ***\nas.factor(hour)5   0.038930   0.012623   3.084             0.002042 ** \nas.factor(hour)6   0.296063   0.012639  23.424 &lt; 0.0000000000000002 ***\nas.factor(hour)7   0.562184   0.012853  43.738 &lt; 0.0000000000000002 ***\nas.factor(hour)8   0.940524   0.012603  74.629 &lt; 0.0000000000000002 ***\nas.factor(hour)9   0.681527   0.012691  53.700 &lt; 0.0000000000000002 ***\nas.factor(hour)10  0.562441   0.012526  44.900 &lt; 0.0000000000000002 ***\nas.factor(hour)11  0.602993   0.012538  48.094 &lt; 0.0000000000000002 ***\nas.factor(hour)12  0.698148   0.012435  56.145 &lt; 0.0000000000000002 ***\nas.factor(hour)13  0.667308   0.012338  54.085 &lt; 0.0000000000000002 ***\nas.factor(hour)14  0.685610   0.012386  55.355 &lt; 0.0000000000000002 ***\nas.factor(hour)15  0.791576   0.012780  61.940 &lt; 0.0000000000000002 ***\nas.factor(hour)16  0.906925   0.012549  72.273 &lt; 0.0000000000000002 ***\nas.factor(hour)17  1.110205   0.012639  87.842 &lt; 0.0000000000000002 ***\nas.factor(hour)18  0.831297   0.012728  65.314 &lt; 0.0000000000000002 ***\nas.factor(hour)19  0.541516   0.012768  42.412 &lt; 0.0000000000000002 ***\nas.factor(hour)20  0.341348   0.012858  26.547 &lt; 0.0000000000000002 ***\nas.factor(hour)21  0.233603   0.012881  18.135 &lt; 0.0000000000000002 ***\nas.factor(hour)22  0.175194   0.012820  13.666 &lt; 0.0000000000000002 ***\nas.factor(hour)23  0.076033   0.012906   5.891   0.0000000038337890 ***\ndotw_simple2       0.065588   0.006869   9.549 &lt; 0.0000000000000002 ***\ndotw_simple3       0.062920   0.006857   9.177 &lt; 0.0000000000000002 ***\ndotw_simple4      -0.050061   0.006571  -7.619   0.0000000000000257 ***\ndotw_simple5      -0.012223   0.006833  -1.789             0.073612 .  \ndotw_simple6      -0.024444   0.006806  -3.591             0.000329 ***\ndotw_simple7      -0.050983   0.006924  -7.363   0.0000000000001797 ***\nTemperature        0.012467   0.000161  77.432 &lt; 0.0000000000000002 ***\nPrecipitation     -1.109233   0.081793 -13.562 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.144 on 410596 degrees of freedom\nMultiple R-squared:  0.1141,    Adjusted R-squared:  0.114 \nF-statistic:  1705 on 31 and 410596 DF,  p-value: &lt; 0.00000000000000022\n\n\nPatterns:\n\nTuesday and Wednesday have positive coefficients (0.066 and 0.063)\nThursday through Sunday have negative coefficients\nTuesday has the highest weekday effect (+0.066)\nThis might be a reflection of concentrated commuting patterns at the beginning of the week, and towards the end of the work week there may be more flexible hyrib or work from jobs influencing fewer trips than a Monday.\n\nHourly Patterns\nHour Coefficient Interpretation 0 (baseline) 0.000 trips/hour (midnight) 1 -0.055 slightly fewer than midnight … 5 +0.039 morning activity starting … 8 +0.941 PEAK morning rush … 10 +0.562 post-rush … 17 +1.110 PEAK evening rush … 23 +0.076 late night minimal\n\n\n\n\nmodel2 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.8428  -0.4697  -0.1265   0.1225  25.5313 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       -0.1971724  0.0120171 -16.408 &lt; 0.0000000000000002 ***\nas.factor(hour)1  -0.0175731  0.0112639  -1.560              0.11873    \nas.factor(hour)2  -0.0093400  0.0110785  -0.843              0.39918    \nas.factor(hour)3  -0.0323163  0.0110523  -2.924              0.00346 ** \nas.factor(hour)4  -0.0064036  0.0109793  -0.583              0.55973    \nas.factor(hour)5   0.0625618  0.0110386   5.668  0.00000001449553616 ***\nas.factor(hour)6   0.2581886  0.0110639  23.336 &lt; 0.0000000000000002 ***\nas.factor(hour)7   0.4087297  0.0112659  36.280 &lt; 0.0000000000000002 ***\nas.factor(hour)8   0.6475387  0.0110725  58.482 &lt; 0.0000000000000002 ***\nas.factor(hour)9   0.2687374  0.0111533  24.095 &lt; 0.0000000000000002 ***\nas.factor(hour)10  0.2122365  0.0109778  19.333 &lt; 0.0000000000000002 ***\nas.factor(hour)11  0.2554385  0.0109944  23.234 &lt; 0.0000000000000002 ***\nas.factor(hour)12  0.3583675  0.0108988  32.881 &lt; 0.0000000000000002 ***\nas.factor(hour)13  0.3239304  0.0108163  29.948 &lt; 0.0000000000000002 ***\nas.factor(hour)14  0.3531805  0.0108536  32.540 &lt; 0.0000000000000002 ***\nas.factor(hour)15  0.4322898  0.0112024  38.589 &lt; 0.0000000000000002 ***\nas.factor(hour)16  0.5127136  0.0110123  46.558 &lt; 0.0000000000000002 ***\nas.factor(hour)17  0.6404994  0.0111130  57.635 &lt; 0.0000000000000002 ***\nas.factor(hour)18  0.3266069  0.0112005  29.160 &lt; 0.0000000000000002 ***\nas.factor(hour)19  0.1612724  0.0111995  14.400 &lt; 0.0000000000000002 ***\nas.factor(hour)20  0.0449273  0.0112804   3.983  0.00006812693842296 ***\nas.factor(hour)21  0.0488508  0.0112677   4.335  0.00001454699054146 ***\nas.factor(hour)22  0.0606778  0.0111954   5.420  0.00000005967221309 ***\nas.factor(hour)23  0.0183651  0.0112646   1.630              0.10303    \ndotw_simple2       0.0070021  0.0059975   1.167              0.24301    \ndotw_simple3      -0.0128484  0.0059925  -2.144              0.03203 *  \ndotw_simple4      -0.0448243  0.0057357  -7.815  0.00000000000000551 ***\ndotw_simple5      -0.0422563  0.0059684  -7.080  0.00000000000144379 ***\ndotw_simple6      -0.0367596  0.0059417  -6.187  0.00000000061504630 ***\ndotw_simple7      -0.0593011  0.0060464  -9.808 &lt; 0.0000000000000002 ***\nTemperature        0.0040641  0.0001426  28.494 &lt; 0.0000000000000002 ***\nPrecipitation     -0.9676346  0.0714492 -13.543 &lt; 0.0000000000000002 ***\nlag1Hour           0.3238000  0.0014825 218.412 &lt; 0.0000000000000002 ***\nlag3Hours          0.1180469  0.0014504  81.387 &lt; 0.0000000000000002 ***\nlag1day            0.2027487  0.0013895 145.912 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9988 on 410593 degrees of freedom\nMultiple R-squared:  0.3252,    Adjusted R-squared:  0.3252 \nF-statistic:  5821 on 34 and 410593 DF,  p-value: &lt; 0.00000000000000022\n\n\nAdding lags improve R² by about 20 percentage points. This is likely because the state of the Indego station an hour or a few hours before affects how many trips can be taken hours later, and the one day lag likely accounts for the peaks and ebs at the same time a day before, so it makes sense that this model can account for more of the variance in the data.\n\n\n\n\nmodel3 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.2603  -0.7198  -0.2719   0.4456  24.6278 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.7009115282  0.0386608126  18.130\nas.factor(hour)1         -0.0339064631  0.0443755122  -0.764\nas.factor(hour)2         -0.0733390073  0.0472207425  -1.553\nas.factor(hour)3         -0.1736198797  0.0563179188  -3.083\nas.factor(hour)4         -0.1602016516  0.0526556216  -3.042\nas.factor(hour)5         -0.0842198175  0.0392042217  -2.148\nas.factor(hour)6          0.1812725910  0.0339389718   5.341\nas.factor(hour)7          0.3659932453  0.0327798040  11.165\nas.factor(hour)8          0.6419351722  0.0315874296  20.322\nas.factor(hour)9          0.0689540443  0.0318174207   2.167\nas.factor(hour)10         0.0455849373  0.0318601406   1.431\nas.factor(hour)11         0.0951277607  0.0317831322   2.993\nas.factor(hour)12         0.1718233329  0.0312986651   5.490\nas.factor(hour)13         0.1501299648  0.0313008986   4.796\nas.factor(hour)14         0.1546199385  0.0311814820   4.959\nas.factor(hour)15         0.2738677333  0.0314129060   8.718\nas.factor(hour)16         0.3825663042  0.0310863390  12.307\nas.factor(hour)17         0.5958195674  0.0311303049  19.140\nas.factor(hour)18         0.1871034280  0.0314626011   5.947\nas.factor(hour)19         0.0105635560  0.0320040115   0.330\nas.factor(hour)20        -0.0963042374  0.0327876103  -2.937\nas.factor(hour)21        -0.0718694380  0.0336061149  -2.139\nas.factor(hour)22        -0.0356940800  0.0343008260  -1.041\nas.factor(hour)23        -0.0793554621  0.0360569574  -2.201\ndotw_simple2              0.0015399442  0.0129720488   0.119\ndotw_simple3             -0.0058302798  0.0131356919  -0.444\ndotw_simple4             -0.0584003672  0.0128820191  -4.533\ndotw_simple5             -0.0902616975  0.0132777688  -6.798\ndotw_simple6             -0.0296928615  0.0133240066  -2.229\ndotw_simple7             -0.0404284310  0.0137730955  -2.935\nTemperature               0.0060129110  0.0003255823  18.468\nPrecipitation            -2.6993476761  0.2537854011 -10.636\nlag1Hour                  0.2432506608  0.0023823455 102.106\nlag3Hours                 0.0728128784  0.0024806873  29.352\nlag1day                   0.1565602280  0.0022935086  68.262\nMed_Inc.x                -0.0000001894  0.0000001237  -1.531\nPercent_Taking_Transit.y -0.0038935558  0.0004528281  -8.598\nPercent_White.y           0.0037864819  0.0002282348  16.590\n                                     Pr(&gt;|t|)    \n(Intercept)              &lt; 0.0000000000000002 ***\nas.factor(hour)1                      0.44482    \nas.factor(hour)2                      0.12040    \nas.factor(hour)3                      0.00205 ** \nas.factor(hour)4                      0.00235 ** \nas.factor(hour)5                      0.03170 *  \nas.factor(hour)6              0.0000000925173 ***\nas.factor(hour)7         &lt; 0.0000000000000002 ***\nas.factor(hour)8         &lt; 0.0000000000000002 ***\nas.factor(hour)9                      0.03022 *  \nas.factor(hour)10                     0.15249    \nas.factor(hour)11                     0.00276 ** \nas.factor(hour)12             0.0000000403123 ***\nas.factor(hour)13             0.0000016175893 ***\nas.factor(hour)14             0.0000007104928 ***\nas.factor(hour)15        &lt; 0.0000000000000002 ***\nas.factor(hour)16        &lt; 0.0000000000000002 ***\nas.factor(hour)17        &lt; 0.0000000000000002 ***\nas.factor(hour)18             0.0000000027402 ***\nas.factor(hour)19                     0.74135    \nas.factor(hour)20                     0.00331 ** \nas.factor(hour)21                     0.03247 *  \nas.factor(hour)22                     0.29805    \nas.factor(hour)23                     0.02775 *  \ndotw_simple2                          0.90550    \ndotw_simple3                          0.65715    \ndotw_simple4                  0.0000058070122 ***\ndotw_simple5                  0.0000000000107 ***\ndotw_simple6                          0.02585 *  \ndotw_simple7                          0.00333 ** \nTemperature              &lt; 0.0000000000000002 ***\nPrecipitation            &lt; 0.0000000000000002 ***\nlag1Hour                 &lt; 0.0000000000000002 ***\nlag3Hours                &lt; 0.0000000000000002 ***\nlag1day                  &lt; 0.0000000000000002 ***\nMed_Inc.x                             0.12579    \nPercent_Taking_Transit.y &lt; 0.0000000000000002 ***\nPercent_White.y          &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.274 on 133822 degrees of freedom\n  (276768 observations deleted due to missingness)\nMultiple R-squared:  0.2192,    Adjusted R-squared:  0.219 \nF-statistic:  1015 on 37 and 133822 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\n\n\nmodel4 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n\nModel 4 R-squared: 0.2469249 \n\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n\nModel 4 Adj R-squared: 0.2454536 \n\n\n\n\n\n\nmodel5 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n\nModel 5 R-squared: 0.2526638 \n\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n\nModel 5 Adj R-squared: 0.251187 \n\n\n\n\n\n\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest &lt;- test %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) &lt;- contr.treatment(7)\n\ntest &lt;- test %&gt;%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results &lt;- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nMean Absolute Error by Model (Test Set)\n\n\nModel\nMAE (trips)\n\n\n\n\n1. Time + Weather\n0.54\n\n\n2. + Temporal Lags\n0.40\n\n\n3. + Demographics\n0.64\n\n\n4. + Station FE\n0.66\n\n\n5. + Rush Hour Interaction\n0.65\n\n\n\n\n\n\n\nTemporal Lags improved the model the most; the biggest drop in mean absolute error was from the model accounting for time of day, day of week, and weather, and then adding in the temporal lags. In fact, adding demographics made the model perform worse than the baseline model, as well as adding station fixed effects and interaction effects on top of that.\nCompared to Q1 2025:\nThe MAE values for Q4 2024 are fairly similar across the models. I believe this might be because both quarters account for season changes into winter (Q4) and out of winter (Q1) that make it harder to predict bike share in the latter part of the quarter based on the first, even when taking into account weather effects. There are also a high concentration of winter holidays that likely affect bike travel in Q4, such as Thanksgiving and Winter Break holidays, and in Q1 New Years, Valentine’s Day, and Spring Break holidays."
  },
  {
    "objectID": "labs/lab5/scripts/Knox_Katie_Assignment5.html#part-1-q4-october---december-2024-data",
    "href": "labs/lab5/scripts/Knox_Katie_Assignment5.html#part-1-q4-october---december-2024-data",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand",
    "section": "",
    "text": "I have chosen to examine Q4 data from 2024 (last year) as we are currently in Q4 of 2025 as of writing, and I believe last year’s patterns for this time of year would be best to inform predictions for Q4 2025.\n\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 &lt;- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n\n\n# Read Q4 2024 data\nindego &lt;- read_csv(\"../data/indego-trips-2024-q4.csv\")\n\n# Quick look at the data\nglimpse(indego)\n\nRows: 299,121\nColumns: 15\n$ trip_id             &lt;dbl&gt; 1050296434, 1050293063, 1050296229, 1050276817, 10…\n$ duration            &lt;dbl&gt; 22, 8, 12, 1, 5, 9, 5, 6, 16, 16, 16, 39, 6, 40, 1…\n$ start_time          &lt;chr&gt; \"10/1/2024 0:00\", \"10/1/2024 0:06\", \"10/1/2024 0:0…\n$ end_time            &lt;chr&gt; \"10/1/2024 0:22\", \"10/1/2024 0:14\", \"10/1/2024 0:1…\n$ start_station       &lt;dbl&gt; 3322, 3166, 3007, 3166, 3075, 3166, 3030, 3010, 33…\n$ start_lat           &lt;dbl&gt; 39.93638, 39.97195, 39.94517, 39.97195, 39.96718, …\n$ start_lon           &lt;dbl&gt; -75.15526, -75.13445, -75.15993, -75.13445, -75.16…\n$ end_station         &lt;dbl&gt; 3375, 3017, 3244, 3166, 3102, 3008, 3203, 3163, 33…\n$ end_lat             &lt;dbl&gt; 39.96036, 39.98003, 39.93865, 39.97195, 39.96759, …\n$ end_lon             &lt;dbl&gt; -75.14020, -75.14371, -75.16674, -75.13445, -75.17…\n$ bike_id             &lt;chr&gt; \"03580\", \"05386\", \"18082\", \"02729\", \"18519\", \"0272…\n$ plan_duration       &lt;dbl&gt; 365, 365, 365, 1, 30, 1, 365, 365, 30, 30, 30, 30,…\n$ trip_route_category &lt;chr&gt; \"One Way\", \"One Way\", \"One Way\", \"Round Trip\", \"On…\n$ passholder_type     &lt;chr&gt; \"Indego365\", \"Indego365\", \"Indego365\", \"Walk-up\", …\n$ bike_type           &lt;chr&gt; \"standard\", \"standard\", \"electric\", \"standard\", \"e…\n\n\n\n\n\n# How many trips?\ncat(\"Total trips in Q4 2024:\", nrow(indego), \"\\n\")\n\nTotal trips in Q4 2024: 299121 \n\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n\nUnique start stations: 256 \n\n# Trip types\ntable(indego$trip_route_category)\n\n\n   One Way Round Trip \n    282675      16446 \n\n# Passholder types\ntable(indego$passholder_type)\n\n\n  Day Pass   Indego30  Indego365 IndegoFlex       NULL    Walk-up \n     10711     159895     112690          1       1165      14659 \n\n# Bike types\ntable(indego$bike_type)\n\n\nelectric standard \n  175503   123618 \n\n\n\n\n\n\nindego &lt;- indego %&gt;%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego %&gt;% select(start_datetime, interval60, week, dotw, hour, weekend))\n\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;ord&gt; &lt;int&gt;   &lt;dbl&gt;\n1 2024-10-01 00:00:00 2024-10-01 00:00:00    40 Tue       0       0\n2 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n3 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n4 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n5 2024-10-01 00:07:00 2024-10-01 00:00:00    40 Tue       0       0\n6 2024-10-01 00:08:00 2024-10-01 00:00:00    40 Tue       0       0\n\n\n\n\n\n\n# Daily trip counts\ndaily_trips &lt;- indego %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q4 2024\",\n    subtitle = \"Fall demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\nRidership falls during autumn to winter transition. There is also a particularly low ridership towards the end of November, perhaps November 28th, Thanksgiving 2024, as well as the end of December, likely December 25th, Christmas. We can confirm this intuition by examining those specific dates.\n\nturkey_day &lt;- daily_trips %&gt;% filter(date == \"2024-11-28\")\nxmas &lt;- daily_trips %&gt;% filter(date == \"2024-12-25\")\n\ntypical_boring_thurs &lt;- indego %&gt;%\n  filter(dotw == \"Thu\", date != \"2024-11-28\") %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n()) %&gt;%\n  summarize(avg_thurs_trips = mean(trips))\n\ntypical_boring_wed &lt;- indego %&gt;%\n  filter(dotw == \"Wed\", date != \"2024-12-25\") %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n()) %&gt;%\n  summarize(avg_wed_trips = mean(trips))\n\n\nprint(turkey_day)\n\n# A tibble: 1 × 2\n  date       trips\n  &lt;date&gt;     &lt;int&gt;\n1 2024-11-28   604\n\nprint(typical_boring_thurs)\n\n# A tibble: 1 × 1\n  avg_thurs_trips\n            &lt;dbl&gt;\n1           3634.\n\nprint(xmas)\n\n# A tibble: 1 × 2\n  date       trips\n  &lt;date&gt;     &lt;int&gt;\n1 2024-12-25   495\n\nprint(typical_boring_wed)\n\n# A tibble: 1 × 1\n  avg_wed_trips\n          &lt;dbl&gt;\n1         3845.\n\n\n\n\n\n\n# Average trips by hour and day type\nhourly_patterns &lt;- indego %&gt;%\n  group_by(hour, weekend) %&gt;%\n  summarize(avg_trips = n() / n_distinct(date)) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\nWeekdays see two peaks during prime work commute hours, whereas weekends see a smooth curve of average hourly trips throughout daylight hours, peaking around mid afternoon.\n\n\n\n\n# Most popular origin stations\ntop_stations &lt;- indego %&gt;%\n  count(start_station, start_lat, start_lon, name = \"trips\") %&gt;%\n  arrange(desc(trips))\n\ntop_stations%&gt;%head(20)%&gt;%\nkable(\n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nTop 20 Indego Stations by Trip Origins\n\n\nstart_station\nstart_lat\nstart_lon\ntrips\n\n\n\n\n3,010\n39.94711\n-75.16618\n5,943\n\n\n3,032\n39.94527\n-75.17971\n4,471\n\n\n3,359\n39.94888\n-75.16978\n3,923\n\n\n3,244\n39.93865\n-75.16674\n3,492\n\n\n3,295\n39.95028\n-75.16027\n3,411\n\n\n3,020\n39.94855\n-75.19007\n3,369\n\n\n3,208\n39.95048\n-75.19324\n3,343\n\n\n3,066\n39.94561\n-75.17348\n3,342\n\n\n3,054\n39.96250\n-75.17420\n3,297\n\n\n3,101\n39.94295\n-75.15955\n3,214\n\n\n3,022\n39.95472\n-75.18323\n3,152\n\n\n3,028\n39.94061\n-75.14958\n3,101\n\n\n3,362\n39.94816\n-75.16226\n3,072\n\n\n3,063\n39.94633\n-75.16980\n2,972\n\n\n3,185\n39.95169\n-75.15888\n2,952\n\n\n3,059\n39.96244\n-75.16121\n2,926\n\n\n3,012\n39.94218\n-75.17747\n2,907\n\n\n3,061\n39.95425\n-75.17761\n2,847\n\n\n3,161\n39.95486\n-75.18091\n2,837\n\n\n3,256\n39.95269\n-75.17779\n2,816\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = top_stations,\n    aes(x = start_lon, y = start_lat, color = trips),\n    size = 0.75, alpha = 0.6\n  ) +\n  scale_color_gradientn(\n    colours = c(\"#FAFF89\", \"#D3321D\"),\n    name = \"Trips\\nOriginating at Station\"\n  )  +\n  guides(\n    fill = guide_colorbar(),\n    color = guide_colorbar()\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    legend.box = \"horizontal\"     # &lt;-- puts them side by side\n  ) +\n  mapTheme\n\n\n\n\n\n\n\n\n\n\n\n\n# Create sf object for stations\nstations_sf &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census &lt;- st_join(stations_sf, philly_census, left = TRUE) %&gt;%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  left_join(\n    stations_census %&gt;% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(has_census = !is.na(Med_Inc))\n\nsummary(stations_for_map$has_census)\n\n   Mode   FALSE    TRUE \nlogical      19     237 \n\n\n\n# Add back to trip data\nindego_census &lt;- indego %&gt;%\n  left_join(\n    stations_census %&gt;% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  left_join(\n    stations_census %&gt;% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %&gt;% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %&gt;% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n\n\n\n\n\n\n\n\n\n# Identify which stations to keep\nvalid_stations &lt;- stations_census %&gt;%\n  filter(!is.na(Med_Inc)) %&gt;%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census &lt;- indego %&gt;%\n  filter(start_station %in% valid_stations) %&gt;%\n  left_join(\n    stations_census %&gt;% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n\n\n\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q4 2024: October 1 - December 31\nweather_data &lt;- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2024-10-01\",\n  date_end = \"2024-12-31\"\n)\n\n# Process weather data\nweather_processed &lt;- weather_data %&gt;%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %&gt;%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %&gt;%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete &lt;- weather_processed %&gt;%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %&gt;%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %&gt;% select(Temperature, Precipitation, Wind_Speed))\n\n  Temperature    Precipitation        Wind_Speed    \n Min.   :12.00   Min.   :0.000000   Min.   : 0.000  \n 1st Qu.:41.00   1st Qu.:0.000000   1st Qu.: 4.000  \n Median :51.00   Median :0.000000   Median : 7.000  \n Mean   :50.80   Mean   :0.004177   Mean   : 7.663  \n 3rd Qu.:60.95   3rd Qu.:0.000000   3rd Qu.:11.000  \n Max.   :83.00   Max.   :0.520000   Max.   :30.000  \n\n\n\n\n\n\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q1 2025\",\n    subtitle = \"Winter to early spring transition\",\n    x = \"Date\",\n    y = \"Temperature (°F)\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\n\n\n\n# Count trips by station-hour\ntrips_panel &lt;- indego_census %&gt;%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %&gt;%\n  summarize(Trip_Count = n()) %&gt;%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n\n[1] 150972\n\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n\n[1] 237\n\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n\n[1] 2208\n\n\n\n\n\n\n# Calculate expected panel size\nn_stations &lt;- length(unique(trips_panel$start_station))\nn_hours &lt;- length(unique(trips_panel$interval60))\nexpected_rows &lt;- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n\nExpected panel rows: 523,296 \n\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\nCurrent rows: 150,972 \n\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\nMissing rows: 372,324 \n\n\n\n# Create complete panel\nstudy_panel &lt;- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %&gt;%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %&gt;%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes &lt;- trips_panel %&gt;%\n  group_by(start_station) %&gt;%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n\nComplete panel rows: 523,296 \n\n\n\n\n\n\nstudy_panel &lt;- study_panel %&gt;%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n\n\n\n\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %&gt;% select(Trip_Count, Temperature, Precipitation))\n\n   Trip_Count       Temperature   Precipitation   \n Min.   : 0.0000   Min.   :12.0   Min.   :0.0000  \n 1st Qu.: 0.0000   1st Qu.:41.0   1st Qu.:0.0000  \n Median : 0.0000   Median :51.0   Median :0.0000  \n Mean   : 0.5178   Mean   :50.8   Mean   :0.0042  \n 3rd Qu.: 1.0000   3rd Qu.:61.0   3rd Qu.:0.0000  \n Max.   :28.0000   Max.   :83.0   Max.   :0.5200  \n                   NA's   :5688   NA's   :5688    \n\n\n\n\n\n\n# Sort by station and time\nstudy_panel &lt;- study_panel %&gt;%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel &lt;- study_panel %&gt;%\n  group_by(start_station) %&gt;%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %&gt;%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete &lt;- study_panel %&gt;%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n\nRows after removing NA lags: 605,298 \n\n\n\n\n\n\n# Sample one station to visualize\nexample_station &lt;- study_panel_complete %&gt;%\n  filter(start_station == 3212) %&gt;%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#F09B4E\",\n    \"24 Hours Ago\" = \"#D3321D\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\n\n\nApproach: Train on weeks 1-9, test on weeks 10-13 (predicting future from past)\n\n# Split by week\n# Q4 has weeks 40-53 (Oct-Dec)\n# Train on weeks 40-49 (Oct 1 - early December)\n# Test on weeks 50-53 (rest of December)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 50) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\nlate_stations &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 50) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations &lt;- intersect(early_stations, late_stations)\n\n# Filter panel to only common stations\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 50)\n\ntest &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 50)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n\nTraining observations: 410,628 \n\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n\nTesting observations: 171,684 \n\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n\nTraining date range: 19997 to 20065 \n\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n\nTesting date range: 20066 to 20088 \n\n\n\n\n\n\n# Create day of week factor with treatment (dummy) coding\ntrain &lt;- train %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) &lt;- contr.treatment(7)\n\n# Now run the model\nmodel1 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\nsummary(model1)\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7089 -0.6783 -0.2159  0.1882 27.2727 \n\nCoefficients:\n                   Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       -0.489156   0.013731 -35.624 &lt; 0.0000000000000002 ***\nas.factor(hour)1  -0.055267   0.012906  -4.282   0.0000184994686084 ***\nas.factor(hour)2  -0.078435   0.012690  -6.181   0.0000000006378151 ***\nas.factor(hour)3  -0.110234   0.012655  -8.711 &lt; 0.0000000000000002 ***\nas.factor(hour)4  -0.074204   0.012566  -5.905   0.0000000035263476 ***\nas.factor(hour)5   0.038930   0.012623   3.084             0.002042 ** \nas.factor(hour)6   0.296063   0.012639  23.424 &lt; 0.0000000000000002 ***\nas.factor(hour)7   0.562184   0.012853  43.738 &lt; 0.0000000000000002 ***\nas.factor(hour)8   0.940524   0.012603  74.629 &lt; 0.0000000000000002 ***\nas.factor(hour)9   0.681527   0.012691  53.700 &lt; 0.0000000000000002 ***\nas.factor(hour)10  0.562441   0.012526  44.900 &lt; 0.0000000000000002 ***\nas.factor(hour)11  0.602993   0.012538  48.094 &lt; 0.0000000000000002 ***\nas.factor(hour)12  0.698148   0.012435  56.145 &lt; 0.0000000000000002 ***\nas.factor(hour)13  0.667308   0.012338  54.085 &lt; 0.0000000000000002 ***\nas.factor(hour)14  0.685610   0.012386  55.355 &lt; 0.0000000000000002 ***\nas.factor(hour)15  0.791576   0.012780  61.940 &lt; 0.0000000000000002 ***\nas.factor(hour)16  0.906925   0.012549  72.273 &lt; 0.0000000000000002 ***\nas.factor(hour)17  1.110205   0.012639  87.842 &lt; 0.0000000000000002 ***\nas.factor(hour)18  0.831297   0.012728  65.314 &lt; 0.0000000000000002 ***\nas.factor(hour)19  0.541516   0.012768  42.412 &lt; 0.0000000000000002 ***\nas.factor(hour)20  0.341348   0.012858  26.547 &lt; 0.0000000000000002 ***\nas.factor(hour)21  0.233603   0.012881  18.135 &lt; 0.0000000000000002 ***\nas.factor(hour)22  0.175194   0.012820  13.666 &lt; 0.0000000000000002 ***\nas.factor(hour)23  0.076033   0.012906   5.891   0.0000000038337890 ***\ndotw_simple2       0.065588   0.006869   9.549 &lt; 0.0000000000000002 ***\ndotw_simple3       0.062920   0.006857   9.177 &lt; 0.0000000000000002 ***\ndotw_simple4      -0.050061   0.006571  -7.619   0.0000000000000257 ***\ndotw_simple5      -0.012223   0.006833  -1.789             0.073612 .  \ndotw_simple6      -0.024444   0.006806  -3.591             0.000329 ***\ndotw_simple7      -0.050983   0.006924  -7.363   0.0000000000001797 ***\nTemperature        0.012467   0.000161  77.432 &lt; 0.0000000000000002 ***\nPrecipitation     -1.109233   0.081793 -13.562 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.144 on 410596 degrees of freedom\nMultiple R-squared:  0.1141,    Adjusted R-squared:  0.114 \nF-statistic:  1705 on 31 and 410596 DF,  p-value: &lt; 0.00000000000000022\n\n\nPatterns:\n\nTuesday and Wednesday have positive coefficients (0.066 and 0.063)\nThursday through Sunday have negative coefficients\nTuesday has the highest weekday effect (+0.066)\nThis might be a reflection of concentrated commuting patterns at the beginning of the week, and towards the end of the work week there may be more flexible hyrib or work from jobs influencing fewer trips than a Monday.\n\nHourly Patterns\nHour Coefficient Interpretation 0 (baseline) 0.000 trips/hour (midnight) 1 -0.055 slightly fewer than midnight … 5 +0.039 morning activity starting … 8 +0.941 PEAK morning rush … 10 +0.562 post-rush … 17 +1.110 PEAK evening rush … 23 +0.076 late night minimal\n\n\n\n\nmodel2 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.8428  -0.4697  -0.1265   0.1225  25.5313 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       -0.1971724  0.0120171 -16.408 &lt; 0.0000000000000002 ***\nas.factor(hour)1  -0.0175731  0.0112639  -1.560              0.11873    \nas.factor(hour)2  -0.0093400  0.0110785  -0.843              0.39918    \nas.factor(hour)3  -0.0323163  0.0110523  -2.924              0.00346 ** \nas.factor(hour)4  -0.0064036  0.0109793  -0.583              0.55973    \nas.factor(hour)5   0.0625618  0.0110386   5.668  0.00000001449553616 ***\nas.factor(hour)6   0.2581886  0.0110639  23.336 &lt; 0.0000000000000002 ***\nas.factor(hour)7   0.4087297  0.0112659  36.280 &lt; 0.0000000000000002 ***\nas.factor(hour)8   0.6475387  0.0110725  58.482 &lt; 0.0000000000000002 ***\nas.factor(hour)9   0.2687374  0.0111533  24.095 &lt; 0.0000000000000002 ***\nas.factor(hour)10  0.2122365  0.0109778  19.333 &lt; 0.0000000000000002 ***\nas.factor(hour)11  0.2554385  0.0109944  23.234 &lt; 0.0000000000000002 ***\nas.factor(hour)12  0.3583675  0.0108988  32.881 &lt; 0.0000000000000002 ***\nas.factor(hour)13  0.3239304  0.0108163  29.948 &lt; 0.0000000000000002 ***\nas.factor(hour)14  0.3531805  0.0108536  32.540 &lt; 0.0000000000000002 ***\nas.factor(hour)15  0.4322898  0.0112024  38.589 &lt; 0.0000000000000002 ***\nas.factor(hour)16  0.5127136  0.0110123  46.558 &lt; 0.0000000000000002 ***\nas.factor(hour)17  0.6404994  0.0111130  57.635 &lt; 0.0000000000000002 ***\nas.factor(hour)18  0.3266069  0.0112005  29.160 &lt; 0.0000000000000002 ***\nas.factor(hour)19  0.1612724  0.0111995  14.400 &lt; 0.0000000000000002 ***\nas.factor(hour)20  0.0449273  0.0112804   3.983  0.00006812693842296 ***\nas.factor(hour)21  0.0488508  0.0112677   4.335  0.00001454699054146 ***\nas.factor(hour)22  0.0606778  0.0111954   5.420  0.00000005967221309 ***\nas.factor(hour)23  0.0183651  0.0112646   1.630              0.10303    \ndotw_simple2       0.0070021  0.0059975   1.167              0.24301    \ndotw_simple3      -0.0128484  0.0059925  -2.144              0.03203 *  \ndotw_simple4      -0.0448243  0.0057357  -7.815  0.00000000000000551 ***\ndotw_simple5      -0.0422563  0.0059684  -7.080  0.00000000000144379 ***\ndotw_simple6      -0.0367596  0.0059417  -6.187  0.00000000061504630 ***\ndotw_simple7      -0.0593011  0.0060464  -9.808 &lt; 0.0000000000000002 ***\nTemperature        0.0040641  0.0001426  28.494 &lt; 0.0000000000000002 ***\nPrecipitation     -0.9676346  0.0714492 -13.543 &lt; 0.0000000000000002 ***\nlag1Hour           0.3238000  0.0014825 218.412 &lt; 0.0000000000000002 ***\nlag3Hours          0.1180469  0.0014504  81.387 &lt; 0.0000000000000002 ***\nlag1day            0.2027487  0.0013895 145.912 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9988 on 410593 degrees of freedom\nMultiple R-squared:  0.3252,    Adjusted R-squared:  0.3252 \nF-statistic:  5821 on 34 and 410593 DF,  p-value: &lt; 0.00000000000000022\n\n\nAdding lags improve R² by about 20 percentage points. This is likely because the state of the Indego station an hour or a few hours before affects how many trips can be taken hours later, and the one day lag likely accounts for the peaks and ebs at the same time a day before, so it makes sense that this model can account for more of the variance in the data.\n\n\n\n\nmodel3 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.2603  -0.7198  -0.2719   0.4456  24.6278 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.7009115282  0.0386608126  18.130\nas.factor(hour)1         -0.0339064631  0.0443755122  -0.764\nas.factor(hour)2         -0.0733390073  0.0472207425  -1.553\nas.factor(hour)3         -0.1736198797  0.0563179188  -3.083\nas.factor(hour)4         -0.1602016516  0.0526556216  -3.042\nas.factor(hour)5         -0.0842198175  0.0392042217  -2.148\nas.factor(hour)6          0.1812725910  0.0339389718   5.341\nas.factor(hour)7          0.3659932453  0.0327798040  11.165\nas.factor(hour)8          0.6419351722  0.0315874296  20.322\nas.factor(hour)9          0.0689540443  0.0318174207   2.167\nas.factor(hour)10         0.0455849373  0.0318601406   1.431\nas.factor(hour)11         0.0951277607  0.0317831322   2.993\nas.factor(hour)12         0.1718233329  0.0312986651   5.490\nas.factor(hour)13         0.1501299648  0.0313008986   4.796\nas.factor(hour)14         0.1546199385  0.0311814820   4.959\nas.factor(hour)15         0.2738677333  0.0314129060   8.718\nas.factor(hour)16         0.3825663042  0.0310863390  12.307\nas.factor(hour)17         0.5958195674  0.0311303049  19.140\nas.factor(hour)18         0.1871034280  0.0314626011   5.947\nas.factor(hour)19         0.0105635560  0.0320040115   0.330\nas.factor(hour)20        -0.0963042374  0.0327876103  -2.937\nas.factor(hour)21        -0.0718694380  0.0336061149  -2.139\nas.factor(hour)22        -0.0356940800  0.0343008260  -1.041\nas.factor(hour)23        -0.0793554621  0.0360569574  -2.201\ndotw_simple2              0.0015399442  0.0129720488   0.119\ndotw_simple3             -0.0058302798  0.0131356919  -0.444\ndotw_simple4             -0.0584003672  0.0128820191  -4.533\ndotw_simple5             -0.0902616975  0.0132777688  -6.798\ndotw_simple6             -0.0296928615  0.0133240066  -2.229\ndotw_simple7             -0.0404284310  0.0137730955  -2.935\nTemperature               0.0060129110  0.0003255823  18.468\nPrecipitation            -2.6993476761  0.2537854011 -10.636\nlag1Hour                  0.2432506608  0.0023823455 102.106\nlag3Hours                 0.0728128784  0.0024806873  29.352\nlag1day                   0.1565602280  0.0022935086  68.262\nMed_Inc.x                -0.0000001894  0.0000001237  -1.531\nPercent_Taking_Transit.y -0.0038935558  0.0004528281  -8.598\nPercent_White.y           0.0037864819  0.0002282348  16.590\n                                     Pr(&gt;|t|)    \n(Intercept)              &lt; 0.0000000000000002 ***\nas.factor(hour)1                      0.44482    \nas.factor(hour)2                      0.12040    \nas.factor(hour)3                      0.00205 ** \nas.factor(hour)4                      0.00235 ** \nas.factor(hour)5                      0.03170 *  \nas.factor(hour)6              0.0000000925173 ***\nas.factor(hour)7         &lt; 0.0000000000000002 ***\nas.factor(hour)8         &lt; 0.0000000000000002 ***\nas.factor(hour)9                      0.03022 *  \nas.factor(hour)10                     0.15249    \nas.factor(hour)11                     0.00276 ** \nas.factor(hour)12             0.0000000403123 ***\nas.factor(hour)13             0.0000016175893 ***\nas.factor(hour)14             0.0000007104928 ***\nas.factor(hour)15        &lt; 0.0000000000000002 ***\nas.factor(hour)16        &lt; 0.0000000000000002 ***\nas.factor(hour)17        &lt; 0.0000000000000002 ***\nas.factor(hour)18             0.0000000027402 ***\nas.factor(hour)19                     0.74135    \nas.factor(hour)20                     0.00331 ** \nas.factor(hour)21                     0.03247 *  \nas.factor(hour)22                     0.29805    \nas.factor(hour)23                     0.02775 *  \ndotw_simple2                          0.90550    \ndotw_simple3                          0.65715    \ndotw_simple4                  0.0000058070122 ***\ndotw_simple5                  0.0000000000107 ***\ndotw_simple6                          0.02585 *  \ndotw_simple7                          0.00333 ** \nTemperature              &lt; 0.0000000000000002 ***\nPrecipitation            &lt; 0.0000000000000002 ***\nlag1Hour                 &lt; 0.0000000000000002 ***\nlag3Hours                &lt; 0.0000000000000002 ***\nlag1day                  &lt; 0.0000000000000002 ***\nMed_Inc.x                             0.12579    \nPercent_Taking_Transit.y &lt; 0.0000000000000002 ***\nPercent_White.y          &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.274 on 133822 degrees of freedom\n  (276768 observations deleted due to missingness)\nMultiple R-squared:  0.2192,    Adjusted R-squared:  0.219 \nF-statistic:  1015 on 37 and 133822 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\n\n\nmodel4 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n\nModel 4 R-squared: 0.2469249 \n\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n\nModel 4 Adj R-squared: 0.2454536 \n\n\n\n\n\n\nmodel5 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n\nModel 5 R-squared: 0.2526638 \n\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n\nModel 5 Adj R-squared: 0.251187 \n\n\n\n\n\n\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest &lt;- test %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) &lt;- contr.treatment(7)\n\ntest &lt;- test %&gt;%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results &lt;- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nMean Absolute Error by Model (Test Set)\n\n\nModel\nMAE (trips)\n\n\n\n\n1. Time + Weather\n0.54\n\n\n2. + Temporal Lags\n0.40\n\n\n3. + Demographics\n0.64\n\n\n4. + Station FE\n0.66\n\n\n5. + Rush Hour Interaction\n0.65\n\n\n\n\n\n\n\nTemporal Lags improved the model the most; the biggest drop in mean absolute error was from the model accounting for time of day, day of week, and weather, and then adding in the temporal lags. In fact, adding demographics made the model perform worse than the baseline model, as well as adding station fixed effects and interaction effects on top of that.\nCompared to Q1 2025:\nThe MAE values for Q4 2024 are fairly similar across the models. I believe this might be because both quarters account for season changes into winter (Q4) and out of winter (Q1) that make it harder to predict bike share in the latter part of the quarter based on the first, even when taking into account weather effects. There are also a high concentration of winter holidays that likely affect bike travel in Q4, such as Thanksgiving and Winter Break holidays, and in Q1 New Years, Valentine’s Day, and Spring Break holidays."
  },
  {
    "objectID": "labs/lab5/scripts/Knox_Katie_Assignment5.html#todo",
    "href": "labs/lab5/scripts/Knox_Katie_Assignment5.html#todo",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand",
    "section": "TODO",
    "text": "TODO\n\nCompare results to Q1 2025:\n\nHow do MAE values compare? Why might they differ?\nAre temporal patterns different (e.g., summer vs. winter)?\nWhich features are most important in your quarter?"
  },
  {
    "objectID": "labs/lab5/scripts/Knox_Katie_Assignment5.html#part-2-error-analysis",
    "href": "labs/lab5/scripts/Knox_Katie_Assignment5.html#part-2-error-analysis",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand",
    "section": "Part 2: Error Analysis",
    "text": "Part 2: Error Analysis\n\nObserved vs. Predicted Error Analysis\n\ntest &lt;- test %&gt;%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour &lt; 7 ~ \"Overnight\",\n      hour &gt;= 7 & hour &lt; 10 ~ \"AM Rush\",\n      hour &gt;= 10 & hour &lt; 15 ~ \"Mid-Day\",\n      hour &gt;= 15 & hour &lt;= 18 ~ \"PM Rush\",\n      hour &gt; 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\nThe model consistently under predicts trips on weekdays, weekends, at all time periods of the day. The one period that appears to be moderately better predicted is the AM Rush period of weekdays, and generally slightly better at predicting weekdays than weekends. However, the consistent under-predicting seems to signify there is a major driver of Indego trips that this model does not account for well.\n\n\nSpatial Error Patterns\n\n# Calculate MAE by station\nstation_errors &lt;- test %&gt;%\n  group_by(start_station, start_lat.x, start_lon.y) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors &lt;- test %&gt;%\n  filter(!is.na(pred2)) %&gt;%\n  group_by(start_station, start_lat.x, start_lon.y) %&gt;%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE\\n(trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Higher in Center City\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand\np2 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg\\nDemand\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\",\n       subtitle = \"Trips per station-hour\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\np1 | p2\n\n\n\n\n\n\n\n\nHigher MAEs are clustered around center city, where trips per hour are also higher on average. This nuances our understanding of the model’s under-performance overall, as we can see that MAEs are not equal across all stations but higher error gaps where there are higher trips. This indicates that there are spatial variables that this model lacks.\n\n\nTemporal Error Patterns\n\n# MAE by time of day and day type\ntemporal_errors &lt;- test %&gt;%\n  group_by(time_of_day, weekend) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nMAEs are higher for weekdays than weekends on average, but combined with the previous plots of observed vs predicted values, this shows that the mean absolute errors partially higher for weekdays because of higher trip counts on weekdays typically than weekends, and higher trip counts during that AM and PM rush. In other words, this again confirms that the model is creating predictions that are too uniform accross time and space.\n\n\nErrors and Demographics\n\n# Join demographic data to station errors\nstation_errors_demo &lt;- station_errors %&gt;%\n  left_join(\n    station_attributes %&gt;% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %&gt;%\n  filter(!is.na(Med_Inc))\n\n# Create plots\ninc_plot &lt;- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\ntransit_plot &lt;- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\nwhite_plot &lt;- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(inc_plot, transit_plot, white_plot, ncol = 2)\n\n\n\n\n\n\n\n\nPrediction errors are higher for higher income and more White areas, but lower for higher transit-taking areas. This again tracks with the general under-performance of the model, as percent White and median income are slightly positively correlated with trips and percent using transit is slightly negatively correlated with trips."
  },
  {
    "objectID": "labs/lab5/scripts/Knox_Katie_Assignment5.html#part-3-feature-engineering-model-improvement",
    "href": "labs/lab5/scripts/Knox_Katie_Assignment5.html#part-3-feature-engineering-model-improvement",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand",
    "section": "Part 3: Feature Engineering & model improvement",
    "text": "Part 3: Feature Engineering & model improvement\nAfter observing the error results from the first five models, I believe the biggest aspects the models are missing are spatial variables that influence bike share usage. Seeing as the under-counting is clustered around center city, I think some likely attributes of center city that make biking more attractive is the relative nearness of start and end destinations to each other (~80% of trips are under 1.5 miles). In other words, I think the model would be a better predictor if it took into account the potential rider’s available close-by destination bike stations they could park the bike within the average trip length that riders take. Additionally, I think people are more likely to feel comfortable biking when there are bikes lanes nearby or roads with a low posted speed limit. I also want to build off the success of adding the spatial lags that improved model 1 to model 2 so much, and add another lag for the same hour in the previous week.\n\n# Step 1: Build LINESTRING geometry for each row\ntrips_sf &lt;- indego %&gt;%\n  filter(\n    !is.na(start_lat),\n    !is.na(start_lon),\n    !is.na(end_lat),\n    !is.na(end_lon)\n  )%&gt;%\n  rowwise() %&gt;%\n  mutate(\n    geometry = list(\n      st_linestring(\n        matrix(\n          c(start_lon, start_lat,\n            end_lon,   end_lat),\n          ncol = 2,\n          byrow = TRUE\n        )\n      )\n    )\n  ) %&gt;%\n  ungroup() %&gt;%\n  st_as_sf(crs = 4326)\n\n\n# Step 2: Transform to PA South (meters)\ntrips_sf &lt;- st_transform(trips_sf, 6539)\n\n\n# Step 3: Calculate trip distance (meters)\ntrips_sf &lt;- trips_sf %&gt;%\n  mutate(distance_f = st_length(geometry))\n\ntrips_sf &lt;- trips_sf %&gt;%\n  mutate(\n    distance_mi = as.numeric(distance_f) / 5280  # 1 mile = 5280 ft\n  )\n\nsummary(trips_sf$distance_mi)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.5484  0.9063  1.0439  1.3783  8.8305 \n\n\nSpatial features:\n\nNumber of low-speed roads near station\nNumber and type of Bike lanes near station\nNearness of station to other stations\n\n\nroads_sf &lt;- st_read(\"../data/RMSADMIN_(Administrative_Classifications_of_Roadway)/RMSADMIN_(Administrative_Classifications_of_Roadway).shp\")\n\nReading layer `RMSADMIN_(Administrative_Classifications_of_Roadway)' from data source `/Users/kknox/Documents/GitHub/portfolio-setup-kkxix/labs/lab5/data/RMSADMIN_(Administrative_Classifications_of_Roadway)/RMSADMIN_(Administrative_Classifications_of_Roadway).shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 240887 features and 34 fields (with 1914 geometries empty)\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825317 xmax: -8314805 ymax: 5201143\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nblanes_sf &lt;- st_read(\"../data/Bike_Network/Bike_Network.shp\")\n\nReading layer `Bike_Network' from data source \n  `/Users/kknox/Documents/GitHub/portfolio-setup-kkxix/labs/lab5/data/Bike_Network/Bike_Network.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5225 features and 8 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -8378937 ymin: 4847835 xmax: -8345146 ymax: 4883978\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n# filter to just roadways in Phildelphia County under 25 mph \n# Phildelphia CTY CODE is 67\nroads_sf&lt;-roads_sf%&gt;%filter(\n  CTY_CODE == 67, \n  SPEED_LIMI&lt;=25\n)\n\nroads_sf&lt;-roads_sf%&gt;%\n  st_transform(st_crs(stations_sf))%&gt;%\n  mutate(label=\"Road &lt;25mph\")\n\nblanes_sf&lt;-blanes_sf%&gt;%\n  st_transform(st_crs(stations_sf))%&gt;%\n  mutate(label=\"Bike Lane\")\n\nstations_sf&lt;-stations_sf%&gt;%mutate(\n  label=\"Indego Station\"\n)\n\n# map with stations\nggplot() +\n  geom_sf(data = roads_sf, aes(color = label), linewidth = 0.1) +\n  geom_sf(data = blanes_sf, aes(color = label), linewidth = 0.1) +\n  geom_sf(data = stations_sf, aes(color = label), size = 0.7, alpha = 0.7) +\n  scale_color_manual(\n    name = \"Legend\",\n    values = c(\n      \"Road &lt;25mph\" = \"#bdd7e7\",\n      \"Bike Lane\" = \"#F09B4E\",\n      \"Indego Station\" = \"#08519c\"\n    )\n  ) +\n  labs(title = \"Indego Stations, Bike Lanes, and &lt;25mph Roads\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\n\nstations_buffered &lt;- stations_sf %&gt;%\n  mutate(buffer_geom = st_buffer(geometry, dist = 500))\n\n# ---- Count roads intersecting each buffer ----\nroad_counts &lt;- st_intersects(stations_buffered$buffer_geom, roads_sf) %&gt;%\n  lengths()\n\n# ---- Count bike lanes intersecting each buffer ----\nblane_counts &lt;- st_intersects(stations_buffered$buffer_geom, blanes_sf) %&gt;%\n  lengths()\n\n# ---- Add results back onto stations_sf ----\nstations_sf &lt;- stations_sf %&gt;%\n  mutate(\n    nearby_roads  = road_counts,\n    nearby_bikelanes = blane_counts\n  )\n\nggplot() +\n  geom_sf(data = roads_sf, color=\"lightgray\", linewidth = 0.1) +\n  geom_sf(data = stations_sf, aes(color = nearby_bikelanes), size = 0.7, alpha = 0.7) +\n  labs(title = \"Stations Colored by Number of Nearby Bike Lanes\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\nIn addition to nearby bike lanes and slow roads, I will add the number of other bike stations within a buffer of the median trip distance of 0.9063 miles.\n\n# 0.9063 miles = 1458.54847 meters\n\nstations_buffered &lt;- stations_sf %&gt;%\n  mutate(buffer_geom = st_buffer(geometry, dist = 1458.54847))\n\n# ---- Count indego stations intersecting each buffer ----\nstation_counts &lt;- st_intersects(stations_buffered$buffer_geom, stations_sf) %&gt;%\n  lengths()\n\n# ---- Add results back onto stations_sf ----\nstations_sf &lt;- stations_sf %&gt;%\n  mutate(\n    nearby_stations  = station_counts\n  )\n\nggplot() +\n  geom_sf(data = roads_sf, color=\"lightgray\", linewidth = 0.1) +\n  geom_sf(data = stations_sf, aes(color = nearby_stations), size = 0.7, alpha = 0.7) +\n  labs(title = \"Stations Colored by Number of Nearby Other Stations\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\n\n# Add back to trip data\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(\n    stations_sf %&gt;% \n      select(start_station, nearby_roads, nearby_bikelanes, nearby_stations),\n    by = \"start_station\"\n  )\n\nTrip history features:\n\nAdd lag for same hour last week\n\n\nstudy_panel &lt;- study_panel %&gt;%\n  group_by(start_station) %&gt;%\n  mutate(\n    lag1week = lag(Trip_Count, 168)\n  ) %&gt;%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete &lt;- study_panel %&gt;%\n  filter(!is.na(lag1week) & !is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n\nRows after removing NA lags: 571,170 \n\n\n\nModel 6: Add nearby spatial features and 1-week lag\n\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 50)\n\ntest &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 50)\n\ntrain &lt;- train %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) &lt;- contr.treatment(7)\n\ntest &lt;- test %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) &lt;- contr.treatment(7)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n\nTraining observations: 377,796 \n\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n\nTesting observations: 171,684 \n\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n\nTraining date range: 20003 to 20065 \n\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n\nTesting date range: 20066 to 20088 \n\n\n\nmodel6 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend +  # Rush hour effects different on weekends\n    nearby_roads + nearby_bikelanes + nearby_stations +\n    lag1week,\n  data = train\n)\n\ncat(\"Model 6 R-squared:\", summary(model6)$r.squared, \"\\n\")\n\nModel 6 R-squared: 0.2569648 \n\ncat(\"Model 6 Adj R-squared:\", summary(model6)$adj.r.squared, \"\\n\")\n\nModel 6 Adj R-squared: 0.2553348 \n\n\n\nmodel7 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + lag1week + as.factor(month) +week+\n    nearby_roads + nearby_bikelanes + nearby_stations,\n  data = train\n)\n\nsummary(model7)\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + lag1week + \n    as.factor(month) + week + nearby_roads + nearby_bikelanes + \n    nearby_stations, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9185  -0.4683  -0.1218   0.1939  25.6004 \n\nCoefficients:\n                      Estimate  Std. Error t value             Pr(&gt;|t|)    \n(Intercept)         0.43496916  0.08269874   5.260  0.00000014438239072 ***\nas.factor(hour)1   -0.02757267  0.01140429  -2.418             0.015617 *  \nas.factor(hour)2   -0.02471651  0.01120616  -2.206             0.027411 *  \nas.factor(hour)3   -0.04223159  0.01122202  -3.763             0.000168 ***\nas.factor(hour)4   -0.02301352  0.01110560  -2.072             0.038243 *  \nas.factor(hour)5    0.04958198  0.01121689   4.420  0.00000985935730785 ***\nas.factor(hour)6    0.23921149  0.01125699  21.250 &lt; 0.0000000000000002 ***\nas.factor(hour)7    0.39229409  0.01149508  34.127 &lt; 0.0000000000000002 ***\nas.factor(hour)8    0.65535940  0.01132090  57.889 &lt; 0.0000000000000002 ***\nas.factor(hour)9    0.28286593  0.01137751  24.862 &lt; 0.0000000000000002 ***\nas.factor(hour)10   0.22494720  0.01126621  19.967 &lt; 0.0000000000000002 ***\nas.factor(hour)11   0.26742698  0.01128002  23.708 &lt; 0.0000000000000002 ***\nas.factor(hour)12   0.35504566  0.01114650  31.853 &lt; 0.0000000000000002 ***\nas.factor(hour)13   0.32677897  0.01096672  29.797 &lt; 0.0000000000000002 ***\nas.factor(hour)14   0.36286831  0.01096530  33.092 &lt; 0.0000000000000002 ***\nas.factor(hour)15   0.45463939  0.01135209  40.049 &lt; 0.0000000000000002 ***\nas.factor(hour)16   0.54290308  0.01120398  48.456 &lt; 0.0000000000000002 ***\nas.factor(hour)17   0.67039251  0.01129843  59.335 &lt; 0.0000000000000002 ***\nas.factor(hour)18   0.37614606  0.01143899  32.883 &lt; 0.0000000000000002 ***\nas.factor(hour)19   0.20110538  0.01143051  17.594 &lt; 0.0000000000000002 ***\nas.factor(hour)20   0.09719693  0.01151132   8.444 &lt; 0.0000000000000002 ***\nas.factor(hour)21   0.07855837  0.01147514   6.846  0.00000000000760801 ***\nas.factor(hour)22   0.06896751  0.01137705   6.062  0.00000000134576430 ***\nas.factor(hour)23   0.02218531  0.01144897   1.938             0.052654 .  \ndotw_simple2        0.02377572  0.00588873   4.037  0.00005403574372275 ***\ndotw_simple3        0.00925967  0.00600759   1.541             0.123238    \ndotw_simple4       -0.03318396  0.00574706  -5.774  0.00000000774364607 ***\ndotw_simple5       -0.03230693  0.00611195  -5.286  0.00000012518044383 ***\ndotw_simple6       -0.03062853  0.00616800  -4.966  0.00000068477166340 ***\ndotw_simple7       -0.05470911  0.00624000  -8.767 &lt; 0.0000000000000002 ***\nTemperature         0.00293576  0.00022137  13.261 &lt; 0.0000000000000002 ***\nPrecipitation      -0.83668132  0.07082242 -11.814 &lt; 0.0000000000000002 ***\nlag1Hour            0.28694920  0.00155823 184.151 &lt; 0.0000000000000002 ***\nlag3Hours           0.08684120  0.00152650  56.889 &lt; 0.0000000000000002 ***\nlag1day             0.17362425  0.00148343 117.043 &lt; 0.0000000000000002 ***\nlag1week            0.09151274  0.00137458  66.575 &lt; 0.0000000000000002 ***\nas.factor(month).L  0.06170679  0.00796574   7.747  0.00000000000000947 ***\nas.factor(month).Q  0.01781405  0.00316482   5.629  0.00000001816291059 ***\nweek               -0.01881709  0.00166046 -11.332 &lt; 0.0000000000000002 ***\nnearby_roads        0.00179729  0.00020358   8.828 &lt; 0.0000000000000002 ***\nnearby_bikelanes    0.00126797  0.00008765  14.466 &lt; 0.0000000000000002 ***\nnearby_stations     0.00452301  0.00015920  28.411 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9737 on 377754 degrees of freedom\nMultiple R-squared:  0.3439,    Adjusted R-squared:  0.3438 \nF-statistic:  4829 on 41 and 377754 DF,  p-value: &lt; 0.00000000000000022\n\n\n\ntest &lt;- test %&gt;%\n  mutate(\n    pred2 = predict(model2, newdata = test),\n    pred6 = predict(model6, newdata = test),\n    pred7 = predict(model7, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results &lt;- data.frame(\n  Model = c(\n    \"2. + Temporal Lags\",\n    \"6. Model 2 + Demographics + Station FE + Rush Hour Interaction + Week Lag + Nearby Features\",\n    \"7. Model 2 + Week Lag + Nearby Features\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred6), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred7), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nMean Absolute Error by Model (Test Set)\n\n\nModel\nMAE (trips)\n\n\n\n\n2. + Temporal Lags\n0.40\n\n\n6. Model 2 + Demographics + Station FE + Rush Hour Interaction + Week Lag + Nearby Features\n0.65\n\n\n7. Model 2 + Week Lag + Nearby Features\n0.41\n\n\n\n\n\n\n\n\ntest &lt;- test %&gt;%\n  mutate(\n    error = Trip_Count - pred7,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour &lt; 7 ~ \"Overnight\",\n      hour &gt;= 7 & hour &lt; 10 ~ \"AM Rush\",\n      hour &gt;= 10 & hour &lt; 15 ~ \"Mid-Day\",\n      hour &gt;= 15 & hour &lt;= 18 ~ \"PM Rush\",\n      hour &gt; 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred7)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\n# Calculate MAE by station\nstation_errors &lt;- test %&gt;%\n  group_by(start_station, start_lat.x, start_lon.y) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors &lt;- test %&gt;%\n  filter(!is.na(pred7)) %&gt;%\n  group_by(start_station, start_lat.x, start_lon.y) %&gt;%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred6), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1.1 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE\\n(trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors \",\n       subtitle = \"Model 7\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\np1 | p1.1\n\n\n\n\n\n\n\n\n**Model 7 MAEs are less severe in the areas just west and north of center city, however the patterns largely follow the same spatial distribution as model 2.\nPoisson Model\n\np_model &lt;- glm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + lag1week + as.factor(month) +week+\n    nearby_roads + nearby_bikelanes + nearby_stations,\n  data = train,\n  family = poisson(link = \"log\")\n)\n\nsummary(p_model)\n\n\nCall:\nglm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + lag1week + \n    as.factor(month) + week + nearby_roads + nearby_bikelanes + \n    nearby_stations, family = poisson(link = \"log\"), data = train)\n\nCoefficients:\n                     Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)        -0.3008142  0.1055903  -2.849              0.00439 ** \nas.factor(hour)1   -0.4532014  0.0312112 -14.520 &lt; 0.0000000000000002 ***\nas.factor(hour)2   -0.7459004  0.0339418 -21.976 &lt; 0.0000000000000002 ***\nas.factor(hour)3   -1.3348163  0.0427056 -31.256 &lt; 0.0000000000000002 ***\nas.factor(hour)4   -1.2040168  0.0400945 -30.029 &lt; 0.0000000000000002 ***\nas.factor(hour)5   -0.0591638  0.0279322  -2.118              0.03417 *  \nas.factor(hour)6    0.8693465  0.0229103  37.946 &lt; 0.0000000000000002 ***\nas.factor(hour)7    1.2721808  0.0217796  58.412 &lt; 0.0000000000000002 ***\nas.factor(hour)8    1.6166590  0.0208355  77.592 &lt; 0.0000000000000002 ***\nas.factor(hour)9    1.2273334  0.0214022  57.346 &lt; 0.0000000000000002 ***\nas.factor(hour)10   1.1005384  0.0216460  50.843 &lt; 0.0000000000000002 ***\nas.factor(hour)11   1.1760243  0.0215526  54.565 &lt; 0.0000000000000002 ***\nas.factor(hour)12   1.2803074  0.0211569  60.515 &lt; 0.0000000000000002 ***\nas.factor(hour)13   1.2686336  0.0210216  60.349 &lt; 0.0000000000000002 ***\nas.factor(hour)14   1.2881641  0.0208965  61.645 &lt; 0.0000000000000002 ***\nas.factor(hour)15   1.3849675  0.0209103  66.234 &lt; 0.0000000000000002 ***\nas.factor(hour)16   1.4503161  0.0206794  70.133 &lt; 0.0000000000000002 ***\nas.factor(hour)17   1.5076652  0.0205865  73.236 &lt; 0.0000000000000002 ***\nas.factor(hour)18   1.2402372  0.0209488  59.203 &lt; 0.0000000000000002 ***\nas.factor(hour)19   1.0421225  0.0214496  48.585 &lt; 0.0000000000000002 ***\nas.factor(hour)20   0.8459129  0.0220879  38.298 &lt; 0.0000000000000002 ***\nas.factor(hour)21   0.7262344  0.0227359  31.942 &lt; 0.0000000000000002 ***\nas.factor(hour)22   0.5944549  0.0233785  25.427 &lt; 0.0000000000000002 ***\nas.factor(hour)23   0.3319015  0.0249391  13.308 &lt; 0.0000000000000002 ***\ndotw_simple2        0.0465151  0.0073389   6.338  0.00000000023258043 ***\ndotw_simple3        0.0192599  0.0076119   2.530              0.01140 *  \ndotw_simple4       -0.0706125  0.0076472  -9.234 &lt; 0.0000000000000002 ***\ndotw_simple5       -0.0686913  0.0080993  -8.481 &lt; 0.0000000000000002 ***\ndotw_simple6       -0.0645385  0.0082180  -7.853  0.00000000000000405 ***\ndotw_simple7       -0.1279885  0.0084311 -15.181 &lt; 0.0000000000000002 ***\nTemperature         0.0068856  0.0002791  24.666 &lt; 0.0000000000000002 ***\nPrecipitation      -3.4025809  0.1945009 -17.494 &lt; 0.0000000000000002 ***\nlag1Hour            0.1370088  0.0010631 128.879 &lt; 0.0000000000000002 ***\nlag3Hours           0.0663740  0.0012058  55.045 &lt; 0.0000000000000002 ***\nlag1day             0.0846167  0.0011095  76.263 &lt; 0.0000000000000002 ***\nlag1week            0.0675999  0.0011719  57.683 &lt; 0.0000000000000002 ***\nas.factor(month).L  0.1681597  0.0109417  15.369 &lt; 0.0000000000000002 ***\nas.factor(month).Q -0.0239398  0.0045030  -5.316  0.00000010584953979 ***\nweek               -0.0602140  0.0021050 -28.605 &lt; 0.0000000000000002 ***\nnearby_roads        0.0054790  0.0002687  20.392 &lt; 0.0000000000000002 ***\nnearby_bikelanes    0.0011617  0.0001093  10.626 &lt; 0.0000000000000002 ***\nnearby_stations     0.0158128  0.0002146  73.690 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 621223  on 377795  degrees of freedom\nResidual deviance: 393742  on 377754  degrees of freedom\nAIC: 685434\n\nNumber of Fisher Scoring iterations: 6\n\n\n\ntest &lt;- test %&gt;%\n  mutate(\n    p_pred = predict(p_model, newdata = test, type = \"response\")\n  )\n\n# Calculate MAE for each model\nmae_results &lt;- data.frame(\n  Model = c(\n    \"2. + Temporal Lags\",\n    \"6. Model 2 + Demographics + Station FE + Rush Hour Interaction + Week Lag + Nearby Features\",\n    \"7. Model 2 + Week Lag + Nearby Features\",\n    \"Poisson Model (Same Variables as Model 7)\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred6), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred7), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$p_pred), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nMean Absolute Error by Model (Test Set)\n\n\nModel\nMAE (trips)\n\n\n\n\n2. + Temporal Lags\n0.40\n\n\n6. Model 2 + Demographics + Station FE + Rush Hour Interaction + Week Lag + Nearby Features\n0.65\n\n\n7. Model 2 + Week Lag + Nearby Features\n0.41\n\n\nPoisson Model (Same Variables as Model 7)\n0.41\n\n\n\n\n\n\n\nThe Poisson Model does not improve MAE very much, however it does not result in any negative predictions, which inherently makes it a better method for predicting trip counts in this context."
  },
  {
    "objectID": "labs/lab5/scripts/Lab_5_Instructions.html",
    "href": "labs/lab5/scripts/Lab_5_Instructions.html",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "For Homework 5, you’ll work either all byyyy yourself or in teams of 2 to:\n\n\n\nDownload data for Q2, Q3, or Q4 2024 from: https://www.rideindego.com/about/data/\nAdapt this code to work with your quarter:\n\nUpdate date ranges for weather data\nCheck for any data structure changes\nCreate the same 5 models\nCalculate MAE for each model\n\nCompare results to Q1 2025:\n\nHow do MAE values compare? Why might they differ?\nAre temporal patterns different (e.g., summer vs. winter)?\nWhich features are most important in your quarter?\n\n\n\n\n\nAnalyze your model’s errors in detail:\n\nSpatial patterns:\n\nCreate error maps\nIdentify neighborhoods with high errors\nHypothesize why (missing features? different demand patterns?)\n\nTemporal patterns:\n\nWhen are errors highest?\nDo certain hours/days have systematic under/over-prediction?\nAre there seasonal patterns?\n\nDemographic patterns:\n\nRelate errors to census characteristics\nAre certain communities systematically harder to predict?\nWhat are the equity implications?\n\n\n\n\n\nBased on your error analysis, add 2-3 NEW features to improve the model:\nPotential features to consider:\nTemporal features:\n\nHoliday indicators (Memorial Day, July 4th, Labor Day)\nSchool calendar (Penn, Drexel, Temple in session?)\nSpecial events (concerts, sports games, conventions)\nDay of month (payday effects?)\n\nWeather features:\n\nFeels-like temperature (wind chill/heat index)\n“Perfect biking weather” indicator (60-75°F, no rain)\nPrecipitation forecast (not just current)\nWeekend + nice weather interaction\n\nSpatial features:\n\nDistance to Center City\nDistance to nearest university\nDistance to nearest park\nPoints of interest nearby (restaurants, offices, bars)\nStation capacity\nBike lane connectivity\n\nTrip history features:\n\nRolling 7-day average demand\nSame hour last week\nStation “type” clustering (residential, commercial, tourist)\n\nImplementation:\n\nAdd your features to the best model\nCompare MAE before and after\nExplain why you chose these features\nDid they improve predictions? Where?\n\nTry a poisson model for count data\n\nDoes this improve model fit?\n\n\n\n\nWrite 1-2 paragraphs addressing:\n\nOperational implications:\n\nIs your final MAE “good enough” for Indego to use?\nWhen do prediction errors cause problems for rebalancing?\nWould you recommend deploying this system? Under what conditions?\n\nEquity considerations:\n\nDo prediction errors disproportionately affect certain neighborhoods?\nCould this system worsen existing disparities in bike access?\nWhat safeguards would you recommend?\n\nModel limitations:\n\nWhat patterns is your model missing?\nWhat assumptions might not hold in real deployment?\nHow would you improve this with more time/data?"
  },
  {
    "objectID": "labs/lab5/scripts/Lab_5_Instructions.html#part-1-replicate-with-different-quarter-alternately-you-could-do-a-longer-time-span-by-merging-multiple-quarters-together.-im-not-picky-about-that",
    "href": "labs/lab5/scripts/Lab_5_Instructions.html#part-1-replicate-with-different-quarter-alternately-you-could-do-a-longer-time-span-by-merging-multiple-quarters-together.-im-not-picky-about-that",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Download data for Q2, Q3, or Q4 2024 from: https://www.rideindego.com/about/data/\nAdapt this code to work with your quarter:\n\nUpdate date ranges for weather data\nCheck for any data structure changes\nCreate the same 5 models\nCalculate MAE for each model\n\nCompare results to Q1 2025:\n\nHow do MAE values compare? Why might they differ?\nAre temporal patterns different (e.g., summer vs. winter)?\nWhich features are most important in your quarter?"
  },
  {
    "objectID": "labs/lab5/scripts/Lab_5_Instructions.html#part-2-error-analysis",
    "href": "labs/lab5/scripts/Lab_5_Instructions.html#part-2-error-analysis",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Analyze your model’s errors in detail:\n\nSpatial patterns:\n\nCreate error maps\nIdentify neighborhoods with high errors\nHypothesize why (missing features? different demand patterns?)\n\nTemporal patterns:\n\nWhen are errors highest?\nDo certain hours/days have systematic under/over-prediction?\nAre there seasonal patterns?\n\nDemographic patterns:\n\nRelate errors to census characteristics\nAre certain communities systematically harder to predict?\nWhat are the equity implications?"
  },
  {
    "objectID": "labs/lab5/scripts/Lab_5_Instructions.html#part-3-feature-engineering-model-improvement",
    "href": "labs/lab5/scripts/Lab_5_Instructions.html#part-3-feature-engineering-model-improvement",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Based on your error analysis, add 2-3 NEW features to improve the model:\nPotential features to consider:\nTemporal features:\n\nHoliday indicators (Memorial Day, July 4th, Labor Day)\nSchool calendar (Penn, Drexel, Temple in session?)\nSpecial events (concerts, sports games, conventions)\nDay of month (payday effects?)\n\nWeather features:\n\nFeels-like temperature (wind chill/heat index)\n“Perfect biking weather” indicator (60-75°F, no rain)\nPrecipitation forecast (not just current)\nWeekend + nice weather interaction\n\nSpatial features:\n\nDistance to Center City\nDistance to nearest university\nDistance to nearest park\nPoints of interest nearby (restaurants, offices, bars)\nStation capacity\nBike lane connectivity\n\nTrip history features:\n\nRolling 7-day average demand\nSame hour last week\nStation “type” clustering (residential, commercial, tourist)\n\nImplementation:\n\nAdd your features to the best model\nCompare MAE before and after\nExplain why you chose these features\nDid they improve predictions? Where?\n\nTry a poisson model for count data\n\nDoes this improve model fit?"
  },
  {
    "objectID": "labs/lab5/scripts/Lab_5_Instructions.html#part-4-critical-reflection",
    "href": "labs/lab5/scripts/Lab_5_Instructions.html#part-4-critical-reflection",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Write 1-2 paragraphs addressing:\n\nOperational implications:\n\nIs your final MAE “good enough” for Indego to use?\nWhen do prediction errors cause problems for rebalancing?\nWould you recommend deploying this system? Under what conditions?\n\nEquity considerations:\n\nDo prediction errors disproportionately affect certain neighborhoods?\nCould this system worsen existing disparities in bike access?\nWhat safeguards would you recommend?\n\nModel limitations:\n\nWhat patterns is your model missing?\nWhat assumptions might not hold in real deployment?\nHow would you improve this with more time/data?"
  },
  {
    "objectID": "labs/lab5/scripts/Lab_5_Instructions.html#what-to-submit-per-team",
    "href": "labs/lab5/scripts/Lab_5_Instructions.html#what-to-submit-per-team",
    "title": "HW5: Your Turn!",
    "section": "What to Submit (per team)",
    "text": "What to Submit (per team)\n\nqmd file with all your code (commented!)\nHTML output with results and visualizations\nBrief report summarizing (with supporting data & visualization):\n\nYour quarter and why you chose it\nModel comparison results\nError analysis insights\nNew features you added and why\nCritical reflection on deployment\n\nYou only need to submitted once as a team (one submit the link to your portfolio on Canvas)"
  },
  {
    "objectID": "labs/lab5/scripts/Lab_5_Instructions.html#tips-for-success",
    "href": "labs/lab5/scripts/Lab_5_Instructions.html#tips-for-success",
    "title": "HW5: Your Turn!",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nStart early - data download and processing takes time\nWork together - pair programming is your friend\nTest incrementally - don’t wait until the end to run code\nDocument everything - explain your choices\nBe creative - the best features come from understanding Philly!\nThink critically - technical sophistication isn’t enough"
  },
  {
    "objectID": "labs/lab5/scripts/Knox_Katie_Assignment5.html#part-4-critical-reflection",
    "href": "labs/lab5/scripts/Knox_Katie_Assignment5.html#part-4-critical-reflection",
    "title": "Assignment 5: Space-Time Prediction of Bike Share Demand",
    "section": "Part 4: Critical Reflection",
    "text": "Part 4: Critical Reflection\nOverall, I would not recommend using any of the models produced in this study in order to predict real, exact bike station counts, however they could be useful rather in posing questions for further research into what factors affect bike station use the most, and how to make more accurate predictions at the extremes while not foregoing less busy stations. The models presented in this study all fell very short of the highest traffic station-times, and these are essential to get right in a usable model because identifying them will equate to predicting where the bulk of the bikes are at a given time in order to make redistribution more efficient. The factors that improved the model the most were time lags, meaning that the trips from a station at various times previous to the current time had a strong predictive effect, however it could be that the influence of these variables makes the model less adequate at predicting the extremes of trips. In other words, the time lags create a model that is more normalized over time, but the key to a useful model would be in predicting the more extreme trip counts that will have higher redistribution needs. The spatial distribution of prediction errors is not particularly egregious from a visual inspection of the map.\nThe biggest takeaway is that predicting bike share station usage by hour would benefit from additional complexity that is able to model the more extreme ends of bike share usage, perhaps using morning and evening rush as factor variables in addition to the hour of the day as a factor variable. Another aspect that might help in future iterations is accounting for holidays, again as a factor variable marking if the day is a holiday or not. Even if there is future improvement on the model, caution should be used on irregular public events or extreme weather events that will significantly skew bike trips in an unpredictable way."
  }
]